{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zY0nC5dRQZo"
      },
      "source": [
        "**ReadMe**\n",
        "\n",
        "The question 1 is answered in part 1.\n",
        "\n",
        "The question 3 is answered in part 2.\n",
        "\n",
        "The question 2 is in part 3 (unfinished)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ7fOoXubGwK",
        "outputId": "09a0bfc9-489d-4fc6-db32-d1c16d679542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Winter 2022/CME241\n"
          ]
        }
      ],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import sys\n",
        "%cd /content/drive/My\\ Drive/Winter\\ 2022/CME241"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Merton's Portfolio Optimization"
      ],
      "metadata": {
        "id": "48kFwRR7snaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will consider the portfoilio can be dynamically adjusted as the time advances in order to maintain the ratio of investment in the risky asset to be a constant $\\pi$.\n",
        "\n",
        "Let the return of a unit investment in a risky asset be $X$, which is a random variable. In this process, let $B_t$ be a brownian motion, and $W_t$ be wealth. Consider the change of wealth at an infinistesimal time\n",
        "\n",
        "\\begin{align*}\n",
        "  dW_t =  (r+\\pi(\\mu-r))W_tdt + \\pi\\sigma W_tdB_t\n",
        "\\end{align*}\n",
        "\n",
        "Thus, from Ito's Formula and the marginal distribution of a brwonian motion,\n",
        "\\begin{align*}\n",
        "  \\log W_1 \\sim \\mathcal{N}(r+\\pi(\\mu-r) - \\frac{\\pi^2\\sigma^2}{2}, \\pi^2\\sigma^2)\n",
        "\\end{align*}\n",
        "\n",
        "Thus, we have to maximize\n",
        "\\begin{align*}\n",
        "  \\mathbf{E}[U(W_1)] = \\mathbf{E}[\\log W_1]\n",
        "  = r+\\pi(\\mu-r) - \\frac{\\pi^2\\sigma^2}{2}\n",
        "\\end{align*}\n",
        "\n",
        "Thus, the optimal $\\pi$ is \n",
        "\\begin{align*}\n",
        "  \\frac{\\mu-r}{\\gamma \\sigma^2}\n",
        "\\end{align*}\n",
        "\n",
        "when $\\mu > r$. Otherwise, $\\pi^* = 0$."
      ],
      "metadata": {
        "id": "QLULZO8n-Tp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Working-Learning Decision"
      ],
      "metadata": {
        "id": "eNf18hVe_Ixc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us first look at a simple case when there is only one job. We can also think of it as a one family of jobs screened through the ability $s$. However, this will exclude the possibility of having multiple skills or having luckiness in getting different jobs (since the job will be determined by only 1 skill). In this case, we can see that the job state will only be either employed or unemployed. If employed, one can choose action by varying $\\alpha$. If unemployed, no action can be made. \n",
        "\n",
        "In this model, let us consider the probability of losing a current job to also be a function of $s$ (By having more skill, one will be less likely to get fired). For a possible extension, we can also consider the case when the rate of lossing job is also dependent on the choice $\\alpha$. However, I suggest that it may be more reasonable to just put a hard constraint on $\\alpha$ instead (similar to the idea of minimum work hour required for some jobs and maximum number of work hours allowed).\n",
        "\n",
        "Moreover, let consider the decay of knowledge to also be a function of $s$. It is obvious that exponential decay is a type of this function of $s$ family. In this case, let the decay be $h(s)$.\n",
        "\n",
        "Thus, every parameter will be a function of $s$."
      ],
      "metadata": {
        "id": "miF8FI73qyjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we look at the state. We want think to be a markov problem, thereby wanting the state to conclude the relevant past. (For example, if having more money allow more possibility of getting job, the money or some sufficient function of it has to also be stored in the state). However, in this problem formulation, the parameter that is relevant for everything such as the probabilkity of losing a job, the probability of getting a new job, the money maded in a job, the gain of knowledge, and the loss of knowledge. Thus, in this problem, we design the state to include employed status, which is a binary, and $s$. Another consideration that we may have to include is about the time horizon of the problem. In one sense, we can formulate it as a finite time process, whose terminal time be the time of retirement (assume that it is pre-determined. ie. no early or late-retirement and no death before retirement). Since we formulate the process to be a discrete time, the time state will be an integer. Let $T$ be the terminal time.\n",
        "\n",
        "Thus,\n",
        "\\begin{align*}\n",
        "  \\mathcal{S} &= \\{0,1\\} \\times \\mathbf{R} \\times \\mathbf{N}_0 \\cap [0,T]\\\\\n",
        "  \\mathcal{T} &= \\{0,1\\} \\times \\mathbf{R} \\times \\{T\\}\\\\\n",
        "  \\mathcal{N} &= \\{0,1\\} \\times \\mathbf{R} \\times \\mathbf{N}_0 \\cap [0,T-1]\n",
        "\\end{align*}\n",
        "\n",
        "The action can be written as\n",
        "\\begin{align*}\n",
        "  \\mathcal{A}(S=(0,s,t)) &= {0}\\\\\n",
        "  \\mathcal{A}(S=(1,s,t)) &= [0,1]\n",
        "\\end{align*}\n",
        "\n",
        "The transition probability will then be\n",
        "\\begin{align*}\n",
        "  \\mathcal{P}((0,s,t), 0, ((x,s',t'),r)) &= \\mathbf{1}_{s'=s-h(s)} \\mathbf{1}_{t'=t+1} \\mathbf{1}_{r=0} (x q(s) + (1-x)(1-q(s)))\\\\\n",
        "  \\mathcal{P}((1,s,t), \\alpha, ((x,s',t'),r)) &= \\mathbf{1}_{s'=s+(1-\\alpha) g(s)} \\mathbf{1}_{t'=t+1} \\mathbf{1}_{r=\\alpha f(s)} (x (1-p(s)) + (1-x)p(s))\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "lFUqTgZq9UWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fact that there exists a finite time horizon, we will get that at timesetp $T-1$, the optimal action will be $\\alpha=1$ if still employed. "
      ],
      "metadata": {
        "id": "WpdjLjMhhV1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Backward Induction"
      ],
      "metadata": {
        "id": "ezulcsnnh5l-"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Assignment7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}