## Overview {#sec:overview .unnumbered}

### Learning Reinforcement Learning {#sec:learning-rl}

Reinforcement Learning (RL) is emerging as a viable and powerful technique for solving a variety of complex business problems across industries that involve Sequential Optimal Decisioning under Uncertainty. Although RL is classified as a branch of Machine Learning (ML), it tends to be viewed and treated quite differently from other branches of ML (Supervised and Unsupervised Learning). Indeed, **RL seems to hold the key to unlocking the promise of AI** – machines that adapt their decisions to vagaries in observed information, while continuously steering towards the optimal outcome. It’s penetration in high-profile problems like self-driving cars, robotics and strategy games points to a future where RL algorithms will have decisioning abilities far superior to humans.

But when it comes getting educated in RL, there seems to be a reluctance to jump right in because RL seems to have acquired a reputation of being mysterious and exotic. We often hear even technical people claim that RL involves “advanced math” and “complicated engineering”, and so there seems to be a psychological barrier to entry. While real-world RL algorithms and implementations do get fairly elaborate and complicated in overcoming the proverbial last-mile of business problems, the foundations of RL can actually be learnt without heavy technical machinery. **The core purpose of this book is to demystify RL by finding a balance between depth of understanding and keeping technical content basic**. So now we list the key features of this book which enable this balance:

* Focus on the foundational theory underpinning RL. Our treatment of this theory is based on undergraduate-level Probability, Optimization, Statistics and Linear Algebra. **We emphasize rigorous but simple mathematical notations and formulations in developing the theory**, and encourage you to write out the equations rather than just reading from the book. Occasionally, we invoke some advanced mathematics (eg: Stochastic Calculus) but the majority of the book is based on easily understandable mathematics. In particular, two basic theory concepts - Bellman Optimality Equation and Generalized Policy Iteration - are emphasized throughout the book as they form the basis of pretty much everything we do in RL, even in the most advanced algorithms.
* Parallel to the mathematical rigor, we bring the concepts to life with simple examples and informal descriptions to help you develop an intuitive understanding of the mathematical concepts. **We drive towards creating appropriate mental models to visualize the concepts**. Often, this involves turning mathematical abstractions into physical examples (emphasizing visual intuition). So we go back and forth between rigor and intuition, between abstractions and visuals, so as to blend them nicely and get the best of both worlds.
* Each time you learn a new mathematical concept or algorithm, we ask you to write small pieces of code (in Python) that implements the concept/algorithm. As an example, if you just learnt a surprising theorem, we’d ask you to write a simulator to simply verify the statement of the theorem. We emphasize this approach not just to bolster the theoretical and intuitive understanding with a hands-on experience, but also because there is a strong emotional effect of seeing expected results emanating from one’s code, which in turn promotes long-term retention of the concepts. Most importantly, we avoid messy and complicated ML/RL/BigData tools/packages and stick to bare-bones Python/numpy as these unnecessary tools/packages are huge blockages to core understanding. We believe **coding-from-scratch is the correct approach to truly understand the concepts/algorithms**. 
* Lastly, it is important to work with examples that are A) simplified versions of real-world problems in a business domain rich with applications, B) adequately comprehensible without prior business-domain knowledge, C) intellectually interesting and D) sufficiently marketable to employers. We’ve chosen Financial Trading applications. For each financial problem, we first cover the traditional approaches (including solutions from landmark papers) and then cast the problems in ways that can be solved with RL. **We have made considerable effort to make this book self-contained in terms of the financial knowledge required to navigate these problems**.

### What you'll learn from this Book

Here is what you will specifically learn and gain from the book:

* You will learn about the simple but powerful theory of Markov Decision Processes (MDPs) – a framework for Sequential Optimal Decisioning under Uncertainty. You will firmly understand the power of Bellman Equations, which is at the heart of all Dynamic Programming as well as all RL algorithms.
* You will master Dynamic Programming (DP) Algorithms, which are a class of (in the language of AI) Planning Algorithms.  You will learn about Policy Iteration, Value Iteration, Backward Induction, Approximate Dynamic Programming and the all-important concept of Generalized Policy Iteration which lies at the heart of all DP as well as all RL algorithms.
* You will gain a solid understanding of a variety of Reinforcement Learning (RL) Algorithms, starting with the basic algorithms like SARSA and Q-Learning and moving on to several important algorithms that work well in practice, including Gradient Temporal Difference, Deep Q-Network, Least-Squares Policy Iteration, Policy Gradient, Monte-Carlo Tree Search. You will learn about how to gain advantages in these algorithms with bootstrapping, off-policy learning and deep-neural-networks-based function approximation. You will also learn how to balance exploration and exploitation with Multi-Armed Bandits techniques like Upper Confidence Bounds, Thompson Sampling, Gradient Bandits and Information State-Space algorithms.
* You will exercise with plenty of “from-scratch” Python implementations of models and algorithms. Throughout the book, we emphasize healthy Python programming practices including interface design, type annotations, functional programming and inheritance-based polymorphism (always ensuring that the programming principles reflect the mathematical principles). The larger take-away from this book will be a rare (and high-in-demand) ability to blend Applied Mathematics concepts with Software Design paradigms.
* You will go deep into important Financial Trading problems, including:

  - (Dynamic) Asset-Allocation to maximize Utility of Consumption
  - Pricing and Hedging of Derivatives in an Incomplete Market
  - Optimal Exercise/Stopping of Path-Dependent American Options
  - Optimal Trade Order Execution (managing Price Impact)
  - Optimal Market-Making (Bid/Ask managing Inventory Risk)
 
* We treat each of the above problems as MDPs (i.e., Optimal Decisioning formulations), first going over classical/analytical solutions to these problems, then introducing real-world frictions/considerations, and tackling with DP and/or RL.
* As a bonus, we throw in a few applications beyond Finance, including a couple from Supply-Chain and Clearance Pricing in a Retail business. 
* We implement a wide range of Algorithms and develop various models in [this git code base](https://github.com/TikhonJelvis/RL-book) that we refer to and explain in detail throughout the book. This code base not only provides detailed clarity on the algorithms/models, but also serves to educate on healthy programming patterns suitable not just for RL, but more generally for any Applied Mathematics work.
* In summary, this book blends Theory/Mathematics, Programming/Algorithms and Real-World Financial Nuances while always keeping things simple and intuitive.

### Expected Background to read this Book

There is no short-cut to learning Reinforcement Learning or learning the Financial Applications content. You will need to allocate at least 50 hours of effort to learn this material (assuming you have no prior background in these topics). Also, although we have kept the Mathematics, Programming and Financial content fairly basic, this topic is only for technically-inclined readers. Below we outline the technical preparation that is required to follow the material covered in this book.

* Experience with (but not necessarily expertise in) Python is expected and a good deal of comfort with numpy is required. The nature of Python programming we do is mainly numerical algorithms. You don’t need to be a professional software developer/engineer but you need to have a healthy interest in learning Python best practices associated with mathematical modeling, algorithms development and numerical programming (we teach these best practices in this book). We don't use any of the popular (but messy and complicated) Big Data/Machine Learning libraries such as Pandas, PySpark, scikit, Tensorflow, PyTorch, OpenCV, NLTK etc. (all you need to know is numpy).
* Familiarity with git and use of an Integrated Development Environment (IDE), eg: Pycharm or Emacs (with Python plugins), is recommended, but not required.
* Familiarity with LaTeX for writing equations is recommended, but not required (other typesetting tools, or even hand-written math is fine, but LaTeX is a skill that is very valuable if you’d like a future in the general domain of Applied Mathematics).
* You need to be strong in undergraduate-level Probability as it is the most important foundation underpinning RL. 
* You will also need to have some preparation in undergraduate-level Numerical Optimization, Statistics, Linear Algebra.
* No background in Finance is required, but a strong appetite for Mathematical Finance is required.

### Decluttering the Jargon linked to Reinforcement Learning

Machine Learning has exploded in the past decade or so, and Reinforcement Learning (treated as a branch of Machine Learning and hence, a branch of A.I.) has surfaced to the mainstream in both academia and in the industry. It is important to understand what Reinforcement Learning aims to solve, rather than the more opaque view of RL as a technique to learn from data. RL aims to solve problems that involve making *Sequential Optimal Decisions under Uncertainty*. Let us break down this jargon so as to develop an intuitive (and high-level) understanding of the features pertaining to the problems RL solves.

Firstly, let us understand the term *Uncertainty*. This means the problems under consideration involve random variables that evolve over time. The technical term for this is *Stochastic Processes*. We will cover this in detail later in this book, but for now, it's important to recognize that evolution of random variables over time is very common in nature (eg: weather) and in business (eg: customer demand or stock prices), but modeling and navigating such random evolutions can be enormously challenging. 

The next term is *Optimal Decisions*, which refers to the technical term *Optimization*. This means there is a well-defined quantity to be maximized (the "goal"). The quantity to be maximized might be financial (like investment value  or business profitability), or it could be a safety or speed metric (such as health of customers or time to travel), or something more complicated like a blend of multiple objectives rolled into a single objective.

The next term is *Sequential*, which refers to the fact that as we move forward in time, the relevant random variables' values evolve, and the optimal decisions have to be adjusted to the "changing circumstances". Due to this non-static nature of the optimal decisions, the term *Dynamic Decisions* is often used in the literature covering this subject. 

Putting together the three notions of (Uncertainty/Stochastic, Optimization, Sequential/Dynamic Decisions), these problems (that RL tackles) have the common feature that one needs to *overpower the uncertainty by persistent steering towards the goal*. This brings us to the term *Control* (in references to *persistent steering*). These problems are often (aptly) characterized by the technical term *Stochastic Control*. So you see that there is indeed a lot of jargon here. All of this jargon will become amply clear after the first few chapters in this book where we develop mathematical formalism to understand these concepts precisely (and also write plenty of code to internalized these concepts). For now, we just wanted to familiarize you with the range of jargon linked to Reinforcement Learning.

This jargon overload is due to the confluence of terms from Control Theory (emerging from Engineering disciplines), from Operations Research, and from Artificial Intelligence (emerging from Computer Science). For simplicity, we prefer to refer to the class of problems RL aims to solve as *Stochastic Control* problems. Reinforcement Learning is a class of algorithms that are used to solve Stochastic Control problems. We should point out here that there are other disciplines (beyond Control Theory, Operations Research and Artificial Intelligence) with a rich history of developing theory and techniques within the general space of Stochastic Control. Figure \ref{fig:many_faces_of_rl} (a popular image on the internet) illustrates the many faces of Stochastic Control, which has recently been refered to as "The many faces of Reinforcement Learning".

![Many Faces of Reinforcement Learning \label{fig:many_faces_of_rl}](./chapter0/ManyFacesOfRL.png "Many Faces of Reinforcement Learning")

It is also important to recognize that Reinforcement Learning is considered to be a branch of Machine Learning. While there is no crisp definition for *Machine Learning* (ML), ML generally refers to the broad set of techniques to infer mathematical models/functions by acquiring ("learning") knowledge of patterns and properties in the presented data. In this regard, Reinforcement Learning does fit this definition. However, unlike the other branches of ML (Supervised Learning and Unsupervised Learning), Reinforcement Learning is a lot more ambitious - it not only learns the patterns and properties of the presented data (internally building a model of the data), it also learns about the appropriate behaviors to be exercised (appropriate decisions to be made) so as to drive towards the optimization objective. It is sometimes said that Supervised Learning and Unsupervised learning are about "minimization" (i.e., they minimize the fitting error of a model to the presented data), while Reinforcement Learning is about "maximization" (i.e., RL also identifies the suitable decisions to be made to maximize a well-defined objective). Figure \ref{fig:ml_branches} depicts the in-vogue classification of Machine Learning.

![Branches of Machine Learning \label{fig:ml_branches}](./chapter0/BranchesOfML.jpg "Branches of Machine Learning")

More importantly, the class of problems RL aims to solve can be described with a simple yet powerful mathematical framework known as *Markov Decision Processes* (abbreviated as MDPs). We have an entire chapter dedicated to deep coverage of MDPs, but we provide a quick high-level introduction to MDPs in the next section.

### Introduction to the Markov Decision Process (MDP) framework

![The MDP Framework \label{fig:mdp_framework}](./chapter0/MDP.png "Agent-Environment Interaction in a MDP")

The framework of a Markov Decision Process is depicted in Figure \ref{fig:mdp_framework}. As the Figure indicates, the *Agent* and the *Environment* interact in a time-sequenced loop. The term *Agent* refers to an algorithm (AI algorithm) and the term *Environment* refers to an abstract entity that serves up uncertain outcomes to the Agent. It is important to note that the Environment is indeed abstract in this framework and can be used to model all kinds of real-world situations such as the financial market serving up random stock prices or customers of a company serving up random demand or a chess opponent serving up random moves (from the perspective of the Agent), or really anything at all you can imagine that serves up something random at each time step (it is up to us to model an Environment appropriately to fit the MDP framework).

As the Figure indicates, at each time step $t$, the Agent observes an abstract piece of information (which we call *State*) and a numerical (real number) quantity that we call *Reward*. Note that the concept of *State* is indeed completely abstract in this framework and we can model *State* to be any data type, as complex or elaborate as we'd like. This flexibility in modeling *State* permits us to model all kinds of real-world situations as an MDP. Upon observing a *State* and a *Reward* at time step $t$, the *Agent* responds by taking an *Action*. Again, the concept of *Action* is completely abstract and is meant to represent an activity performed by an AI algorithm. It could be a purchase or sale of a stock responding to market stock price movements, or it could be movement of inventory from a warehouse to a store in response to large sales at the store, or it could be a chess move in response to the opponent's chess move (opponent is *Environment*), or really anything at all you can imagine that responds to observations (*State* and *Reward*) served by the *Environment*.

Upon receiving an *Action* from the *Agent* at time step $t$, the *Environment* responds (with time ticking over to $t+1$) by serving up the next time step's random *State* and random *Reward*. A technical detail (that we shall explain in detail later) is that the *State* is assumed to have the *Markov Property*, which means:

* The next *State/Reward* depends only on Current *State* (for a given *Action*).
* The current *State* encapsulates all relevant information from the history of the interaction between the *Agent* and the *Environment*.
* The current *State* is a sufficient statistic of the future (for a given *Action*).

The goal of the *Agent* at any point in time is to maximize the *Expected Sum* of all future *Reward*s by controlling (at each time step) the *Action* as a function of the observed *State* (at that time step). This function from a *State* to *Action* at any time step is known as the *Policy* function. So we say that the agent's job is exercise control by determining the optimal *Policy* function. Hence, this is a dynamic (i.e., time-sequenced) control system under uncertainty. If the above description was too terse, don't worry - we will explain all of this in great detail in the coming chapters. For now, we just wanted to provide a quick flavor for what the MDP framework looks like. Now we sketch the above description with some (terse) mathematical notation to provide a bit more of the overview of the MDP framework. The following notation is for discrete time steps (continuous time steps notation is analogous, but technically more complicated to describe here):

We denote time steps as $t = 1, 2, 3, \ldots$. Markov State at time $t$ is denoted as $S_t \in \mathcal{S}$ where $\mathcal{S}$ is refered to as the *State Space* (a countable set). Action at time $t$ is denoted as $A_t \in \mathcal{A}$ where $\mathcal{A}$ is refered to as the *Action Space* (a countable set). Reward at time $t$ is denoted as $R_t \in \mathcal{D}$ where $\mathcal{D}$ is a countable subset of $\mathbb{R}$ (representing the numerical feedback served by the Environment, along with the State, at each time step $t$). 

We represent the transition probabilities from one time step to the next with the following notation:

$$p(r,s'|s,a) = \mathbb{P}[(R_{t+1}=r, S_{t+1}=s')|S_t=s,A_t=a]$$

$\gamma \in [0,1]$ is known as the discount factor used to discount Rewards when accumulating Rewards, as follows:

$$\text{Return } G_t = R_{t+1} + \gamma \cdot R_{t+2} + \gamma^2 \cdot R_{t+3} + \ldots$$

The discount factor $\gamma$ allows us to model situations where a future reward is less desirable than a current reward of the same quantity.

The goal is to find a *Policy* $\pi: \mathcal{S} \rightarrow \mathcal{A}$ that maximizes  $\mathbb{E}[G_t|S_t = s]$ for all $s \in \mathcal{S}$. In subsequent chapters, we clarify that the MDP framework actually considers more general policies than described here - policies that are stochastic, i.e., functions that take as input a state and output a probability distribution of actions (rather than a single action). However, for ease of understanding of the core concepts, in this chapter, we stick to deterministic policies $\pi: \mathcal{S} \rightarrow \mathcal{A}$.

The intuition here is that the two entities *Agent* and *Environment* interact in a time-sequenced loop wherein the *Environment* serves up next states and rewards based on the transition probability function $p$ and the *Agent* exerts control over the vagaries of $p$ by exercising the policy $\pi$ in a way that optimizes the Expected "accumulated rewards" (i.e., Expected Return) from any state.

![Baby Learning MDP \label{fig:baby_mdp}](./chapter0/BabyMDP.jpg "Baby Learning MDP")

It's worth pointing out that the MDP framework is inspired by how babies (*Agent*) learn to perform tasks (i.e., take *Action*s) in response to the random activities and events (*State*s and *Reward*s) they observe as being served up from the world (*Environment*) around them. Figure \ref{fig:baby_mdp} illustrates this - at the bottom of the Figure (labeled "World", i.e., *Environment*), we have a room in a house with a vase atop a bookcase. At the top of the Figure is a baby (learning *Agent*) on the other side of the room who wants to make her way to the bookcase, reach for the vase, and topple it - doing this efficiently (i.e., in quick time and quietly) would mean a large *Reward* for the baby. At each time step, the baby finds herself in a certain posture (eg: lying on the floor, or sitting up, or trying to walk etc.) and observes various visuals around the room - her posture and her visuals would constitute the *State* for the baby at each time step. The baby's *Action*s are various options of physical movements to try to get to the other side of the room (assume the baby is still learning how to walk). The baby tries one physical movement, but is unable to move forward with that movement. That would mean a negative *Reward* - the baby quickly learns that this movement is probably not a good idea. Then she tries a different movement, perhaps trying to stand on her feet and start walking. She makes a couple of good steps forward (positive *Reward*s), but then falls down and hurts herself (that would be a big negative *Reward*). So by trial and error, the baby learns about the consequences of different movements (different actions). Eventually, the baby learns that by holding on to the couch, she can walk across, and then when she reaches the bookcase, she learns (again by trial and error) a technique to climb the bookcase that is quick yet quiet (so she doesn't raise her mom's attention). This means the baby learns of the optimal policy (best actions for each of the states she finds herself in) after essentially what is a "trial and error" method of learning what works and what doesn't. This example is essentially generalized in the MDP framework, and the baby's "trial and error" way of learning is essentially a special case of the general technique of Reinforcement Learning.

### Real-world problems that fit the MDP framework

As you might imagine by now, all kinds of problems in nature and in business (and indeed, in our personal lives) can be modeled as Markov Decision Processes. Here is a sample of such problems:

* Self-driving vehicle (Actions constitute speed/steering to optimize safety/time).
* Game of Chess (Actions constitute moves of the pieces to optimize chances of winning the game).
* Complex Logistical Operations, such as those in a Warehouse (Actions constitute inventory movements to optimize throughput/time).
* Making a humanoid robot walk/run on a difficult terrain (Actions are walking movements to optimize time to destination).
* Manage an investment portfolio (Actions are trades to optimize long-term investment gains).
* Optimal decisions during a football game (Actions are strategic game calls to optimize chances of winning the game).
* Strategy to win an election (Actions constitute political decisions to optimize chances of winning the election).

![Self-driving Car MDP \label{fig:car_mdp}](./chapter0/CarMDP.jpg "Self-driving Car MDP")

Figure \ref{fig:car_mdp} illustrates the MDP for a self-driving car. At the top of the figure is the *Agent* (the car's driving algorithm) and at the bottom of the figure is the *Environment* (constituting everything the car faces when driving - other vehicles, traffic signals, road conditions, weather etc.). The *State* consists of the car's location, velocity, and all of the information picked up by the car's sensors/cameras. The *Action* consists of the steering, acceleration and brake. The *Reward* would be a combination of metrics on ride comfort and safety, as well as the negative of each time step (because maximizing the accumulated Reward would then amount to minimizing time taken to reach the destination).

### The inherent difficulty in solving MDPs

"Solving" an MDP refers to identifying the optimal policy with an algorithm. This section paints an intuitive picture of why solving a general MDP is fundamentally a hard problem. Often, the challenge is simply that the *State Space* is very large (involving many variables) or complex (elaborate data structure), and hence, is computationally intractable. Likewise, sometimes the *Action Space* can be quite large or complex. 

But the main reason for why solving an MDP is inherently difficult is the fact that there is no direct feedback on what the "correct" Action is for a given State. What we mean by that is that unlike a supervised learning framework, the MDP framework doesn't give us anything other than a *Reward* feedback to indicate if an Action is the *right one* or not. A large Reward might encourage the Agent, but it's not clear if one just got lucky with the large Reward or if there could be an even larger Reward if the Agent tries the Action again. The linkage between Actions and Rewards is further complicated by the fact that there is time-sequenced complexity in an MDP, meaning an Action can influence future States, which in turn influences future Actions. Consequently, we sometimes find that Actions can have delayed consequences, i.e., the Rewards for a good Action might come after many time steps (eg: in a game of Chess, a brilliant move leads to a win after several further moves). 

The other problem one encounters in real-world situations is that the Agent often doesn't know the *Model* of the *Environment*. By *Model*, we are refering to the probabilities of state-transitions and rewards, i.e., the function $p$ we defined above. This means the Agent has to simultaneously learn the Model (from the real-world data stream) and solve for the Optimal Policy. 

Lastly, when there are many actions, the Agent needs to try them all to check if there are some hidden gems (great actions that haven't been tried yet), which in turn means one could end up wasting effort on "duds" (bad actions). So the agent has to find the balance between *exploitation* (retrying actions which have yielded good rewards so far) and *exploration* (trying actions that have either not been tried enough or not been tried at all).

All of this seems to indicate that we don't have much hope in solving MDPs in a reliable and efficient manner. But it turns out that with some clever mathematics, we can indeed make some good inroads. We outline the core idea of this "clever mathematics" in the next section.

### Value Function, Bellman Equation, Dynamic Programming and RL

Perhaps the most important concept we want to highlight in this entire book is the idea of a *Value Function* and how
we can compute it in an efficient manner with either Planning or Learning algorithms. The Value Function $V^{\pi}: \mathcal{S} \rightarrow \mathbb{R}$ for a given policy $\pi$ is defined as:

$$V^{\pi}(s) = \mathbb{E}[G_t|S_t = s] \text{ for all } s \in \mathcal{S}$$

The intuitive way to understand Value Function is that it tells us how much "accumulated future reward" (i.e., *Return*) we expect to obtain from a given state. The randomness under the expectation comes from the uncertain future states and rewards the Agent is going to see upon taking future actions prescribed by the policy $\pi$. The key in evaluating the Value Function for a given policy is that it can be expressed recursively, in terms of the Value Function for the next time step's states. In other words, 
$$V^{\pi}(s) = \sum_{r,s'} p(r,s'|s,\pi(s)) \cdot (r + \gamma \cdot V^{\pi}(s')) \text{ for all } s \in \mathcal{S}$$
This equation says that when the Agent is in a given state $s$, it takes an action $a=\pi(s)$, then sees a random next state $s'$ and a random reward $r$, so $V^{\pi}(s)$ can be broken into the expectation of $r$ (first step's expected reward) and the remainder of the future expected accumulated rewards (which can be written in terms of the expectation of $V^{\pi}(s')$). We won't get into the details of how to solve this recursive formulation in this chapter (will cover this in great detail in future chapters), but it's important for you to recognize for now that this recursive formulation is the key to evaluating the Value Function for a given policy.

However, evaluating the Value Function for a given policy is not the end goal - it is simply a means to the end goal of evaluating the *Optimal Value Function* (from which we obtain the *Optimal Policy*). The Optimal Value Function $V^*: \mathcal{S} \rightarrow \mathbb{R}$ is defined as:
$$V^*(s) = \max_{\pi} V^{\pi}(s) \text{ for all } s \in \mathcal{S}$$

The good news is that even the Optimal Value Function can be expressed recursively, as follows:

$$V^*(s) = \max_{a} \sum_{r,s'} p(r,s'|s,a) \cdot (r + \gamma \cdot V^*(s')) \text{ for all } s \in \mathcal{S}$$

Furthermore, we can prove that there exists an Optimal Policy $\pi^*$ achieving $V^*(s)$ for all $s \in \mathcal{S}$ (the proof is constructive, which gives a simple method to obtain the function $\pi^*$ from the function $V^*$). Specifically, this means that the Value Function obtained by following the optimal policy $\pi^*$ is the same as the Optimal Value Function $V^*$, i.e.,

$$V^{\pi^*}(s) = V^*(s) \text{ for all } s \in \mathcal{S}$$

There is a bit of terminology here to get familiar with. The problem of calculating $V^{\pi}(s)$ (Value Function for a give policy) is known as the *Prediction* problem (since this amounts of statistical estimation of the expected returns from any given state when following a policy $\pi$). The problem of calculating the Optimal Value Function $V^*$ (and hence, Optimal Policy $\pi^*$), is known as the *Control* problem (since this requires steering of the policy such that we obtain the maximum expected return from any state). Solving the Prediction problem is typically a stepping stone towards solving the (harder) problem of Control. These recursive equations for $V^{\pi}$ and $V^*$ are known as the (famous) Bellman Equations (which you will hear a lot about in future chapters). In a continuous-time formulation, the Bellman Equation is refered to as the famous *Hamilton-Jacobi-Bellman (HJB)* equation.

The algorithms to solve the prediction and control problems based on Bellman equations are broadly classified as:

* Dynamic Programming, a class of (in the language of A.I.) *Planning* algorithms.
* Reinforcement Learning, a class of (in the language of A.I.) *Learning* algorithms.

Now let's talk a bit about the difference between Dynamic Programming and Reinforcement Learning algorithms. Dynamic Programming algorithms (which we cover a lot of in this book) assume that the agent knows of the transition probabilities $p$ and the algorithm takes advantage of the knowledge of those probabilities (leveraging the Bellman Equation to efficiently calculate the Value Function). Dynamic Programming algorithms are considered to be *Planning* and not *Learning* (in the language of A.I.) because the algorithm doesn't need to interact with the Environment and doesn't need to learn from the (states, rewards) data stream coming from the Environment. Rather, armed with the transition probabilities, the algorithm can reason about future probabilistic outcomes and perform the requisite optimization calculation to infer the Optimal Policy. So it *plans* it's path to success, rather than *learning* about how to succeed.

However, in typical real-word situations, one doesn't really know the transition probabilities $p$. This is the realm of Reinforcement Learning (RL). RL algorithms interact with the Environment, learn with each new (state, reward) pair received from the Environment, and incrementally figure out the Optimal Value Function (with the "trial and error" approach that we outlined earlier). However, note that the Environment interaction could be *real* interaction or *simulated* interaction. In the latter case, we do have a model of the transitions but the structure of the model is so complex that we only have access to samples of the next state and reward (rather than an explicit representation of the probabilities). This is known as a *Sampling Model* of the Environment. With access to such a sampling model of the environment (eg: a robot learning on a simulated terrain), we can employ the same RL algorithm that we would have used when interacting with a real environment (eg: a robot learning on an actual terrain). In fact, most RL algorithms in practice learn from simulated models of the environment. As we explained earlier, RL is essentially a "trial and error" learning approach and hence, is quite laborious and fundamentally inefficient. The recent progress in RL is coming from more efficient ways of learning the Optimal Value Function, and better ways of approximating the Optimal Value Function. One of the key challenges for RL in the future is to identify better ways of finding the balance between "exploration" and "exploitation" of actions. In any case, one of the key reasons RL has started doing well lately is due to the assistance it has obtained from Deep Learning (typically Deep Neural Networks are used to approximate the Value Function and/or to approximate the Value Function). RL with such deep learning approximations is known by the catchy modern term *Deep RL*.

We believe the current promise of A.I. is dependent on the success of Deep RL. The next decade will be exciting as RL research will likely yield improved algorithms and it's pairing with Deep Learning will hopefully enable us to solve fairly complex real-world stochastic control problems. 

### Outline of Chapters

The chapters in this book are organized into 3 modules as follows:

* Module I: Processes and Planning Algorithms (Chapters 1-4)
* Module II: Modeling Financial Applications (Chapters 5-8)
* Module III: Reinforcement Learning Algorithms (Chapters 9-15)

Before covering the contents of the chapters in these 3 modules, the book starts with 2 unnumbered chapters. The first of these unnumbered chapters is *this chapter* (the one you are reading) which serves as an *Overview*, covering the pedagogical aspects of learning RL (and more generally Applied Math), outline of the learnings to be acquired from this book, background required to read this book, a high-level overview of Stochastic Control, MDP, Value Function, Bellman Equation and RL, and finally the outline of chapters in this book. The second unnumbered chapter is called *Design Paradigms for Applied Mathematics implementations in Python*. Since this book makes heavy use of Python code for developing mathematical models and for algorithms implementations, we cover the requisite Python background (specifically the design paradigms we use in our Python code) in this chapter. To be clear, this chapter is not a full Python tutorial – the reader is expected to have some background in Python already. It is a tutorial of some key techniques and practices in Python (that many readers of this book might not be accustomed to) that we use heavily in this book and that are also highly relevant to programming in the broader area of Applied Mathematics. We cover the topics of Type Annotations, List and Dict Comprehensions, Functional Programming, Interface Design with Abstract Base Classes, Generics Programming and Generators.

The remaining chapters in this book are organized in the 3 modules we listed above.

#### Module I: Processes and Planning Algorithms

The first module of this book covers the theory of Markov Decision Processes (MDP), Dynamic Programming (DP) and Approximate Dynamics Programming (ADP) across Chapters 1-4.

In order to understand the MDP framework, we start  with the foundations of *Markov Processes* (sometimes refered to as Markov Chains) in Chapter [-@sec:mrp-chapter]. Markov Processes do not have any Rewards or Actions, they only have states and states transitions. We believe spending a lot of time on this simplified framework of Markov Processes is excellent preparation before getting to MDPs. Chapter [-@sec:mrp-chapter] then builds upon Markov Processes to include the concept of Reward (but not Action) - the inclusion of Reward yields a framework known as Markov Reward Process. With Markov Reward Processes, we can talk about Value Functions and Bellman Equation, which serve as great preparation for understanding Value Function and Bellman Equation later in the context of MDPs. Chapter [-@sec:mrp-chapter] motivates these concepts with examples of stock prices and with a simple inventory example that serves first as a Markov Process and then as a Markov Reward Process. There is also a significant amount of programming content in this chapter to develop comfort as well as depth in these concepts.

Chapter [-@sec:mdp-chapter] on *Markov Decision Processes* lays the foundational theory underpinning RL – the framework for representing problems dealing with sequential optimal decisioning under uncertainty (Markov Decision Process). You will learn about the relationship between Markov Decision Processes and Markov Reward Processes, about the Value Function and the Bellman Equations. Again, there is a considerable amount of programming exercises in this chapter. The heavy investment in this theory together with hands-on programming will put you in a highly advantaged position to learn the following chapters in a very clear and speedy manner.

Chapter [-@sec:dp-chapter] on *Dynamic Programming* covers the Planning technique of Dynamic Programming (DP), which is an important class of foundational algorithms that can be an alternative to RL if the MDP is not too large or too complex. Also, learning these algorithms provides important foundations to be able to understand subsequent RL algorithms more deeply. You will learn about several important DP algorithms by the end of the chapter and you will learn about why DP gets difficult in practice which draws you to the motivation behind RL. Again, we cover plenty of programming exercises that are quick to implement and will aid considerably in internalizing the concepts. Finally, we emphasize a special algorithm - Backward Induction - for solving finite-horizon Markov Decision Processes, which is the setting for the financial applications we cover in this book.

The Dynamic Programming algorithms covered in Chapter [-@sec:dp-chapter] suffer from the two so-called curses: Curse of Dimensionality and Curse of Modeling. These curses can be cured with a combination of sampling and function approximation. Module III covers the sampling cure (using Reinforcement Learning). Chapter [-@sec:funcapprox-chapter] on *Function Approximation and Approximate Dynamic Programming* covers the topic of function approximation and shows how an intermediate cure - Approximate Dynamic Programming (function approximation without sampling) - is often quite viable and can be suitable for some problems. As part of this chapter, we implement linear function approximation and approximation with deep neural networks (forward and back propagation algorithms) so we can use these approximations in Approximate Dynamic Programming algorithms and later also in RL.

#### Module II: Modeling Financial Applications

The second module of this book covers the background on Utility Theory and 5 financial applications of Stochastic Control across Chapter 5-8.

We begin this module with Chapter [-@sec:utility-theory-chapter] on *Utility Theory* which covers a very important Economics concept that is a pre-requisite for most of the Financial Applications we cover in subsequent chapters. This is the concept of risk-aversion (i.e., how people want to be compensated for taking risk) and the related concepts of risk-premium and Utility functions. The remaining chapters in this module cover not only the 5 financial applications, but also great detail on how to model them as MDPs, develop DP/ADP algorithms to solve them, and write plenty of code to implement the algorithms, which helps internalize the learnings quite well. Note that in practice these financial applications can get fairly complex and DP/ADP algorithms don't quite scale, which means we need to tap into RL algorithms to solve them. So we revisit these financial applications in Module III when we cover RL algorithms.

Chapter [-@sec:portfolio-chapter] is titled *Dynamic Asset Allocation and Consumption*. This chapter covers the first of the 5 Financial Applications. This problem is about how to adjust the allocation of one's wealth to various investment choices in response to changes in financial markets. The problem also involves how much wealth to consume in each interval over one's lifetime so as to obtain the best utility from wealth consumption. Hence, it is the joint problem of (dynamic) allocation of wealth to financial assets and appropriate consumption of one's wealth over a period of time. This problem is best understood in the context of Merton’s landmark paper in 1969 where he stated and solved this problem. This chapter is mainly focused on the mathematical derivation of Merton’s solution of this problem with Dynamic Programming. You will also learn how to solve the asset allocation problem in a simple setting with Approximate Backward Induction (an ADP algorithm covered in Chapter [-@sec:funcapprox-chapter]). 

Chapter [-@sec:derivatives-pricing-chapter] covers a very important topic in Mathematical Finance: *Pricing and Hedging of Derivatives*. Full and rigorous coverage of derivatives pricing and hedging is a fairly elaborate and advanced topic, and beyond the scope of this book. But we have provided a way to understand the theory by considering a very simple setting - that of a single-period with discrete outcomes and no provision for rebalancing of the hedges, that is typical in the general theory. Following the coverage of the foundational theory, we cover the problem of optimal pricing/hedging of derivatives in an *incomplete market* and the problem of optimal exercise of American Options (both problems are modeled as MDPs). In this chapter, you will learn about some highly important financial foundations such as the concepts of arbitrage, replication, market completeness, and the all-important risk-neutral measure. You will learn the proofs of the two fundamental theorems of asset pricing in this simple setting. We also provide an overview of the general theory (beyond this simple setting). Next you will learn about how to price/hedge derivatives incorporating real-world frictions by modeling this problem as an MDP. In the final module of this chapter, you will learn how to model the more general problem of optimal stopping as an MDP. You will learn how to use Backward Induction (a DP algorithm we learnt in Chapter [-@sec:dp-chapter]) to solve this problem when the state-space is not too big. By the end of this chapter, you would have developed significant expertise in pricing and hedging complex derivatives, a skill that is in high demand in the finance industry.

Chapter [-@sec:order-book-algos-chapter] on *Order-Book Algorithms* covers the remaining two Financial Applications, pertaining to the world of Algorithmic Trading. The current practice in Algorithmic Trading is to employ techniques that are rules-based and heuristic. However, Algorithmic Trading is quickly transforming into Machine Learning-based Algorithms. In this chapter, you will be first introduced to the mechanics of trade order placements (market orders and limit orders), and then introduced to a very important real-world problem – how to submit a large-sized market order by splitting the shares to be transacted and timing the splits optimally in order to overcome “price impact” and gain maximum proceeds. You will learn about the classical methods based on Dynamic Programming. Next you will learn about the market frictions and the need to tackle them with RL. In the second half of this chapter, we cover the Algorithmic-Trading twin of the Optimal Execution problem – that of a market-maker having to submit dynamically-changing bid and ask limit orders so she can make maximum gains. You will learn about how market-makers (a big and thriving industry) operate. Then you will learn about how to formulate this problem as an MDP. We will do a thorough coverage of the classical Dynamic Programming solution by Avellaneda and Stoikov. Finally, you will be exposed to the real-world nuances of this problem, and hence, the need to tackle with a market-calibrated simulator and RL. 

#### Module III: Reinforcement Learning Algorithms

The third module of this book covers Reinforcement Learning algorithms across Chapter 9-15.
 
Chapter [-@sec:rl-prediction-chapter] on *Monte-Carlo and Temporal-Difference methods for Prediction* starts a new phase in this book - our entry into the world of RL algorithms. To understand the basics of RL, we start this chapter by restricting the RL problem to a very simple one – one where the state space is small and manageable as a table enumeration (known as tabular RL) and one where we only have to calculate the Value Function for a Fixed Policy (this problem is known as the Prediction problem, versus the optimization problem which is known as the Control problem). The restriction to Tabular Prediction is important because it makes it much easier to understand the core concepts of Monte-Carlo (MC) and Temporal-Difference (TD) in this simplified setting. The later part of this chapter extends Tabular Prediction to Prediction with Function Approximation (leveraging the function approximation foundations we had developed in Chapter [-@sec:funcapprox-chapter] in the context of ADP). The remaining chapters will build upon this chapter by adding more complexity and more nuances, while retaining much of the key core concepts developed in this chapter. As ever, you will learn by coding plenty of MC and TD algorithms from scratch.

Chapter [-@sec:rl-control-chapter] on *Monte-Carlo and Temporal-Difference for Control* makes the natural extension from Prediction to Control, while initially remaining in the tabular setting. The investments made in understanding the core concepts of MC and TD in Chapter [-@sec:rl-prediction-chapter] bear fruit here as important Control Algorithms such as SARSA and Q-learning can now be learnt with enormous clarity. In this chapter, we implement both SARSA and Q-Learning from scratch in Python. This chapter also introduces a very important concept for the future success of RL in the real-world: off-policy learning (Q-Learning is the simplest off-policy learning algorithm and it has had good success in various applications). The later part of this chapter extends Tabular Control to Control with Function Approximation (leveraging the function approximation foundations we had developed in Chapter [-@sec:funcapprox-chapter]).

Chapter [-@sec:batch-rl-chapter] on *Experience Replay, Least-Squares Policy Iteration and Gradient TD* moves on from basic and more traditional RL algorithms to recent innovations in RL. We start this chapter with the important idea of *Experience Replay* which makes more efficient use of data by storing data as it comes and re-using it throughout the learning process of the algorithm. We also emphasize a simple but important linear function approximation algorithm that learns and improves through batches of data (versus the traditional algorithms that learn incrementally upon each new piece of data) - this *Batch Algorithm* is known as Least-Squares Temporal Difference (to solve the Prediction problem) and it's extension to solve the Control problem is known as Least-Squares Policy Iteration. We will discuss these algorithms in the context of Financial Applications that were covered in Module II. In the later part of this chapter, we provide deeper insights into the core mathematics underpinning RL algorithms (back to the basics of Bellman Equation). Understanding *Value Function Geometry* will place you in a highly advantaged situation in terms of truly understanding what is it that makes some Algorithms succeed in certain situations and fail in other situations. This chapter also explains how to break out of the so-called Deadly Triad (when bootstrapping, function approximation and off-policy are employed together, RL algorithms tend to fail). The state-of-the-art Gradient TD Algorithm resists the deadly triad and we dive deep into its inner workings to understand how and why.

Chapter [-@sec:policy-gradient-chapter] on *Policy Gradient Algorithms* introduces a very different class of RL algorithms that are based on improving the policy using the gradient of the policy function approximation (rather than the usual policy improvement based on explicit argmax on Q-Value Function). When action spaces are large or continuous, Policy Gradient tends to be the only option and so, this chapter is useful to overcome many real-world challenges (including those in many financial applications) where the action space is indeed large. You will learn about the mathematical proof of the elegant Policy Gradient Theorem and implement a couple of Policy Gradient Algorithms from scratch. You will learn about state-of-the-art Actor-Critic methods. Lastly, you will also learn about Evolutionary Strategies, an algorithm that looks quite similar to Policy Gradient Algorithms, but is technically not an RL Algorithm. However, learning about Evolutionary Strategies is important because some real-world applications, including Financial Applications, can indeed be tackled well with Evolutionary Strategies.

Chapter [-@sec:learning-vs-planning-chapter] on *Learning versus Planning* brings the various pieces of Planning and Learning concepts learnt in this book together. You will learn that in practice, one needs to be creative about blending planning and learning concepts (a technique known as Model-based RL). In practice, many problems are indeed tackled using Model-based RL. You will also get familiar with an algorithm (Monte Carlo Tree Search) that was highly popularized when it solved the Game of GO, a problem that was thought to be insurmountable by present AI technology.

Chapter [-@sec:multi-armed-bandits-chapter] on *Multi-Armed Bandits: Exploration versus Exploitation* is a deep-dive into the topic of balancing exploration and exploitation, a topic of great importance in RL algorithms. Exploration versus Exploitation is best understood in the simpler setting of the Multi-Armed Bandit (MAB) problem. You will learn about various state-of-the-art MAB algorithms, implement them in Python, and draw various graphs to understand how they perform versus each other in various problem settings. You will then be exposed to Contextual Bandits which is a popular approach in optimizing choices of Advertisement placements. Finally, you will learn how to apply the MAB algorithms within RL.

Chapter [-@sec:concluding-chapter] is the concluding chapter titled *RL in Real-World Finance: Reality versus Hype, Present versus Future*. The purpose of this chapter is to put the entire book’s content into perspective relative to the current state of the financial industry, and the practical challenges in adoption of RL. We also provide some guidance on how to go about building an end-to-end system for financial applications based on RL. The reader will be guided on reality versus hype in the current “AI-First” landscape. You will also gain a perspective of where RL stands today and what the future holds.

#### Short Appendix Chapters

Finally, we have 6 short Appendix chapters at the end of this book. The first appendix is on *Moment Generating Functions* and it's use in various calculations across this book. The second appendix is a technical perspective of *Function Approximations as Vector Spaces*, which helps develop a deeper mathematical understanding of function approximations. The second appendix is on *Portfolio Theory* covering the mathematical foundations of balancing return versus risk in portfolios and the much-celebrated Capital Asset Pricing Model (CAPM). The third appendix covers the basics of *Stochastic Calculus* as we need some of this theory (*Ito Integral*, *Ito's Lemma* etc.) in the derivations in a couple of the chapters in Module II. The fourth appendix is on the *HJB Equation*, which as a key part of the derivation of the closed-form solutions for 2 of the 5 financial applicatons we cover in Module II. The fifth and final appendix covers the derivation of the famous *Black-Scholes Equation* (and it's solution for Call/Put Options).
