{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ReadMe**\n",
        "\n",
        "The question 1 is answered in part 3.\n",
        "\n",
        "The question 2 is answered in part 2.\n",
        "\n",
        "The question 3 is answered in part 1.\n",
        "\n",
        "The question 4 is answered in part 2."
      ],
      "metadata": {
        "id": "9zY0nC5dRQZo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ7fOoXubGwK",
        "outputId": "7547ed46-41af-44c8-f598-2a961b929d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Winter 2022/CME241\n"
          ]
        }
      ],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import sys\n",
        "%cd /content/drive/My\\ Drive/Winter\\ 2022/CME241"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r394i0lObkhY"
      },
      "outputs": [],
      "source": [
        "from rl.markov_process import *\n",
        "from rl.chapter2.simple_inventory_mrp import *\n",
        "from rl.distribution import *\n",
        "from rl.td_lambda import *\n",
        "from rl.function_approx import *\n",
        "from typing import Tuple, Sequence, Iterator, List, Dict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcY29EPnRizD"
      },
      "source": [
        "#1. MC Error \\& TD Error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider that, by definition,\n",
        "\\begin{align*}\n",
        "  G_t = R_{t+1} + \\gamma \\cdot G_{t+1}\n",
        "\\end{align*}\n",
        "\n",
        "Therefore, \n",
        "\\begin{align*}\n",
        "  G_t - V(S_t) &= R_{t+1} + \\gamma \\cdot (G_{t+1} - V(S_{t+1})) + \\gamma \\cdot V(S_{t+1}) - V(S_t)\\\\\n",
        "  &= \\gamma \\cdot (G_{t+1} - V(S_{t+1}) + (R_{t+1}+ \\gamma \\cdot V(S_{t+1}) - V(S_t))\n",
        "\\end{align*}\n",
        "\n",
        "We can then summation on both side of the equation above with the multiplication of $\\gamma^t$ and get that\n",
        "\n",
        "\\begin{align*}\n",
        "  \\sum_{u=t}^{T-1} (\\gamma^{u}(G_u - V(S_u))) &= \\sum_{u=t}^{T-1} (\\gamma^{u+1} \\cdot (G_{u+1} - V(S_{u+1}) + \\gamma^{u} \\cdot (R_{u+1}+ \\gamma \\cdot V(S_{u+1}) - V(S_u)))\\\\\n",
        "  &= \\sum_{u=t+1}^{T} (\\gamma^{u+1} \\cdot (G_{u} - V(S_{u})) + \\sum_{u=t}^{T-1} (\\gamma^{u} \\cdot (R_{u+1}+ \\gamma \\cdot V(S_{u+1}) - V(S_u)))\n",
        "\\end{align*}\n",
        "\n",
        "Therefore,\n",
        "\n",
        "\\begin{align*}\n",
        "  \\gamma^{t}(G_t - V(S_t)) - \\gamma^{T}(G_T - V(S_T)) &= \\sum_{u=t}^{T-1} (\\gamma^{u} \\cdot (R_{u+1}+ \\gamma \\cdot V(S_{u+1}) - V(S_u)))\\\\\n",
        "  (G_t - V(S_t)) - \\gamma^{T-t}(G_T - V(S_T)) &= \\sum_{u=t}^{T-1} (\\gamma^{u-t} \\cdot (R_{u+1}+ \\gamma \\cdot V(S_{u+1}) - V(S_u)))\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "jpiCRV2OPprI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If $T-t$ is sufficeintly high, $\\gamma^{T-t}(G_T - V(S_T)) \\to 0$, so\n",
        "\\begin{align*}\n",
        "  G_t - V(S_t))&= \\sum_{u=t}^{T-1} (\\gamma^{u-t} \\cdot (R_{u+1}+ \\gamma \\cdot V(S_{u+1}) - V(S_u)))\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "xbcRtM5aUn_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. TD(λ)"
      ],
      "metadata": {
        "id": "mwpQOifjaVPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create MRP"
      ],
      "metadata": {
        "id": "rhorfCwYImPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_capacity = 2\n",
        "user_poisson_lambda = 1.0\n",
        "user_holding_cost = 1.0\n",
        "user_stockout_cost = 10.0\n",
        "user_gamma = 0.9\n",
        "\n",
        "si_mrp = SimpleInventoryMRPFinite(\n",
        "        capacity=user_capacity,\n",
        "        poisson_lambda=user_poisson_lambda,\n",
        "        holding_cost=user_holding_cost,\n",
        "        stockout_cost=user_stockout_cost\n",
        "    )"
      ],
      "metadata": {
        "id": "rJ7-LsaNIZkw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the idea of constructing MC, we have to keep track of memory. In TD($λ$), the every memory will be updated at every timestep (by reducing the moemory of the state not occuring and increase the memories of the current state.) One can think of the idea of using decaying memories as a way to incorporate momentum."
      ],
      "metadata": {
        "id": "CexQnAl0OTP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize value of each non-terminal state as a dict\n",
        "V_td_lambda_tabular = dict()\n",
        "memory_td_lambda_tabular = dict()\n",
        "for non_terminal_state in si_mrp.non_terminal_states:\n",
        "  V_td_lambda_tabular[non_terminal_state] = 0\n",
        "  memory_td_lambda_tabular[non_terminal_state] = 0"
      ],
      "metadata": {
        "id": "wwPZL2x8O82Q"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us first consider the case of using only one episode"
      ],
      "metadata": {
        "id": "n_cNm7hzUZYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_trace = next(si_mrp.reward_traces(Choose(si_mrp.non_terminal_states)))"
      ],
      "metadata": {
        "id": "MFXYlX6UPliV"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify lambda\n",
        "lambdaa = 0.8\n",
        "\n",
        "#Specify learning rate\n",
        "alpha = 0.5\n",
        "\n",
        "count = 0\n",
        "for step in reward_trace:\n",
        "  count += 1\n",
        "  if count > 1e4:\n",
        "    break\n",
        "  #Update the memory\n",
        "  for non_terminal_state in si_mrp.non_terminal_states:\n",
        "    memory_td_lambda_tabular[non_terminal_state] = lambdaa * memory_td_lambda_tabular[non_terminal_state]\n",
        "  memory_td_lambda_tabular[step.state] += 1\n",
        "  #Update the value\n",
        "  V_td_lambda_tabular[step.state] = V_td_lambda_tabular[step.state] + alpha * (step.reward + user_gamma * V_td_lambda_tabular[step.next_state] - V_td_lambda_tabular[step.state]) * memory_td_lambda_tabular[step.state] "
      ],
      "metadata": {
        "id": "eMq3xe_zPJuh"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afterwards, we can stack everything together."
      ],
      "metadata": {
        "id": "IVMu1iRQUzcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify alpha (learning rate)\n",
        "alpha = 0.7\n",
        "\n",
        "#Specify lambda\n",
        "lambdaa = 0.6\n",
        "\n",
        "#Sepecify number of episode we are given\n",
        "num_episodes = 1000\n",
        "\n",
        "#Specify max steps per episode to be used\n",
        "max_steps = round(np.log(1e-6) / np.log(user_gamma)) \n",
        "\n",
        "#Create reward traces\n",
        "reward_traces_gen = si_mrp.reward_traces(Choose(si_mrp.non_terminal_states))\n",
        "\n",
        "#Initialize value of each non-terminal state as a dict\n",
        "V_td_lambda_tabular = dict()\n",
        "memory_td_lambda_tabular = dict()\n",
        "for non_terminal_state in si_mrp.non_terminal_states:\n",
        "  V_td_lambda_tabular[non_terminal_state] = 0\n",
        "  memory_td_lambda_tabular[non_terminal_state] = 0\n",
        "\n",
        "for _ in range(num_episodes): \n",
        "  #Get episode\n",
        "  reward_trace = next(reward_traces_gen)\n",
        "  for _ in range(max_steps):\n",
        "    step = next(reward_trace)\n",
        "    #Update the memory\n",
        "    for non_terminal_state in si_mrp.non_terminal_states:\n",
        "      memory_td_lambda_tabular[non_terminal_state] = lambdaa * memory_td_lambda_tabular[non_terminal_state]\n",
        "    memory_td_lambda_tabular[step.state] += 1\n",
        "    #Update the current state value\n",
        "    V_td_lambda_tabular[step.state] = V_td_lambda_tabular[step.state] + alpha * (step.reward + user_gamma * V_td_lambda_tabular[step.next_state] - V_td_lambda_tabular[step.state]) * memory_td_lambda_tabular[step.state] "
      ],
      "metadata": {
        "id": "QkaXhZ-YTZ9v"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V_td_lambda_tabular"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jiT8b3CU3bt",
        "outputId": "0d8bcd87-d573-4cbe-e109-dc4a6096f073"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -31.462943753107556,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -20.057778487655405,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -23.995989615702687,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -24.073713587096016,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -23.24526764347817,\n",
              " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -24.555340109677985}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reward_trace = next(td_lambda_prediction(si_mrp.reward_traces(Choose(si_mrp.non_terminal_states)), Tabular(), user_gamma, 0.5))"
      ],
      "metadata": {
        "id": "2LTTMPoRUyZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "td_lambda_Vs = td_lambda_prediction(si_mrp.reward_traces(Choose(si_mrp.non_terminal_states)), Tabular(), user_gamma, 0.6)"
      ],
      "metadata": {
        "id": "wDRLyxJoWZZz"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(num_episodes*max_steps):\n",
        "  next(td_lambda_Vs)\n",
        "td_lambda_V = next(td_lambda_Vs)"
      ],
      "metadata": {
        "id": "rAl5ZGRHWgEp"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "td_lambda_V.values_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBzBMVZ0XtPc",
        "outputId": "0237067a-fda9-42d1-f13c-099e915edf99"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -31.33172395881345,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -23.734365072784943,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -24.158251300454612,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -24.73152589556043,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -25.13808084425424,\n",
              " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -26.14869634472357}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. n-Step Bootstraping"
      ],
      "metadata": {
        "id": "v_epUSBdVBmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_capacity = 2\n",
        "user_poisson_lambda = 1.0\n",
        "user_holding_cost = 1.0\n",
        "user_stockout_cost = 10.0\n",
        "user_gamma = 0.9\n",
        "\n",
        "si_mrp = SimpleInventoryMRPFinite(\n",
        "        capacity=user_capacity,\n",
        "        poisson_lambda=user_poisson_lambda,\n",
        "        holding_cost=user_holding_cost,\n",
        "        stockout_cost=user_stockout_cost\n",
        "    )"
      ],
      "metadata": {
        "id": "a4wLs2BaPWWd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first look into the case of 1 episode"
      ],
      "metadata": {
        "id": "oPIs9CqPQkrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_traces = si_mrp.reward_traces(Choose(si_mrp.non_terminal_states))\n",
        "episode = next(reward_traces)"
      ],
      "metadata": {
        "id": "Ota22gxUQkMx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to create a queue of state and a queue of reward to store the dynamics. \n",
        "\n",
        "We also have to create dictionary of V as before, but we do not have to have a memory dictionary like in MC or TD(λ), since all memory that we will use will have already been stored in the queues."
      ],
      "metadata": {
        "id": "rv5EDKzJQqqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify the learning rate\n",
        "alpha = 0.6\n",
        "\n",
        "#Initialize value of each non-terminal state as a dict\n",
        "V_n_step_tabular = dict()\n",
        "for non_terminal_state in si_mrp.non_terminal_states:\n",
        "  V_n_step_tabular[non_terminal_state] = 1"
      ],
      "metadata": {
        "id": "DUTDYx2nZJvY"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize a queue of state and a queue of reward\n",
        "state_queue = list()\n",
        "reward_queue = list()"
      ],
      "metadata": {
        "id": "iYPUORVjZPHq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simplicity, we will first look into the case of n = 2 for the sake of simplicity."
      ],
      "metadata": {
        "id": "JGmD5RX_RTX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 2\n",
        "\n",
        "#Fill up the queues to be of length n\n",
        "while len(state_queue) < n:\n",
        "  step = next(episode)\n",
        "  state_queue.append(step.state)\n",
        "  reward_queue.append(step.reward)"
      ],
      "metadata": {
        "id": "5GhGwLBtPc24"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we look into what we want to update at the next step"
      ],
      "metadata": {
        "id": "0YBt64AGsxkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Find G\n",
        "G = V_n_step_tabular[step.next_state]\n",
        "for i in range(n):\n",
        "  G *= user_gamma\n",
        "  G += reward_queue[-i-1]\n",
        "\n",
        "#Update the value function\n",
        "V_n_step_tabular[state_queue[0]] += alpha * (G-V_n_step_tabular[state_queue[0]])\n",
        "\n",
        "#Update the queue\n",
        "\n",
        "#Remove old state\n",
        "state_queue.pop(0)\n",
        "reward_queue.pop(0)\n",
        "\n",
        "#Observe the next step and append new state\n",
        "step = next(episode)\n",
        "state_queue.append(step.state)\n",
        "reward_queue.append(step.reward)"
      ],
      "metadata": {
        "id": "LxPca4l-s5h4"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can stack them together."
      ],
      "metadata": {
        "id": "oCAaY2ZZzhcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create MRP and specify gamma\n",
        "user_capacity = 2\n",
        "user_poisson_lambda = 1.0\n",
        "user_holding_cost = 1.0\n",
        "user_stockout_cost = 10.0\n",
        "user_gamma = 0.9\n",
        "\n",
        "si_mrp = SimpleInventoryMRPFinite(\n",
        "        capacity=user_capacity,\n",
        "        poisson_lambda=user_poisson_lambda,\n",
        "        holding_cost=user_holding_cost,\n",
        "        stockout_cost=user_stockout_cost\n",
        "    )\n",
        "\n",
        "reward_traces = si_mrp.reward_traces(Choose(si_mrp.non_terminal_states))\n",
        "\n",
        "#Specify num_epoch and num_step\n",
        "num_epoch = 500\n",
        "num_step = 200\n",
        "\n",
        "#Specify learning rate\n",
        "alpha = 0.6\n",
        "\n",
        "#Specify n\n",
        "n = 8\n",
        "\n",
        "#Initialize value of each non-terminal state as a dict\n",
        "V_n_step_tabular = dict()\n",
        "for non_terminal_state in si_mrp.non_terminal_states:\n",
        "  V_n_step_tabular[non_terminal_state] = 1\n",
        "\n",
        "for _ in range(num_epoch):\n",
        "  episode = next(reward_traces)\n",
        "  #Initialize a queue of state and a queue of reward\n",
        "  state_queue = list()\n",
        "  reward_queue = list()\n",
        "\n",
        "  while len(state_queue) < n:\n",
        "    step = next(episode)\n",
        "    state_queue.append(step.state)\n",
        "    reward_queue.append(step.reward)\n",
        "\n",
        "  for _ in range(num_step):\n",
        "    #Find G\n",
        "    G = V_n_step_tabular[step.next_state]\n",
        "    for i in range(n):\n",
        "      G *= user_gamma\n",
        "      G += reward_queue[-i-1]\n",
        "\n",
        "    #Update the value function\n",
        "    V_n_step_tabular[state_queue[0]] += alpha * (G-V_n_step_tabular[state_queue[0]])\n",
        "\n",
        "    #Update the queue\n",
        "\n",
        "    #Remove old state\n",
        "    state_queue.pop(0)\n",
        "    reward_queue.pop(0)\n",
        "\n",
        "    #Observe the next step and append new state\n",
        "    step = next(episode)\n",
        "    state_queue.append(step.state)\n",
        "    reward_queue.append(step.reward)"
      ],
      "metadata": {
        "id": "J4qY1ROis-Hg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V_n_step_tabular"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z34CZK6dws12",
        "outputId": "417e10ae-a086-4a7a-e5ea-25da5c9dd301"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -37.807714186968894,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -29.124456060833182,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -29.15288942452803,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -30.51249981545952,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -35.85437287393737,\n",
              " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -33.357567194852436}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Assignment12.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}