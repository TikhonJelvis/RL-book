{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zY0nC5dRQZo"
      },
      "source": [
        "**ReadMe**\n",
        "\n",
        "The part 1 is a discussion on the use of tabular algorithm.\n",
        "\n",
        "The question 1 is answered in part 2.\n",
        "\n",
        "The question 2 is answered in part 2.\n",
        "\n",
        "The question 3 is answered in part 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ7fOoXubGwK",
        "outputId": "44a26d73-ffce-4a87-ee13-7f5e4e101642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Winter 2022/CME241\n"
          ]
        }
      ],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import sys\n",
        "%cd /content/drive/My\\ Drive/Winter\\ 2022/CME241"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "r394i0lObkhY"
      },
      "outputs": [],
      "source": [
        "from rl.distribution import *\n",
        "from rl.function_approx import *\n",
        "from rl.markov_decision_process import *\n",
        "from rl.policy import *\n",
        "from rl.dynamic_programming import *\n",
        "from rl.monte_carlo import *\n",
        "from rl.chapter2.simple_inventory_mrp import *\n",
        "import numpy as np\n",
        "from itertools import islice\n",
        "from rl.gen_utils.plot_funcs import plot_list_of_curves\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from typing import Tuple, Sequence, Iterator, List\n",
        "from __future__ import annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Why using Tabular \\& Limitations? "
      ],
      "metadata": {
        "id": "eaQ7Mope48eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the second part of this assignment, we will deal with the case of finite Markov reward process. Although the fact that the process is finite does not necessarily force us to use tabular algorithm in approximating the dynamics, it is one of the most straightforward ways to understand the expected reward while also not implicitly enforcing any parametric guess on the form of the predicted value.[link text](https://)"
      ],
      "metadata": {
        "id": "1oqyGEgRtYGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tabularness of the prediction will allow us to treat each data input (which has finite variations) as an almost disconnected value in the sense that, even if this data input is similar to another in the input space, we will not enforce any guess that it will also look similar. Although this assumption of non-assuming similarity for similar input may sounds bad for our prediction since we expect similar things to yield similar behavior, this may not be true especially when we deal with a system with high sensitivity in which small change in the input can result in a tremendous change in the output. Moreover, assuming dependence upon neighbor can also introduce bias into the model, as what we predict for this case will rather also use the observation from other cases."
      ],
      "metadata": {
        "id": "IdvbciQst8zM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another use case for the tabular algorithm is when we do not know about the input. For example, we may not know how to measure the distance or the similarity of the inputs.\n",
        "\n",
        "For example, if we want to estimate the willing to pay prices for our products, which are cars, chopsticks, and sneakers. It is possible likely that sneakers maybe more similar to cars than to chopsticks but it is also quite possible in the differ\\ence case. The similarity in this sense may be hard to qunatify and hard to be used. Therefore, even though we have several datapoints for bith cars and chopsticks but a very few datapoints for sneakers, we may not want to interpolate the relationship from the cars and chopsticks into the sneakers."
      ],
      "metadata": {
        "id": "RJxqv3VMwXGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, this market model has been heavily challenged by the recent growth of online markets.\n",
        "\n",
        "We can see that in online marketplace, although the total number of items are still finite, the number can be very large. Moreover, the purchase can be sparse. (When a person tries to buy some items from Amazon, he may search on the product he wants to look into and gets to see hundreds of products. He eventually choose to purchase only one of them or maybe non of them.)  Therefore, if we treat each item as a discrete point without any interaction, we may get a very unreliable model. Moreover, as the end objective of the consumer demand estimation is not to actually know the demand, but to rather use the demand to understand what will happen if we launch a new product of this characteristics or what will ahppen if we put some promotion upon this products. We want to get a counterfactual prediction, in which the product we ask the model to predict for us has never been seen by the model before. \n",
        "\n",
        "(We say that imposing a promotion can be treated as a new product as the price will change so the product can be said to be difference. In some model consideration, people will only discretize the item input by only the phsyical characteristics but not the price.)\n",
        "\n",
        "One technique that we can do is to just think about what is the most similar product to this one and use the same value as a prediction. Another naive way is to find k most similar product and take an average or a weughted average. This is indeed the same as KNN (k-Nearest Neighbor). In KNN such as in image classifier, the input is an image that we can quantify as a vector of pixel values. However, in the training process, we do not use those value at all, and we do proceed to evaluate each item individually. Since we discretize directly by putting image with exacty the same vector representation into 1 point (in the table), we will see that one table will only see 1 instance! (since it is quite hgard for two images to have the same vector representations in the highdimensional vector space). Thus, the training is to observe that 1 value per 1 input in the table, the best way to do so is then to take taht value as the predicted value. Therefore, the trainign of the KNN does not require any actual computation, but only requires the storing of the big table of the same size as teh data input itself.\n",
        "\n",
        "In the evaluation stage, we input an image to the KNN. It is likelythat the image we ask the model to predict will not have any overlapping with the training data point, so we fidn the k-nearest neghbor and find the average prediction instead."
      ],
      "metadata": {
        "id": "UNIYt7u5xjLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, let consider an easier to understand case.\n",
        "\n",
        "If we try to predict a curvature of a two dimensional curve at different x value.\n",
        "\n",
        "If we want to assume some regulation on the conitnuity of the curve, we will see that if we perturb the value of x to be $x+\\delta x$, we will expect to see that the crosssections at the two x values will be only alittle bit different. Therefore, we can use the data of observation at x to help the prediction at $x+\\delta x$ even though it is not at the exactlyt same spot. For example, one can do so by adding regularization while training."
      ],
      "metadata": {
        "id": "geMCyBRlvSp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the neural networks model where we impose the L2 regularization can also be thought of that we want similar input to not have so stark difference. For example if the model is\n",
        "\\begin{align*}\n",
        "  y = wx\n",
        "\\end{align*}\n",
        "Therefore,\n",
        "\\begin{align*}\n",
        "  \\vert \\Delta y \\vert  = \\vert w \\vert \\vert \\Delta x \\vert \n",
        "\\end{align*}\n",
        "\n",
        "Thus, as the regularization tries to push the value of $w$ to be closer top zero. The relative difference will also be smaller.\n",
        "\n",
        "This, therefore, will help reducing variance in the prediction. However, if we want to model a system where we have already know that there should be a great variation (such as modelling rectangular wave), the regularization can lead to serious bias."
      ],
      "metadata": {
        "id": "vPpMmBc52PxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For physics example, we can see that the nature has a regularity enforcement upon most mechanical system. For example, the speed cannot be faster than the speed of light. Therefore, we will know for sure that the displacement of an object of interests cannot be more than $ct$.\n",
        "\n",
        "Therefore, if we observe the location of a car at time = 8.00am, we will be able to bound the possible location of the car at time = 1.00pm. \n",
        "\n",
        "The question is that will we use that bound to guide our prediction. \n",
        "\n",
        "The answer is clearly \"No.\"\n",
        "\n",
        "Within that time span of three hours, the car (if only limited by the speed of lights, can go back and forth from the earth to the sun for more than ten times! Therefore, this bound will says that any place in the world is possible."
      ],
      "metadata": {
        "id": "ZcqH5yvx3T-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Simple Inventory Self-Dynamics via Reinforcement Learning"
      ],
      "metadata": {
        "id": "vonstEbU5FZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We, first, denote that this problem will be a case when we do not impose any intervention into the system. In other words, we will not submit any more order for the store. Therefore, we will be able to observe the self-dynamics of the inventory."
      ],
      "metadata": {
        "id": "ai8YUsCh5Etv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first initialize the simple inventory model."
      ],
      "metadata": {
        "id": "hdSpXy1esuC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_capacity = 2\n",
        "user_poisson_lambda = 1.0\n",
        "user_holding_cost = 1.0\n",
        "user_stockout_cost = 10.0\n",
        "user_gamma = 0.9\n",
        "\n",
        "si_mrp = SimpleInventoryMRPFinite(\n",
        "        capacity=user_capacity,\n",
        "        poisson_lambda=user_poisson_lambda,\n",
        "        holding_cost=user_holding_cost,\n",
        "        stockout_cost=user_stockout_cost\n",
        "    )"
      ],
      "metadata": {
        "id": "rfY5wHeErbCe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first have a look at its transitions, we can see that the non-terminal nodes that cannot be achieve by starting at any other places is when we have 2 onhand and 0 in store. We therefore choose to use this point as a start point of the reward traces generators. However, we can use randomization fro the reward traces if we want."
      ],
      "metadata": {
        "id": "hvp5_XhJ6_Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "si_mrp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_P928SX69-s",
        "outputId": "fb8141fe-19e3-49e3-e7fe-892b51a5b4e9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "From State InventoryState(on_hand=0, on_order=0):\n",
              "  To [State InventoryState(on_hand=0, on_order=2) and Reward -10.000] with Probability 1.000\n",
              "From State InventoryState(on_hand=0, on_order=1):\n",
              "  To [State InventoryState(on_hand=1, on_order=1) and Reward -0.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=0, on_order=1) and Reward -3.679] with Probability 0.632\n",
              "From State InventoryState(on_hand=0, on_order=2):\n",
              "  To [State InventoryState(on_hand=2, on_order=0) and Reward -0.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=0, on_order=0) and Reward -1.036] with Probability 0.264\n",
              "From State InventoryState(on_hand=1, on_order=0):\n",
              "  To [State InventoryState(on_hand=1, on_order=1) and Reward -1.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=0, on_order=1) and Reward -4.679] with Probability 0.632\n",
              "From State InventoryState(on_hand=1, on_order=1):\n",
              "  To [State InventoryState(on_hand=2, on_order=0) and Reward -1.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=0, on_order=0) and Reward -2.036] with Probability 0.264\n",
              "From State InventoryState(on_hand=2, on_order=0):\n",
              "  To [State InventoryState(on_hand=2, on_order=0) and Reward -2.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=1, on_order=0) and Reward -2.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=0, on_order=0) and Reward -3.036] with Probability 0.264"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "si_mrp.non_terminal_states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxO7H4tk7bpH",
        "outputId": "a88eb1ee-b3cb-4e15-851a-cc954d987513"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[NonTerminal(state=InventoryState(on_hand=0, on_order=0)),\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=1)),\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=2)),\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=0)),\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=1)),\n",
              " NonTerminal(state=InventoryState(on_hand=2, on_order=0))]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward_traces_gen = si_mrp.reward_traces(si_mrp.non_terminal_states[2])"
      ],
      "metadata": {
        "id": "UMFB_sWF6zgR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "si_mrp.non_terminal_states[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dRqsktzsX23",
        "outputId": "5a2bf87c-aeb3-4ab5-9349-faa456d4dcb6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NonTerminal(state=InventoryState(on_hand=0, on_order=2))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mc_func_approx = LinearFunctionApprox.create(feature_functions=[(lambda x, s=s: float(x.state == s.state)) for s in si_mrp.non_terminal_states],\n",
        "        adam_gradient=AdamGradient(\n",
        "    learning_rate=0.05,\n",
        "    decay1=0.9,\n",
        "    decay2=0.999\n",
        ")\n",
        "    )"
      ],
      "metadata": {
        "id": "ViTTlz8s5gRm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "what = mc_prediction(reward_traces_gen, mc_func_approx, 0.3)"
      ],
      "metadata": {
        "id": "gv5uUi3Z-PQX"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episodes: Iterator[Iterator[mp.ReturnStep[S]]] = \\\n",
        "        (returns(trace, 0.3, 1e-6) for trace in reward_traces_gen)"
      ],
      "metadata": {
        "id": "0fB6Sc5DCYwh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(episodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "G564a1zWCf12",
        "outputId": "100923d8-a9f1-4d50-8bf9-ea32ee305a73"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-e48942e08650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-416f589c01c6>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepisodes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReturnStep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreward_traces_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/Winter 2022/CME241/rl/returns.py\u001b[0m in \u001b[0;36mreturns\u001b[0;34m(trace, γ, tolerance)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;34m*\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_transition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     return_steps = iterate.accumulate(\n",
            "\u001b[0;32m/content/drive/MyDrive/Winter 2022/CME241/rl/markov_process.py\u001b[0m in \u001b[0;36msimulate_reward\u001b[0;34m(self, start_state_distribution)\u001b[0m\n\u001b[1;32m    215\u001b[0m         '''\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_state_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0mreward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NonTerminal' object has no attribute 'sample'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(what)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "T85HzZrV_7gP",
        "outputId": "2362d260-0aa8-4033-b77d-0a336b5b111d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-5c2d2076ec3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/Winter 2022/CME241/rl/monte_carlo.py\u001b[0m in \u001b[0;36mmc_prediction\u001b[0;34m(traces, approx_0, γ, episode_length_tolerance)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         f = last(f.iterate_updates(\n\u001b[1;32m     52\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Winter 2022/CME241/rl/monte_carlo.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m     '''\n\u001b[1;32m     45\u001b[0m     \u001b[0mepisodes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReturnStep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mγ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length_tolerance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapprox_0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Winter 2022/CME241/rl/returns.py\u001b[0m in \u001b[0;36mreturns\u001b[0;34m(trace, γ, tolerance)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;34m*\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_transition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     return_steps = iterate.accumulate(\n",
            "\u001b[0;32m/content/drive/MyDrive/Winter 2022/CME241/rl/markov_process.py\u001b[0m in \u001b[0;36msimulate_reward\u001b[0;34m(self, start_state_distribution)\u001b[0m\n\u001b[1;32m    215\u001b[0m         '''\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_state_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0mreward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NonTerminal' object has no attribute 'sample'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_prediction(\n",
        "    traces: Iterable[Iterable[mp.TransitionStep[S]]],\n",
        "    approx_0: ValueFunctionApprox[S],\n",
        "    γ: float,\n",
        "    episode_length_tolerance: float = 1e-6\n",
        ") -> Iterator[ValueFunctionApprox[S]]:\n",
        "    '''Evaluate an MRP using the monte carlo method, simulating episodes\n",
        "    of the given number of steps.\n",
        "\n",
        "    Each value this function yields represents the approximated value\n",
        "    function for the MRP after one additional epsiode.\n",
        "\n",
        "    Arguments:\n",
        "      traces -- an iterator of simulation traces from an MRP\n",
        "      approx_0 -- initial approximation of value function\n",
        "      γ -- discount rate (0 < γ ≤ 1), default: 1\n",
        "      episode_length_tolerance -- stop iterating once γᵏ ≤ tolerance\n",
        "\n",
        "    Returns an iterator with updates to the approximated value\n",
        "    function after each episode.\n",
        "\n",
        "    '''\n",
        "    episodes: Iterator[Iterator[mp.ReturnStep[S]]] = \\\n",
        "        (returns(trace, γ, episode_length_tolerance) for trace in traces)\n",
        "    f = approx_0\n",
        "    yield f\n",
        "\n",
        "    for episode in episodes:\n",
        "        f = last(f.iterate_updates(\n",
        "            [(step.state, step.return_)] for step in episode\n",
        "        ))\n",
        "        yield f\n"
      ],
      "metadata": {
        "id": "z5yLuaZlqmqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Univariate BSpline"
      ],
      "metadata": {
        "id": "wkqTP3XaDjjr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Assignment10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}