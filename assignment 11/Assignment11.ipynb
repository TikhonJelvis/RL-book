{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zY0nC5dRQZo"
      },
      "source": [
        "**ReadMe**\n",
        "\n",
        "The part 1 is a discussion on the use of tabular algorithm.\n",
        "\n",
        "The question 1 is answered in part 2.\n",
        "\n",
        "The question 2 is answered in part 2.\n",
        "\n",
        "The question 3 is answered in part 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ7fOoXubGwK",
        "outputId": "1f70556d-f3d0-4d48-e95a-04c41902f488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Winter 2022/CME241\n"
          ]
        }
      ],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import sys\n",
        "%cd /content/drive/My\\ Drive/Winter\\ 2022/CME241"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "r394i0lObkhY"
      },
      "outputs": [],
      "source": [
        "from rl.distribution import *\n",
        "from rl.function_approx import *\n",
        "from rl.markov_decision_process import *\n",
        "from rl.policy import *\n",
        "from rl.dynamic_programming import *\n",
        "from rl.approximate_dynamic_programming import * \n",
        "from rl.monte_carlo import *\n",
        "from rl.chapter2.simple_inventory_mrp import *\n",
        "import numpy as np\n",
        "from itertools import islice\n",
        "from rl.gen_utils.plot_funcs import plot_list_of_curves\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from typing import Tuple, Sequence, Iterator, List\n",
        "from rl.td import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Why using Tabular \\& Limitations? "
      ],
      "metadata": {
        "id": "eaQ7Mope48eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the second part of this assignment, we will deal with the case of finite Markov reward process. Although the fact that the process is finite does not necessarily force us to use tabular algorithm in approximating the dynamics, it is one of the most straightforward ways to understand the expected reward while also not implicitly enforcing any parametric guess on the form of the predicted value.[link text](https://)"
      ],
      "metadata": {
        "id": "1oqyGEgRtYGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tabularness of the prediction will allow us to treat each data input (which has finite variations) as an almost disconnected value in the sense that, even if this data input is similar to another in the input space, we will not enforce any guess that it will also look similar. Although this assumption of non-assuming similarity for similar input may sounds bad for our prediction since we expect similar things to yield similar behavior, this may not be true especially when we deal with a system with high sensitivity in which small change in the input can result in a tremendous change in the output. Moreover, assuming dependence upon neighbor can also introduce bias into the model, as what we predict for this case will rather also use the observation from other cases."
      ],
      "metadata": {
        "id": "IdvbciQst8zM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another use case for the tabular algorithm is when we do not know about the input. For example, we may not know how to measure the distance or the similarity of the inputs.\n",
        "\n",
        "For example, if we want to estimate the willing to pay prices for our products, which are cars, chopsticks, and sneakers. It is possible likely that sneakers maybe more similar to cars than to chopsticks but it is also quite possible in the differ\\ence case. The similarity in this sense may be hard to qunatify and hard to be used. Therefore, even though we have several datapoints for bith cars and chopsticks but a very few datapoints for sneakers, we may not want to interpolate the relationship from the cars and chopsticks into the sneakers."
      ],
      "metadata": {
        "id": "RJxqv3VMwXGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, this market model has been heavily challenged by the recent growth of online markets.\n",
        "\n",
        "We can see that in online marketplace, although the total number of items are still finite, the number can be very large. Moreover, the purchase can be sparse. (When a person tries to buy some items from Amazon, he may search on the product he wants to look into and gets to see hundreds of products. He eventually choose to purchase only one of them or maybe non of them.)  Therefore, if we treat each item as a discrete point without any interaction, we may get a very unreliable model. Moreover, as the end objective of the consumer demand estimation is not to actually know the demand, but to rather use the demand to understand what will happen if we launch a new product of this characteristics or what will ahppen if we put some promotion upon this products. We want to get a counterfactual prediction, in which the product we ask the model to predict for us has never been seen by the model before. \n",
        "\n",
        "(We say that imposing a promotion can be treated as a new product as the price will change so the product can be said to be difference. In some model consideration, people will only discretize the item input by only the phsyical characteristics but not the price.)\n",
        "\n",
        "One technique that we can do is to just think about what is the most similar product to this one and use the same value as a prediction. Another naive way is to find k most similar product and take an average or a weughted average. This is indeed the same as KNN (k-Nearest Neighbor). In KNN such as in image classifier, the input is an image that we can quantify as a vector of pixel values. However, in the training process, we do not use those value at all, and we do proceed to evaluate each item individually. Since we discretize directly by putting image with exacty the same vector representation into 1 point (in the table), we will see that one table will only see 1 instance! (since it is quite hgard for two images to have the same vector representations in the highdimensional vector space). Thus, the training is to observe that 1 value per 1 input in the table, the best way to do so is then to take taht value as the predicted value. Therefore, the trainign of the KNN does not require any actual computation, but only requires the storing of the big table of the same size as teh data input itself.\n",
        "\n",
        "In the evaluation stage, we input an image to the KNN. It is likelythat the image we ask the model to predict will not have any overlapping with the training data point, so we fidn the k-nearest neghbor and find the average prediction instead."
      ],
      "metadata": {
        "id": "UNIYt7u5xjLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, let consider an easier to understand case.\n",
        "\n",
        "If we try to predict a curvature of a two dimensional curve at different x value.\n",
        "\n",
        "If we want to assume some regulation on the conitnuity of the curve, we will see that if we perturb the value of x to be $x+\\delta x$, we will expect to see that the crosssections at the two x values will be only alittle bit different. Therefore, we can use the data of observation at x to help the prediction at $x+\\delta x$ even though it is not at the exactlyt same spot. For example, one can do so by adding regularization while training."
      ],
      "metadata": {
        "id": "geMCyBRlvSp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the neural networks model where we impose the L2 regularization can also be thought of that we want similar input to not have so stark difference. For example if the model is\n",
        "\\begin{align*}\n",
        "  y = wx\n",
        "\\end{align*}\n",
        "Therefore,\n",
        "\\begin{align*}\n",
        "  \\vert \\Delta y \\vert  = \\vert w \\vert \\vert \\Delta x \\vert \n",
        "\\end{align*}\n",
        "\n",
        "Thus, as the regularization tries to push the value of $w$ to be closer top zero. The relative difference will also be smaller.\n",
        "\n",
        "This, therefore, will help reducing variance in the prediction. However, if we want to model a system where we have already know that there should be a great variation (such as modelling rectangular wave), the regularization can lead to serious bias."
      ],
      "metadata": {
        "id": "vPpMmBc52PxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For physics example, we can see that the nature has a regularity enforcement upon most mechanical system. For example, the speed cannot be faster than the speed of light. Therefore, we will know for sure that the displacement of an object of interests cannot be more than $ct$.\n",
        "\n",
        "Therefore, if we observe the location of a car at time = 8.00am, we will be able to bound the possible location of the car at time = 1.00pm. \n",
        "\n",
        "The question is that will we use that bound to guide our prediction. \n",
        "\n",
        "The answer is clearly \"No.\"\n",
        "\n",
        "Within that time span of three hours, the car (if only limited by the speed of lights, can go back and forth from the earth to the sun for more than ten times! Therefore, this bound will says that any place in the world is possible."
      ],
      "metadata": {
        "id": "ZcqH5yvx3T-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Simple Inventory Self-Dynamics via Reinforcement Learning"
      ],
      "metadata": {
        "id": "vonstEbU5FZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We, first, denote that this problem will be a case when we do not impose any intervention into the system. In other words, we will not submit any more order for the store. Therefore, we will be able to observe the self-dynamics of the inventory."
      ],
      "metadata": {
        "id": "ai8YUsCh5Etv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first initialize the simple inventory model."
      ],
      "metadata": {
        "id": "hdSpXy1esuC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_capacity = 2\n",
        "user_poisson_lambda = 1.0\n",
        "user_holding_cost = 1.0\n",
        "user_stockout_cost = 10.0\n",
        "user_gamma = 0.9\n",
        "\n",
        "si_mrp = SimpleInventoryMRPFinite(\n",
        "        capacity=user_capacity,\n",
        "        poisson_lambda=user_poisson_lambda,\n",
        "        holding_cost=user_holding_cost,\n",
        "        stockout_cost=user_stockout_cost\n",
        "    )"
      ],
      "metadata": {
        "id": "rfY5wHeErbCe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first take a look at the dybnamics, and can see the cyclical structure. In other words, we can observe any state by starting at anypoint given that we have enough time."
      ],
      "metadata": {
        "id": "DPhQz7OQjClA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "si_mrp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_P928SX69-s",
        "outputId": "2d0ab7ce-1029-4ede-9e5f-a1fb398b2770"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "From State InventoryState(on_hand=0, on_order=0):\n",
              "  To [State InventoryState(on_hand=0, on_order=2) and Reward -10.000] with Probability 1.000\n",
              "From State InventoryState(on_hand=0, on_order=1):\n",
              "  To [State InventoryState(on_hand=1, on_order=1) and Reward -0.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=0, on_order=1) and Reward -3.679] with Probability 0.632\n",
              "From State InventoryState(on_hand=0, on_order=2):\n",
              "  To [State InventoryState(on_hand=2, on_order=0) and Reward -0.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=0, on_order=0) and Reward -1.036] with Probability 0.264\n",
              "From State InventoryState(on_hand=1, on_order=0):\n",
              "  To [State InventoryState(on_hand=1, on_order=1) and Reward -1.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=0, on_order=1) and Reward -4.679] with Probability 0.632\n",
              "From State InventoryState(on_hand=1, on_order=1):\n",
              "  To [State InventoryState(on_hand=2, on_order=0) and Reward -1.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=0, on_order=0) and Reward -2.036] with Probability 0.264\n",
              "From State InventoryState(on_hand=2, on_order=0):\n",
              "  To [State InventoryState(on_hand=2, on_order=0) and Reward -2.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=1, on_order=0) and Reward -2.000] with Probability 0.368\n",
              "  To [State InventoryState(on_hand=0, on_order=0) and Reward -3.036] with Probability 0.264"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, in some other cases, there can be several source, which we cannot observe it again after we have observe it once, or there may be different part of the system where starting from a source cannot lead to all possible states. We therefore assume that each non terminal state is equally chosen from the pool of non-terminal state of the markov reward process. In some sense, this method may be inefficient. However, it will always ensure that each state will be observed. Therefore, this is quite crucial for tabular method since we enforce no regularization between each state's value directly."
      ],
      "metadata": {
        "id": "-6edzioOFoeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Monte Carlo Method"
      ],
      "metadata": {
        "id": "Wm0vn8o446EK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we try to define the count_to_weight_func."
      ],
      "metadata": {
        "id": "72zANee5yleQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_to_weight_func(n):\n",
        "  return 1/n"
      ],
      "metadata": {
        "id": "RXn18FVHxRzu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also try to do so by using lambda"
      ],
      "metadata": {
        "id": "DcHKWTExypeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_to_weight_func = lambda n: 1/ n\n",
        "\n",
        "#Test\n",
        "count_to_weight_func(6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOWZiWw9xgJP",
        "outputId": "cfed0746-9e23-4143-9b68-95f13b1569dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16666666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we want to run MC, we have to first sample an episode and find the return of each popint in the episode. \n",
        "\n",
        "As our system does not have any sink, the chain does go on forever. We therefore have to use the tool of value_tolerance (this can be done since user_gamma = 0.9 < 1).\n",
        "\n",
        "The function we use wkill be returns.\n",
        "\n",
        "This function will take in the stream of reward trace and find the value such that the gamma to the power of it becomes less than the tolerance. Afterwards, the function will keep only two time of that value as the size of the sequence of step and rewards. The function will then perform the accumulation of rewards and return the return of each state (in the first half of the terminated sequence)"
      ],
      "metadata": {
        "id": "9wHunt_Ky8KO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_traces_gen = si_mrp.reward_traces(Choose(si_mrp.non_terminal_states))\n",
        "\n",
        "#get episode\n",
        "episode = returns(next(reward_traces_gen), user_gamma, tolerance=1e-6)"
      ],
      "metadata": {
        "id": "2FTvCyVmy5gj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize value of each non-terminal state as a dict\n",
        "V_tabular: Dict[[NonTerminal], float] = dict()\n",
        "count_tabular: Dict[[NonTerminal], int] = dict()\n",
        "for non_terminal_state in si_mrp.non_terminal_states:\n",
        "  V_tabular[non_terminal_state] = 0\n",
        "  count_tabular[non_terminal_state] = 0\n",
        "\n",
        "for step in episode:\n",
        "  #identify the current non-terminal state\n",
        "  non_terminal_state = step.state\n",
        "  #update the count\n",
        "  count_tabular[non_terminal_state] += 1\n",
        "  #update the value\n",
        "  temp_f = count_to_weight_func(count_tabular[non_terminal_state])\n",
        "  V_tabular[non_terminal_state] = ((1-temp_f)*V_tabular[non_terminal_state]) + (temp_f*(step.return_))"
      ],
      "metadata": {
        "id": "5bFx3IHp070x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can concatenate the framework together"
      ],
      "metadata": {
        "id": "tqk8WcA011jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define count_to_weight_func\n",
        "count_to_weight_func = lambda n: 1/ n\n",
        "\n",
        "#Sepecify number of episode we are given\n",
        "num_episodes = 1000\n",
        "\n",
        "#Create reward traces\n",
        "reward_traces_gen = si_mrp.reward_traces(Choose(si_mrp.non_terminal_states))\n",
        "\n",
        "#Initialize value of each non-terminal state as a dict\n",
        "V_tabular: Dict[[NonTerminal], float] = dict()\n",
        "count_tabular: Dict[[NonTerminal], int] = dict()\n",
        "for non_terminal_state in si_mrp.non_terminal_states:\n",
        "  V_tabular[non_terminal_state] = 0\n",
        "  count_tabular[non_terminal_state] = 0\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "  #Get episode\n",
        "  episode = returns(next(reward_traces_gen), user_gamma, tolerance=1e-6)\n",
        "\n",
        "  #Use every step in the terminated episode\n",
        "  for step in episode:\n",
        "    #identify the current non-terminal state\n",
        "    non_terminal_state = step.state\n",
        "    #update the count\n",
        "    count_tabular[non_terminal_state] += 1\n",
        "    #update the value\n",
        "    temp_f = count_to_weight_func(count_tabular[non_terminal_state])\n",
        "    V_tabular[non_terminal_state] = ((1-temp_f)*V_tabular[non_terminal_state]) + (temp_f*(step.return_))"
      ],
      "metadata": {
        "id": "C4nMDSEV1y14"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V_tabular"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZCm7c_o3BIM",
        "outputId": "d19c8c2b-9563-4334-dbac-43998a798386"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.51194237017225,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.94854059105788,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.351705815341017,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.947159807473067,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.36572727768168,\n",
              " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.40777982255558}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then try to compare the value with the framework we have given."
      ],
      "metadata": {
        "id": "9p1VZOuF3PKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sepecify number of episode we are given\n",
        "num_episodes = 1000\n",
        "\n",
        "#Create reward traces\n",
        "reward_traces_gen = si_mrp.reward_traces(Choose(si_mrp.non_terminal_states))\n",
        "\n",
        "mc_V_tabulars = mc_prediction(reward_traces_gen, Tabular(count_to_weight_func = lambda n: 1/ n), user_gamma)\n",
        "\n",
        "#Note that the first next does not yiled anything useful (due to the design of the code)\n",
        "for _ in range(num_episodes):\n",
        "  next(mc_V_tabulars)\n",
        "\n",
        "mc_V_tabular = next(mc_V_tabulars)"
      ],
      "metadata": {
        "id": "eVSzlZbX10sV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mc_V_tabular.values_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3LY9_F_4LTj",
        "outputId": "05e30d62-05d2-4940-c5a0-40d23a32ea0f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.570323673178756,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.938426994942077,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.418909861059895,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.940955438526046,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.371675647976833,\n",
              " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.41068878359819}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Temporal Difference Method"
      ],
      "metadata": {
        "id": "quON1Riu495t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have seen that in Monte Carlo method, we have to wait until the episode is ended and uodate the value function. This implication makes Monte Carlo method undesirable in some use cases despite of its straightforwardness."
      ],
      "metadata": {
        "id": "WvvVx6Fc6L_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first try to work with 1 episode."
      ],
      "metadata": {
        "id": "BGxU57yK7C5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify alpha (learning rate)\n",
        "alpha = 0.1\n",
        "\n",
        "#Create reward traces\n",
        "reward_traces_gen = si_mrp.reward_traces(Choose(si_mrp.non_terminal_states))\n",
        "\n",
        "#Get reward trace of 1 episode\n",
        "#Note that we dont have to calculate the return of each point in the trace\n",
        "reward_trace = next(reward_traces_gen)"
      ],
      "metadata": {
        "id": "as2hJSDr6ZA-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we will create a V function.\n",
        "\n",
        "However, we do not have to keep track of counts."
      ],
      "metadata": {
        "id": "DSl97w-J9LHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize value of each non-terminal state as a dict\n",
        "td_V_tabular: Dict[[NonTerminal], float] = dict()\n",
        "for non_terminal_state in si_mrp.non_terminal_states:\n",
        "  td_V_tabular[non_terminal_state] = 0"
      ],
      "metadata": {
        "id": "FUoKHzdt9Khc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will try to find what we want from one episode. Since one episode can run on forever, we have to set some stop for it."
      ],
      "metadata": {
        "id": "ZzMGREWG-xNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = round(np.log(1e-6) / np.log(user_gamma)) "
      ],
      "metadata": {
        "id": "RSe_eE809mgr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for step in reward_trace:\n",
        "  if i >= max_steps:\n",
        "    break\n",
        "  i += 1\n",
        "  td_V_tabular[step.state] = td_V_tabular[step.state] + alpha * (step.reward + user_gamma * td_V_tabular[step.next_state] - td_V_tabular[step.state])"
      ],
      "metadata": {
        "id": "xoXOjVKN9qR8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, since we know for sure that it will run forever without interruption, we can chane to be iteration over i."
      ],
      "metadata": {
        "id": "kqYVdkT8_qiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(max_steps):\n",
        "  step = next(reward_trace)\n",
        "  td_V_tabular[step.state] = td_V_tabular[step.state] + alpha * (step.reward + user_gamma * td_V_tabular[step.next_state] - td_V_tabular[step.state])"
      ],
      "metadata": {
        "id": "7NhlfJgs9tH0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afterwards, we can concatenate everything together."
      ],
      "metadata": {
        "id": "WMCN9j69_p0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify alpha (learning rate)\n",
        "alpha = 0.5\n",
        "\n",
        "#Sepecify number of episode we are given\n",
        "num_episodes = 1000\n",
        "\n",
        "#Specify max steps per episode to be used\n",
        "max_steps = round(np.log(1e-6) / np.log(user_gamma)) \n",
        "\n",
        "#Create reward traces\n",
        "reward_traces_gen = si_mrp.reward_traces(Choose(si_mrp.non_terminal_states))\n",
        "\n",
        "#Initialize value of each non-terminal state as a dict\n",
        "td_V_tabular: Dict[[NonTerminal], float] = dict()\n",
        "for non_terminal_state in si_mrp.non_terminal_states:\n",
        "  td_V_tabular[non_terminal_state] = 0\n",
        "\n",
        "for _ in range(num_episodes): \n",
        "  #Get episode\n",
        "  reward_trace = next(reward_traces_gen)\n",
        "  for _ in range(5):\n",
        "    step = next(reward_trace)\n",
        "    td_V_tabular[step.state] = td_V_tabular[step.state] + alpha * (step.reward + user_gamma * td_V_tabular[step.next_state] - td_V_tabular[step.state])"
      ],
      "metadata": {
        "id": "s1-n4x9R7B_o"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "td_V_tabular"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtMPDgHaAVaw",
        "outputId": "4d56bb58-59b6-4bf5-98c0-e4eaea050ac2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -37.13597759567777,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -29.482079653217838,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.46355159613232,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -29.79350612177187,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -31.01063420393296,\n",
              " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -34.081303020840934}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we try to compare it with td_prediction framework"
      ],
      "metadata": {
        "id": "cAg62MMlB9wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sepecify number of episode we are given\n",
        "num_episodes = 1000\n",
        "\n",
        "#Create reward traces\n",
        "reward_traces_gen = si_mrp.reward_traces(Choose(si_mrp.non_terminal_states))\n",
        "td_experiences = itertools.chain.from_iterable(itertools.islice(episode, max_steps) for episode in reward_traces_gen)\n",
        "\n",
        "#Specify max steps per episode to be used\n",
        "max_steps = round(np.log(1e-6) / np.log(user_gamma)) \n",
        "\n",
        "td_V_tabulars_ = td_prediction(td_experiences, Tabular(count_to_weight_func=lambda n: 0.5), user_gamma)\n",
        "\n",
        "for _ in range(num_episodes*max_steps):\n",
        "  next(td_V_tabulars_)\n",
        "\n",
        "td_V_tabular_ = next(td_V_tabulars_)"
      ],
      "metadata": {
        "id": "0t1oNkv2J7yA"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "td_V_tabular_.values_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcoQOzd_I6Lt",
        "outputId": "c4e46ce3-6acb-4fb9-9018-bc65919639e8"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -36.00128786420203,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -38.79654310726185,\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -17.373573643260446,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -40.019720772830475,\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -25.770576265106047,\n",
              " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.248607157624576}"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Assignment11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}