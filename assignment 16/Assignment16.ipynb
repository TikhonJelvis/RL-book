{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ReadMe**\n",
        "\n",
        "The question / is answered in part 1."
      ],
      "metadata": {
        "id": "9zY0nC5dRQZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. SoftMax"
      ],
      "metadata": {
        "id": "sd_DrgdFMF_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We select the policy via softmax.\n",
        "\n",
        "In one sense, we can think of it as that if two actions yield a close enough optimal values. We think that we should give about the same probability of choosing both of them instead of giving all 100% into the slightly higher valued action.\n",
        "\n",
        "This will help the problem of overconfidence in misspeciation adn the lack of competition between Q function approximation."
      ],
      "metadata": {
        "id": "UKtKXoaEHnQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.1 Noise Addition"
      ],
      "metadata": {
        "id": "abbRl7O3Mdrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This same technique has also been used in various field of categorical prediction.\n",
        "\n",
        "For example, in customer demand estimation, we usually assign utiity value for each item choice provided to a customer. If two items have about the same utility, we expect the probability of the item being chosen to be about the same. The explanation for this case is that there exists some intrinsic randomness and the utiltity approximation model just approximate the mean of the actual utility. We can see that in that sense, randomness does make sense. Otherwise, given the same set of offers, a customer will always choose the same item., which is not likely.\n",
        "\n",
        "The idea of randomness may also stemmed from the decision theory in signal processing.\n",
        "\n",
        "In signal processing, we assume that there may be gaussian noise addition during the sending process. This suggests that the fact that the observed signal is higher may not always come from the fact tat the original signal at the source is hugher, but may come from the noise being added.\n",
        "\n",
        "However, due to the difficulty in dealing with gaussian cdf, in most modern machinelearning will use sfot max instead (which will be equivalent to other types of symmetric noise addition)."
      ],
      "metadata": {
        "id": "yZEdiWVPMgnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Surrogate Risk"
      ],
      "metadata": {
        "id": "vbdW1LL2Jqhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our case on optimal policy in a more formal setting the justification on the randomness may be less obvious. However, we will see that it allows us to perform gradient descent easily."
      ],
      "metadata": {
        "id": "pABy2nl0Kz0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the assymptotic sense, we can see that soft max loss (cross entropy loss with softmax) will also be a surrogate risk for the mispeciation, meaning that training by minimizing the softmax loss will minimize the 0-1 loss.\n",
        "\n",
        "However, this relies on the fact that each datapoint will have infinity occurrence and that the function approximators have great expressivity as in Tabular case, which can be not practical.\n",
        "\n",
        "In this sense, the fact that a loss is a surrogate risk for another loss may be insufficient for getting a good performance."
      ],
      "metadata": {
        "id": "FsE8h9sTJvfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.3 Score Function"
      ],
      "metadata": {
        "id": "ZG1k7prJMH02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{align*}\n",
        "  \\pi(s,a; \\theta) &= \\frac{e^{\\phi(s,a)^T \\theta}}{ \\sum_{b \\in A(s)} e^{\\phi(s,b)^T \\theta}}\\\\\n",
        "  &= \\frac{e^{\\sum_{i=1}^{m } \\phi(s,a)_i \\theta_i}}{ \\sum_{b \\in A(s)}  e^{\\sum_{i=1}^{m } \\phi(s,b)_i \\theta_i}}\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "VOY0Pjj4MYu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{align*}\n",
        "  \\log \\pi(s,a; \\theta) \n",
        "  &= \\sum_{i=1}^{m } \\phi(s,a)_i \\theta_i - \\log \\left( \\sum_{b \\in A(s)}  e^{\\sum_{i=1}^{m } \\phi(s,b)_i \\theta_i}\\right)\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "fFzXJBlEOI9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{align*}\n",
        "  \\frac{\\partial \\log \\pi(s,a; \\theta)}{\\partial \\theta_i} &= \\frac{\\partial \\phi(s,a)_i \\theta_i}{\\partial\\theta_i} - \\frac{1}{\\sum_{b \\in A(s)}  e^{\\sum_{i=1}^{m } \\phi(s,b)_i \\theta_i}}\n",
        "  \\frac{\\partial \\sum_{b \\in A(s)}  e^{\\sum_{i=1}^{m } \\phi(s,b)_i \\theta_i}}{\\partial\\theta_i}\\\\\n",
        "  &= \\phi(s,a)_i - \\frac{1}{\\sum_{b \\in A(s)}  e^{\\sum_{i=1}^{m } \\phi(s,b)_i \\theta_i}} \\sum_{b \\in A(s)} \\frac{\\partial e^{\\sum_{i=1}^{m } \\phi(s,b)_i \\theta_i}}{\\partial\\theta_i}\\\\\n",
        "  &= \\phi(s,a)_i - \\frac{1}{\\sum_{b \\in A(s)}  e^{\\sum_{i=1}^{m } \\phi(s,b)_i \\theta_i}} \\sum_{b \\in A(s)} \\left(e^{\\sum_{i=1}^{m } \\phi(s,b)_i \\theta_i} \\frac{\\partial {\\sum_{i=1}^{m } \\phi(s,b)_i \\theta_i}}{\\partial\\theta_i}\\right)\\\\\n",
        "  &= \\phi(s,a)_i - \\frac{1}{\\sum_{b \\in A(s)}  e^{\\sum_{i=1}^{m } \\phi(s,b)_i \\theta_i}} \\sum_{b \\in A(s)} \\left(e^{\\sum_{i=1}^{m } \\phi(s,b)_i \\theta_i} \\phi(s,b)_i \\right)\\\\\n",
        "  &= \\phi(s,a)_i -  \\sum_{b \\in A(s)} \\left( \\frac{e^{\\sum_{i=1}^{m } \\phi(s,b)_i \\theta_i}}{\\sum_{b \\in A(s)}  e^{\\sum_{i=1}^{m } \\phi(s,b)_i \\theta_i}} \\phi(s,b)_i \\right)\\\\\n",
        "  &= \\phi(s,a)_i -  \\sum_{b \\in A(s)} \\left( \\pi(s,b;\\theta)\\phi(s,b)_i \\right)\\\\\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "gXEjLGHZNdXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore,\n",
        "\\begin{align*}\n",
        "  \\nabla_{\\theta} \\log \\pi(s,a; \\theta)\n",
        "  &= \\phi(s,a)-  \\sum_{b \\in A(s)} \\left( \\pi(s,b;\\theta)\\phi(s,b)\\right)\\\\\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "WZQxA2tdSFDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.4 Policy Gradient"
      ],
      "metadata": {
        "id": "SPC0JHROTh3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use linear policy."
      ],
      "metadata": {
        "id": "wx_SR59NTwSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{align*}\n",
        "  Q(s,a;w) = (\\nabla_{\\theta} \\log \\pi(s,a; \\theta))^Tw\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "ZcHe0XEGTyia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore,\n",
        "\\begin{align*}\n",
        "  \\mathbf{E}_{\\pi}[Q(s,a;w)] &= \\sum_{a\\in A(s)} \\pi(s,a;\\theta) Q(s,a;w)\\\\\n",
        "  &= \\sum_{a\\in A(s)}  \\pi(s,a;\\theta) (\\nabla_{\\theta} \\log \\pi(s,a; \\theta))^Tw\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "CJCRoPfvUW62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the $i^\\text{th}$ element.\n",
        "\\begin{align*}\n",
        "  \\mathbf{E}_{\\pi}[Q_i(s,a;w)] &= \\sum_{a\\in A(s)}  \\pi(s,a;\\theta) \\frac{\\partial \\log \\pi(s,a; \\theta)}{\\partial \\theta_i} w_i\\\\\n",
        "  &= w_i \\sum_{a\\in A(s)} \\left(\\pi(s,a;\\theta) \\phi(s,a)_i -  \\sum_{b \\in A(s)} \\left( \\pi(s,a;\\theta)\\pi(s,b;\\theta)\\phi(s,b)_i \\right) \\right)\\\\\n",
        "  &= w_i \\left( \\sum_{a\\in A(s)} \\pi(s,a;\\theta) \\phi(s,a)_i -  \\sum_{b \\in A(s)} \\sum_{a\\in A(s)}  \\pi(s,a;\\theta)\\pi(s,b;\\theta)\\phi(s,b)_i \\right)\\\\\n",
        "  &= w_i \\left( \\sum_{a\\in A(s)} \\pi(s,a;\\theta) \\phi(s,a)_i -  \\sum_{b \\in A(s)}\\pi(s,b;\\theta)\\phi(s,b)_i \\right)\\\\ &= 0\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "OljEG-cNU1tm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Assignment16.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}