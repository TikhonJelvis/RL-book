{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ReadMe**\n",
        "\n",
        "The question 1,2 is answered in part 1\n",
        "\n",
        "The question 3 is answered in part 2\n",
        "\n",
        "The question 4 is answered in part 3"
      ],
      "metadata": {
        "id": "9zY0nC5dRQZo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ7fOoXubGwK",
        "outputId": "f130c527-c664-4b9b-faab-25f35cf56dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Winter 2022/CME241\n"
          ]
        }
      ],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import sys\n",
        "%cd /content/drive/My\\ Drive/Winter\\ 2022/CME241"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r394i0lObkhY"
      },
      "outputs": [],
      "source": [
        "from rl.markov_process import *\n",
        "from rl.distribution import *\n",
        "from rl.markov_decision_process import *\n",
        "from rl.td import *\n",
        "from rl.function_approx import *\n",
        "from typing import Tuple, Sequence, Iterator, List, Dict\n",
        "from rl.chapter3.simple_inventory_mdp_cap import *\n",
        "from rl.approximate_dynamic_programming import *\n",
        "from operator import itemgetter\n",
        "from rl.policy import *\n",
        "from rl.monte_carlo import *\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from rl.dynamic_programming import *\n",
        "from typing import Sequence, Callable, Tuple, Iterator, List\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcY29EPnRizD"
      },
      "source": [
        "#1. DP & RL for Finite MDP: Simple Inventory Control"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "capacity: int = 2\n",
        "poisson_lambda: float = 1.0\n",
        "holding_cost: float = 1.0\n",
        "stockout_cost: float = 10.0\n",
        "user_gamma: float = 0.9\n",
        "\n",
        "si_mdp: SimpleInventoryMDPCap = SimpleInventoryMDPCap(\n",
        "    capacity=capacity,\n",
        "    poisson_lambda=poisson_lambda,\n",
        "    holding_cost=holding_cost,\n",
        "    stockout_cost=stockout_cost\n",
        ")"
      ],
      "metadata": {
        "id": "kF9V6nmF5Ex_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1 Dynamic Programming"
      ],
      "metadata": {
        "id": "g6UwIvi_m-Sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Iteration"
      ],
      "metadata": {
        "id": "2AuPxmt8meKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_iteration_result(mdp = si_mdp, gamma = user_gamma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61rmlt2zhHiS",
        "outputId": "2553f3ae-f490-48c0-9f91-9767bc018321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894855781630035,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.660960231637507,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.991900091403533,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.660960231637507,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.991900091403533,\n",
              "  NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991900091403533},\n",
              " For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
              " For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
              " For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
              " For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
              " For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
              " For State InventoryState(on_hand=2, on_order=0): Do Action 0)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value Iteration"
      ],
      "metadata": {
        "id": "5f58b3LxmgHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value_iteration_result(mdp = si_mdp, gamma = user_gamma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0CdlBA8miYX",
        "outputId": "10573de2-7131-4f1d-df59-2cf40d519eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894855194671294,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.66095964467877,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.99189950444479,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.66095964467877,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.99189950444479,\n",
              "  NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991899504444792},\n",
              " For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
              " For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
              " For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
              " For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
              " For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
              " For State InventoryState(on_hand=2, on_order=0): Do Action 0)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Reinforcement Learning"
      ],
      "metadata": {
        "id": "SdShEeR2nO1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2.1 Monte Carlo"
      ],
      "metadata": {
        "id": "tYbdIryaNC1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the case when we always start at having full inventory on hand."
      ],
      "metadata": {
        "id": "KUQSi800dLFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "si_mdp.non_terminal_states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQCuFPpwdFK7",
        "outputId": "ee478b8f-d284-4b9c-b631-36f222560954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[NonTerminal(state=InventoryState(on_hand=0, on_order=0)),\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=1)),\n",
              " NonTerminal(state=InventoryState(on_hand=0, on_order=2)),\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=0)),\n",
              " NonTerminal(state=InventoryState(on_hand=1, on_order=1)),\n",
              " NonTerminal(state=InventoryState(on_hand=2, on_order=0))]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "THus, we will set the initial distirbution to have only the last state"
      ],
      "metadata": {
        "id": "V2DO1WrtdoEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_state_distribution = Constant(si_mdp.non_terminal_states[-1])"
      ],
      "metadata": {
        "id": "JD64RKYXdi09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qfs: Iterator[QValueFunctionApprox[S, A]] = glie_mc_control(si_mdp,  start_state_distribution, Tabular(), 0.9, lambda n: 1/n)\n",
        "next(qfs)\n",
        "\n",
        "def almost_equal_qfs(\n",
        "    v1: QValueFunctionApprox[S, A],\n",
        "    v2: QValueFunctionApprox[S, A],\n",
        "    tolerance: float = 1e-1\n",
        ") -> bool:\n",
        "    '''Return whether the two value function tables are within the given\n",
        "    tolerance of each other.\n",
        "    '''\n",
        "    return max(abs(v1.values_map[s] - v2.values_map[s]) for s in v1.values_map) < DEFAULT_TOLERANCE\n",
        "\n",
        "opt_qf: QValueFunctionApprox[S, A] = converged(qfs, done=almost_equal_qfs)"
      ],
      "metadata": {
        "id": "6xR6vToKylTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_qf.values_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueLa_O3u8ia-",
        "outputId": "cb16a78e-d12c-40a5-bb43-b746e8246b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(NonTerminal(state=InventoryState(on_hand=0, on_order=0)),\n",
              "  0): -48.10485424176997,\n",
              " (NonTerminal(state=InventoryState(on_hand=0, on_order=0)),\n",
              "  1): -34.905129903961466,\n",
              " (NonTerminal(state=InventoryState(on_hand=0, on_order=0)),\n",
              "  2): -42.40188264324315,\n",
              " (NonTerminal(state=InventoryState(on_hand=0, on_order=1)),\n",
              "  0): -33.91955816407902,\n",
              " (NonTerminal(state=InventoryState(on_hand=0, on_order=1)),\n",
              "  1): -27.676522103892697,\n",
              " (NonTerminal(state=InventoryState(on_hand=0, on_order=2)),\n",
              "  0): -36.19133135362624,\n",
              " (NonTerminal(state=InventoryState(on_hand=1, on_order=0)),\n",
              "  0): -38.05627157227237,\n",
              " (NonTerminal(state=InventoryState(on_hand=1, on_order=0)),\n",
              "  1): -28.678752392142272,\n",
              " (NonTerminal(state=InventoryState(on_hand=1, on_order=1)),\n",
              "  0): -29.00880230308017,\n",
              " (NonTerminal(state=InventoryState(on_hand=2, on_order=0)),\n",
              "  0): -30.006490204283047}"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we get optimal q function, we will tyhen try to convert it back into policy and v function"
      ],
      "metadata": {
        "id": "udjjDhMu_vnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_vf: Mapping[NonTerminal[S], float] = dict()\n",
        "greedy_policy_dict: Dict[S, A] = dict()\n",
        "for s in si_mdp.non_terminal_states:\n",
        "  opt_vf[s] =  max(opt_qf((s, a)) for a in si_mdp.actions(s))\n",
        "  _, greedy_policy_dict[s.state] = opt_qf.argmax((s, a) for a in si_mdp.actions(s))"
      ],
      "metadata": {
        "id": "5JLYtrCJABwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_vf, FiniteDeterministicPolicy(greedy_policy_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ikm7fVGlEPWw",
        "outputId": "383f5136-f709-4c77-8d03-0151cdd62cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.905129903961466,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.676522103892697,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -36.19133135362624,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.678752392142272,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.00880230308017,\n",
              "  NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.006490204283047},\n",
              " For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
              " For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
              " For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
              " For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
              " For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
              " For State InventoryState(on_hand=2, on_order=0): Do Action 0)"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check with the value from DP"
      ],
      "metadata": {
        "id": "HgO1KXM0Elso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value_iteration_result(mdp = si_mdp, gamma = user_gamma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5HHKorEERW0",
        "outputId": "afe4358a-002f-4248-8e91-5be0a3b25666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894855194671294,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.66095964467877,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.99189950444479,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.66095964467877,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.99189950444479,\n",
              "  NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991899504444792},\n",
              " For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
              " For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
              " For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
              " For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
              " For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
              " For State InventoryState(on_hand=2, on_order=0): Do Action 0)"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see that the value is different when on_order is 2."
      ],
      "metadata": {
        "id": "EoB1PIyfEwrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afterwards, we can create a function."
      ],
      "metadata": {
        "id": "9uOBAq-SFN1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Distribution[NonTerminal]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTPmdFDCFtSg",
        "outputId": "5851b811-7154-4364-dd61-5a030d61ba46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rl.distribution.Distribution[rl.markov_process.NonTerminal]"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def finite_glie_mc_result(mdp: FiniteMarkovDecisionProcess, start_state_distribution: Distribution[NonTerminal], gamma: float, epsilon_as_func_of_episodes: Callable[[int], float] = lambda n: 1/n, tolerance:float = DEFAULT_TOLERANCE, episode_length_tolerance: float=1e-06):\n",
        "  qfs: Iterator[QValueFunctionApprox[S, A]] = glie_mc_control(mdp,  start_state_distribution, Tabular(), gamma, epsilon_as_func_of_episodes)\n",
        "  next(qfs)\n",
        "\n",
        "  def almost_equal_qfs(\n",
        "      v1: QValueFunctionApprox[S, A],\n",
        "      v2: QValueFunctionApprox[S, A],\n",
        "      tolerance: float = tolerance\n",
        "  ) -> bool:\n",
        "      '''Return whether the two value function tables are within the given\n",
        "      tolerance of each other.\n",
        "      '''\n",
        "      return max(abs(v1.values_map[s] - v2.values_map[s]) for s in v1.values_map) < tolerance\n",
        "\n",
        "  opt_qf: QValueFunctionApprox[S, A] = converged(qfs, done=almost_equal_qfs)\n",
        "\n",
        "  opt_vf: Mapping[NonTerminal[S], float] = dict()\n",
        "  greedy_policy_dict: Dict[S, A] = dict()\n",
        "  for s in si_mdp.non_terminal_states:\n",
        "    opt_vf[s] =  max(opt_qf((s, a)) for a in si_mdp.actions(s))\n",
        "    _, greedy_policy_dict[s.state] = opt_qf.argmax((s, a) for a in si_mdp.actions(s))\n",
        "\n",
        "  return opt_vf, FiniteDeterministicPolicy(greedy_policy_dict)"
      ],
      "metadata": {
        "id": "SD1pZDfTE8aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example with early ending (tolerance is high)"
      ],
      "metadata": {
        "id": "Ty-U1VauMMrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finite_glie_mc_result(mdp=si_mdp, \n",
        "                      start_state_distribution=Choose(si_mdp.non_terminal_states), \n",
        "                      gamma=user_gamma,\n",
        "                      tolerance = 1e-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i4u2ZxlLx04",
        "outputId": "3c12681d-8d23-4e17-e7fc-95639561189d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -37.46180438467768,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -29.18641922751893,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -30.562545763100374,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -30.460660034866002,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -30.800628905579266,\n",
              "  NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -32.47535514053803},\n",
              " For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
              " For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
              " For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
              " For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
              " For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
              " For State InventoryState(on_hand=2, on_order=0): Do Action 0)"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2.2 SARSA "
      ],
      "metadata": {
        "id": "RpmVwzM7M-Di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define new glie sarsa function"
      ],
      "metadata": {
        "id": "kRv5lhHPXKtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "si_mdp.actions(NonTerminal(state))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-jGqszeylRA",
        "outputId": "19656faf-5d14-4492-9a6c-de0eb0311961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([0, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main difference is taht instead of yielding the function at every update step, we update the decision at the end of the episode.\n",
        "\n",
        "(Note that the actual updating will still occurring in the online meanner, but we will not print it out.)"
      ],
      "metadata": {
        "id": "vEWWL9c62gc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Note that states in this case mean the start states distribution.\n",
        "def new_glie_sarsa(\n",
        "    mdp: MarkovDecisionProcess[S, A],\n",
        "    states: NTStateDistribution[S],\n",
        "    approx_0: QValueFunctionApprox[S, A],\n",
        "    γ: float,\n",
        "    ϵ_as_func_of_episodes: Callable[[int], float],\n",
        "    max_episode_length: int\n",
        ") -> Iterator[QValueFunctionApprox[S, A]]:\n",
        "    q: QValueFunctionApprox[S, A] = approx_0\n",
        "    yield q\n",
        "    num_episodes: int = 0\n",
        "    while True:\n",
        "        num_episodes += 1\n",
        "        ϵ: float = ϵ_as_func_of_episodes(num_episodes)\n",
        "        #Start the episode by choosing 1 from the states\n",
        "        state: NonTerminal[S] = states.sample()\n",
        "        #Choose 1 action according to the epsilon_greedy_policy (which is indeed also adapted with Q (for the 1-epsilon case))\n",
        "        #State in the argument is our current state\n",
        "        #mdp.actions(state) is a dictionary key\n",
        "        action: A = epsilon_greedy_action(q, state, mdp.actions(state), ϵ=ϵ)\n",
        "        steps: int = 0\n",
        "\n",
        "        #Repeat until the state is non-terminal or the episode ends \n",
        "        while isinstance(state, NonTerminal) and steps < max_episode_length:\n",
        "            #Update the state\n",
        "            #Sample one possible next_state, reward pair associated with the action (which has been decided)\n",
        "            next_state, reward = mdp.step(state, action).sample()\n",
        "            if isinstance(next_state, NonTerminal):\n",
        "                #Guess the action for the next state using the same (current) q.\n",
        "                next_action: A = epsilon_greedy_action(q, next_state, set(mdp.actions(next_state)), ϵ)\n",
        "                #After that, we will be able to update the q\n",
        "                q = q.update([((state, action), reward + γ * q((next_state, next_action)))])\n",
        "                action = next_action\n",
        "            else:\n",
        "                #Because we assume that the last state will have the reward of zero\n",
        "                q = q.update([((state, action), reward)])\n",
        "            steps += 1\n",
        "            state = next_state\n",
        "        yield q"
      ],
      "metadata": {
        "id": "tyIjZwtVXNfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the convergence test into it."
      ],
      "metadata": {
        "id": "lDO84YcjXOEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def finite_glie_sarsa_result(mdp: FiniteMarkovDecisionProcess, start_state_distribution: Distribution[NonTerminal], gamma: float, epsilon_as_func_of_episodes: Callable[[int], float] = lambda n: 1/n, tolerance:float = DEFAULT_TOLERANCE, max_episode_length: float=100):\n",
        "  \n",
        "  qfs: Iterator[QValueFunctionApprox[S, A]] = new_glie_sarsa(mdp,\n",
        "                                                         start_state_distribution,\n",
        "                                                         Tabular(),\n",
        "                                                         gamma,\n",
        "                                                         epsilon_as_func_of_episodes,\n",
        "                                                         max_episode_length)\n",
        "  \n",
        "  next(qfs)\n",
        "\n",
        "  def almost_equal_qfs(\n",
        "      v1: QValueFunctionApprox[S, A],\n",
        "      v2: QValueFunctionApprox[S, A],\n",
        "      tolerance: float = tolerance\n",
        "  ) -> bool:\n",
        "      '''Return whether the two value function tables are within the given\n",
        "      tolerance of each other.\n",
        "      '''\n",
        "      return max(abs(v1.values_map[s] - v2.values_map[s]) for s in v1.values_map) < tolerance\n",
        "  \n",
        "  opt_qf: QValueFunctionApprox[S, A] = converged(qfs, done=almost_equal_qfs)\n",
        "\n",
        "  opt_vf: Mapping[NonTerminal[S], float] = dict()\n",
        "  greedy_policy_dict: Dict[S, A] = dict()\n",
        "  for s in si_mdp.non_terminal_states:\n",
        "    opt_vf[s] =  max(opt_qf((s, a)) for a in si_mdp.actions(s))\n",
        "    _, greedy_policy_dict[s.state] = opt_qf.argmax((s, a) for a in si_mdp.actions(s))\n",
        "\n",
        "  return opt_vf, FiniteDeterministicPolicy(greedy_policy_dict)"
      ],
      "metadata": {
        "id": "fQgKj4rCMV-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finite_glie_sarsa_result(mdp=si_mdp,\n",
        "                      start_state_distribution=Choose(si_mdp.non_terminal_states),\n",
        "                      gamma = user_gamma,\n",
        "                      max_episode_length=1000, tolerance=1e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBLr8ET7SWbI",
        "outputId": "de3daa1f-00ce-4d6a-daa5-a272bdd0cf3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -27.784916328399795,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -20.836969141233247,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -20.826921289702042,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -21.786121262687033,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -21.973576238178676,\n",
              "  NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -22.887821070657093},\n",
              " For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
              " For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
              " For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
              " For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
              " For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
              " For State InventoryState(on_hand=2, on_order=0): Do Action 0)"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2.3 Q-Learning"
      ],
      "metadata": {
        "id": "nlGZGSKkXThy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SARSA is complicated in the sense that we have to adapt the policy at every timestep.\n",
        "\n",
        "However, for Q-Learning, the algorithm can look more complicated. \n",
        "\n",
        "Thus, we will restrict attention on annotating the q-learning function first."
      ],
      "metadata": {
        "id": "Iqnmbqkm1hjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(\n",
        "    mdp: MarkovDecisionProcess[S, A],\n",
        "    policy_from_q: PolicyFromQType,\n",
        "    states: NTStateDistribution[S],\n",
        "    approx_0: QValueFunctionApprox[S, A],\n",
        "    γ: float,\n",
        "    max_episode_length: int\n",
        ") -> Iterator[QValueFunctionApprox[S, A]]:\n",
        "    q: QValueFunctionApprox[S, A] = approx_0\n",
        "    yield q\n",
        "\n",
        "    #Different from SARSA, we do not have to adjust epsilon according to the episode. (so we do not have to count the number of epsiode)\n",
        "    while True:\n",
        "        #Initialize state according to the start state distribution\n",
        "        state: NonTerminal[S] = states.sample()\n",
        "        #The step size will be used for terminating sequence in case the state evolve infinitely\n",
        "        steps: int = 0\n",
        "        \n",
        "        #Notice that we just have to specifiy state\n",
        "        #The action will not be determined yet\n",
        "        \n",
        "        #Note that epsilon is not shown explicitly, but will be used implicitly via policy_from_q (if we choose it to be epsilon-greedy policy)\n",
        "\n",
        "        #If the state is terminal or we have run sufficiently long step (hard-indicated via max_episode_length)\n",
        "        while isinstance(state, NonTerminal) and steps < max_episode_length:\n",
        "            #Find new policy adpated to q\n",
        "            policy: Policy[S, A] = policy_from_q(q, mdp)\n",
        "            #Sample action\n",
        "            action: A = policy.act(state).sample()\n",
        "            #After the action is realized, we can sample the transition according to the state and the action (and also the mdp)\n",
        "            next_state, reward = mdp.step(state, action).sample()\n",
        "            \n",
        "            #Use greedy return according to q (which is current q)\n",
        "            #If the next_state is terminal, we set the return after that state to be zero\n",
        "            next_return: float = max(q((next_state, a)) for a in mdp.actions(next_state)) if isinstance(next_state, NonTerminal) else 0.\n",
        "            \n",
        "            #We update the q\n",
        "            q = q.update([((state, action), reward + γ * next_return)])\n",
        "\n",
        "            yield q\n",
        "\n",
        "            steps += 1\n",
        "            state = next_state"
      ],
      "metadata": {
        "id": "ItHz01HlVfLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let look into the case when we want the yielding of the q function to be after each episode instead of each step.\n",
        "\n",
        "We will also use epsilon greedy in each exploration."
      ],
      "metadata": {
        "id": "iD5o3Hix7wub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def new_q_learning(\n",
        "    mdp: MarkovDecisionProcess[S, A],\n",
        "    policy_from_q: PolicyFromQType,\n",
        "    states: NTStateDistribution[S],\n",
        "    approx_0: QValueFunctionApprox[S, A],\n",
        "    γ: float,\n",
        "    max_episode_length: int\n",
        ") -> Iterator[QValueFunctionApprox[S, A]]:\n",
        "    q: QValueFunctionApprox[S, A] = approx_0\n",
        "    yield q\n",
        "\n",
        "    #Different from SARSA, we do not have to adjust epsilon according to the episode. (so we do not have to count the number of epsiode)\n",
        "    while True:\n",
        "        #Initialize state according to the start state distribution\n",
        "        state: NonTerminal[S] = states.sample()\n",
        "        #The step size will be used for terminating sequence in case the state evolve infinitely\n",
        "        steps: int = 0\n",
        "        \n",
        "        #Notice that we just have to specifiy state\n",
        "        #The action will not be determined yet\n",
        "        \n",
        "        #Note that epsilon is not shown explicitly, but will be used implicitly via policy_from_q (if we choose it to be epsilon-greedy policy)\n",
        "\n",
        "        #If the state is terminal or we have run sufficiently long step (hard-indicated via max_episode_length)\n",
        "        while isinstance(state, NonTerminal) and steps < max_episode_length:\n",
        "            #Find new policy adpated to q\n",
        "            policy: Policy[S, A] = policy_from_q(q, mdp)\n",
        "            #Sample action\n",
        "            action: A = policy.act(state).sample()\n",
        "            #After the action is realized, we can sample the transition according to the state and the action (and also the mdp)\n",
        "            next_state, reward = mdp.step(state, action).sample()\n",
        "            \n",
        "            #Use greedy return according to q (which is current q)\n",
        "            #If the next_state is terminal, we set the return after that state to be zero\n",
        "            next_return: float = max(q((next_state, a)) for a in mdp.actions(next_state)) if isinstance(next_state, NonTerminal) else 0.\n",
        "            \n",
        "            #We update the q\n",
        "            q = q.update([((state, action), reward + γ * next_return)])\n",
        "\n",
        "            steps += 1\n",
        "            state = next_state\n",
        "        yield q"
      ],
      "metadata": {
        "id": "l0YaIFqK7Up1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning_epsilon_greedy(\n",
        "    mdp: MarkovDecisionProcess[S, A],\n",
        "    epsilon: float,\n",
        "    start_state_distribution: NTStateDistribution[S],\n",
        "    initial_func_approx: QValueFunctionApprox[S, A],\n",
        "    gamma: float,\n",
        "    max_episode_length: int\n",
        ") -> Iterator[QValueFunctionApprox[S, A]]:\n",
        "\n",
        "    def policy_from_q(qf, mdp, epsilon=epsilon):\n",
        "      return epsilon_greedy_policy(qf, mdp, epsilon)\n",
        "\n",
        "    return new_q_learning(mdp, policy_from_q, start_state_distribution, initial_func_approx, gamma, max_episode_length)"
      ],
      "metadata": {
        "id": "hphQ0-yH8IzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hi = q_learning_epsilon_greedy(si_mdp, 1e-1, Choose(si_mdp.non_terminal_states), Tabular(), user_gamma, 200)"
      ],
      "metadata": {
        "id": "S4o5HrSZ95OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning_epsilon_greedy_result(mdp: FiniteMarkovDecisionProcess, \n",
        "                                     start_state_distribution: Distribution[NonTerminal], \n",
        "                                     gamma: float, \n",
        "                                     epsilon: float,\n",
        "                                     tolerance:float = DEFAULT_TOLERANCE,\n",
        "                                     max_episode_length: float=100):\n",
        "  \n",
        "  qfs: Iterator[QValueFunctionApprox[S, A]] = q_learning_epsilon_greedy(mdp,\n",
        "                                                                        epsilon,\n",
        "                                                                        start_state_distribution,\n",
        "                                                                        Tabular(),\n",
        "                                                                        gamma,\n",
        "                                                                        max_episode_length)\n",
        "  \n",
        "  next(qfs)\n",
        "\n",
        "  def almost_equal_qfs(\n",
        "      v1: QValueFunctionApprox[S, A],\n",
        "      v2: QValueFunctionApprox[S, A],\n",
        "      tolerance: float = tolerance\n",
        "  ) -> bool:\n",
        "      '''Return whether the two value function tables are within the given\n",
        "      tolerance of each other.\n",
        "      '''\n",
        "      return max(abs(v1.values_map[s] - v2.values_map[s]) for s in v1.values_map) < tolerance\n",
        "  \n",
        "  opt_qf: QValueFunctionApprox[S, A] = converged(qfs, done=almost_equal_qfs)\n",
        "\n",
        "  opt_vf: Mapping[NonTerminal[S], float] = dict()\n",
        "  greedy_policy_dict: Dict[S, A] = dict()\n",
        "  for s in si_mdp.non_terminal_states:\n",
        "    opt_vf[s] =  max(opt_qf((s, a)) for a in si_mdp.actions(s))\n",
        "    _, greedy_policy_dict[s.state] = opt_qf.argmax((s, a) for a in si_mdp.actions(s))\n",
        "\n",
        "  return opt_vf, FiniteDeterministicPolicy(greedy_policy_dict)"
      ],
      "metadata": {
        "id": "VH_VMa_g-a1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_learning_epsilon_greedy_result(si_mdp, Choose(si_mdp.non_terminal_states), user_gamma, 0.1, 1e-2, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SIyYMQ9Ighc",
        "outputId": "2488c36d-d42a-4dab-be87-e2f7f15b9941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -23.708881697939493,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -16.477160943574702,\n",
              "  NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -16.64774178411582,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -17.4760098810234,\n",
              "  NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -17.713356855566282,\n",
              "  NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -18.656796562971696},\n",
              " For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
              " For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
              " For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
              " For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
              " For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
              " For State InventoryState(on_hand=2, on_order=0): Do Action 0)"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. ADP & RL for Non-Finite MDP: Discrete Asset Allocation"
      ],
      "metadata": {
        "id": "2eEN5DwwOof_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class AssetAllocDiscrete:\n",
        "    risky_return_distributions: Sequence[Distribution[float]]\n",
        "    riskless_returns: Sequence[float]\n",
        "    utility_func: Callable[[float], float]\n",
        "    risky_alloc_choices: Sequence[float]\n",
        "    feature_functions: Sequence[Callable[[Tuple[float, float]], float]]\n",
        "    dnn_spec: DNNSpec\n",
        "    initial_wealth_distribution: Distribution[float]\n",
        "\n",
        "    def time_steps(self) -> int:\n",
        "        return len(self.risky_return_distributions)\n",
        "\n",
        "    def uniform_actions(self) -> Choose[float]:\n",
        "        return Choose(self.risky_alloc_choices)\n",
        "\n",
        "    def get_mdp(self, t: int) -> MarkovDecisionProcess[float, float]:\n",
        "        \"\"\"\n",
        "        State is Wealth W_t,\n",
        "        Action is investment in risky asset = x_t\n",
        "        \"\"\"\n",
        "\n",
        "        distr: Distribution[float] = self.risky_return_distributions[t]\n",
        "        rate: float = self.riskless_returns[t]\n",
        "        alloc_choices: Sequence[float] = self.risky_alloc_choices\n",
        "        steps: int = self.time_steps()\n",
        "        utility_f: Callable[[float], float] = self.utility_func\n",
        "\n",
        "        class AssetAllocMDP(MarkovDecisionProcess[float, float]):\n",
        "            #wealth is state\n",
        "            #alloc is action\n",
        "            #both are stored as float\n",
        "\n",
        "            #step will indicate how current state and action affect the next distribution of (state and reward) in tuple form\n",
        "            def step(self, wealth: NonTerminal[float], alloc: float) -> SampledDistribution[Tuple[State[float], float]]:\n",
        "                #state reward sampler\n",
        "                def sr_sampler_func(wealth=wealth, alloc=alloc) -> Tuple[State[float], float]:\n",
        "                    next_wealth: float = alloc * (1 + distr.sample()) + (wealth.state - alloc) * (1 + rate)\n",
        "                    #if it is going to be terminated, give a reward.\n",
        "                    #otherwsie, do nothing\n",
        "                    reward: float = utility_f(next_wealth) if t == steps - 1 else 0.\n",
        "                    next_state: State[float] = Terminal(next_wealth) if t == steps - 1 else NonTerminal(next_wealth)\n",
        "                    return (next_state, reward)\n",
        "                return SampledDistribution(sampler=sr_sampler_func, expectation_samples=100)\n",
        "\n",
        "            #Possible actions\n",
        "            def actions(self, wealth: NonTerminal[float]) -> Sequence[float]:\n",
        "                return alloc_choices\n",
        "\n",
        "        return AssetAllocMDP()\n",
        "\n",
        "    def get_qvf_func_approx(self) -> DNNApprox[Tuple[NonTerminal[float], float]]:\n",
        "        adam_gradient: AdamGradient = AdamGradient(learning_rate=0.1, decay1=0.9, decay2=0.999)\n",
        "        #List of function fetaures: Nonterminal State x Action ---> features\n",
        "        ffs: List[Callable[[Tuple[NonTerminal[float], float]], float]] = []\n",
        "        for f in self.feature_functions:\n",
        "            def this_f(pair: Tuple[NonTerminal[float], float], f=f) -> float:\n",
        "                return f((pair[0].state, pair[1]))\n",
        "            ffs.append(this_f)\n",
        "\n",
        "        return DNNApprox.create(feature_functions=ffs, dnn_spec=self.dnn_spec, adam_gradient=adam_gradient)\n",
        "\n",
        "    #Get states distribution at time t not codnitioning on doing anything\n",
        "    def get_states_distribution(self, t: int) -> SampledDistribution[NonTerminal[float]]:\n",
        "        \n",
        "        #Action distribution\n",
        "        actions_distr: Choose[float] = self.uniform_actions()\n",
        "\n",
        "        def states_sampler_func() -> NonTerminal[float]:\n",
        "            wealth: float = self.initial_wealth_distribution.sample()\n",
        "            for i in range(t):\n",
        "                distr: Distribution[float] = self.risky_return_distributions[i]\n",
        "                rate: float = self.riskless_returns[i]\n",
        "                #sample the ation\n",
        "                alloc: float = actions_distr.sample()\n",
        "                #sample the wealth\n",
        "                wealth = alloc * (1 + distr.sample()) + (wealth - alloc) * (1 + rate)\n",
        "            return NonTerminal(wealth)\n",
        "\n",
        "        return SampledDistribution(states_sampler_func)\n",
        "\n",
        "\n",
        "    def backward_induction_qvf(self) -> Iterator[QValueFunctionApprox[float, float]]:\n",
        "\n",
        "        #Initilaize function approximtaion\n",
        "        init_fa: DNNApprox[Tuple[NonTerminal[float], float]] = self.get_qvf_func_approx()\n",
        "\n",
        "        mdp_f0_mu_triples: Sequence[Tuple[MarkovDecisionProcess[float, float], DNNApprox[Tuple[NonTerminal[float], float]], SampledDistribution[NonTerminal[float]]]] = [(self.get_mdp(i), init_fa, self.get_states_distribution(i)) for i in range(self.time_steps())]\n",
        "\n",
        "        num_state_samples: int = 300\n",
        "        error_tolerance: float = 1e-6\n",
        "\n",
        "        return back_opt_qvf(mdp_f0_mu_triples=mdp_f0_mu_triples, γ=1.0, num_state_samples=num_state_samples, error_tolerance=error_tolerance)\n",
        "\n",
        "    def get_vf_func_approx(self, ff: Sequence[Callable[[NonTerminal[float]], float]]) -> DNNApprox[NonTerminal[float]]:\n",
        "\n",
        "        adam_gradient: AdamGradient = AdamGradient(learning_rate=0.1, decay1=0.9,decay2=0.999)\n",
        "        return DNNApprox.create(feature_functions=ff, dnn_spec=self.dnn_spec, adam_gradient=adam_gradient)\n",
        "\n",
        "    def backward_induction_vf_and_pi(self, ff: Sequence[Callable[[NonTerminal[float]], float]]) -> Iterator[Tuple[ValueFunctionApprox[float], DeterministicPolicy[float, float]]]:\n",
        "\n",
        "        init_fa: DNNApprox[NonTerminal[float]] = self.get_vf_func_approx(ff)\n",
        "\n",
        "        mdp_f0_mu_triples: Sequence[Tuple[MarkovDecisionProcess[float, float], DNNApprox[NonTerminal[float]], SampledDistribution[NonTerminal[float]]]] = [(self.get_mdp(i), init_fa, self.get_states_distribution(i)) for i in range(self.time_steps())]\n",
        "\n",
        "        num_state_samples: int = 100\n",
        "        error_tolerance: float = 1e-3\n",
        "\n",
        "        return back_opt_vf_and_policy(mdp_f0_mu_triples=mdp_f0_mu_triples, γ=1.0, num_state_samples=num_state_samples, error_tolerance=error_tolerance)"
      ],
      "metadata": {
        "id": "kTB6BDlL_Ev3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "steps: int = 2\n",
        "μ: float = 0.13\n",
        "σ: float = 0.2\n",
        "r: float = 0.07\n",
        "a: float = 1.0\n",
        "init_wealth: float = 1.0\n",
        "init_wealth_stdev: float = 0.1\n",
        "\n",
        "excess: float = μ - r\n",
        "var: float = σ * σ\n",
        "base_alloc: float = excess / (a * var)\n",
        "\n",
        "risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
        "riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
        "utility_function: Callable[[float], float] = lambda x: - np.exp(-a * x) / a\n",
        "alloc_choices: Sequence[float] = np.linspace(2 / 3 * base_alloc, 4 / 3 * base_alloc,11)\n",
        "feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = [lambda _: 1., lambda w_x: w_x[0], lambda w_x: w_x[1], lambda w_x: w_x[1] * w_x[1]]\n",
        "dnn: DNNSpec = DNNSpec(neurons=[], bias=False, hidden_activation=lambda x: x, hidden_activation_deriv=lambda y: np.ones_like(y), output_activation=lambda x: - np.sign(a) * np.exp(-x),output_activation_deriv=lambda y: -y)\n",
        "init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_stdev)\n",
        "\n",
        "aad: AssetAllocDiscrete = AssetAllocDiscrete(\n",
        "        risky_return_distributions=risky_ret,\n",
        "        riskless_returns=riskless_ret,\n",
        "        utility_func=utility_function,\n",
        "        risky_alloc_choices=alloc_choices,\n",
        "        feature_functions=feature_funcs,\n",
        "        dnn_spec=dnn,\n",
        "        initial_wealth_distribution=init_wealth_distr\n",
        "    )"
      ],
      "metadata": {
        "id": "nHaScme7OtJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    it_qvf: Iterator[QValueFunctionApprox[float, float]] = \\\n",
        "        aad.backward_induction_qvf()\n",
        "\n",
        "    print(\"Backward Induction on Q-Value Function\")\n",
        "    print(\"--------------------------------------\")\n",
        "    print()\n",
        "    for t, q in enumerate(it_qvf):\n",
        "        print(f\"Time {t:d}\")\n",
        "        print()\n",
        "        opt_alloc: float = max(\n",
        "            ((q((NonTerminal(init_wealth), ac)), ac) for ac in alloc_choices),\n",
        "            key=itemgetter(0)\n",
        "        )[1]\n",
        "        val: float = max(q((NonTerminal(init_wealth), ac))\n",
        "                         for ac in alloc_choices)\n",
        "        print(f\"Opt Risky Allocation = {opt_alloc:.3f}, Opt Val = {val:.3f}\")\n",
        "        print(\"Optimal Weights below:\")\n",
        "        for wts in q.weights:\n",
        "            pprint(wts.weights)\n",
        "        print()\n",
        "\n",
        "    print(\"Analytical Solution\")\n",
        "    print(\"-------------------\")\n",
        "    print()\n",
        "\n",
        "    for t in range(steps):\n",
        "        print(f\"Time {t:d}\")\n",
        "        print()\n",
        "        left: int = steps - t\n",
        "        growth: float = (1 + r) ** (left - 1)\n",
        "        alloc: float = base_alloc / growth\n",
        "        vval: float = - np.exp(- excess * excess * left / (2 * var)\n",
        "                               - a * growth * (1 + r) * init_wealth) / a\n",
        "        bias_wt: float = excess * excess * (left - 1) / (2 * var) + \\\n",
        "            np.log(np.abs(a))\n",
        "        w_t_wt: float = a * growth * (1 + r)\n",
        "        x_t_wt: float = a * excess * growth\n",
        "        x_t2_wt: float = - var * (a * growth) ** 2 / 2\n",
        "\n",
        "        print(f\"Opt Risky Allocation = {alloc:.3f}, Opt Val = {vval:.3f}\")\n",
        "        print(f\"Bias Weight = {bias_wt:.3f}\")\n",
        "        print(f\"W_t Weight = {w_t_wt:.3f}\")\n",
        "        print(f\"x_t Weight = {x_t_wt:.3f}\")\n",
        "        print(f\"x_t^2 Weight = {x_t2_wt:.3f}\")\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM5vWJPaPwVq",
        "outputId": "02428742-3e17-4b5c-92e1-cc93ebb81782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backward Induction on Q-Value Function\n",
            "--------------------------------------\n",
            "\n",
            "Time 0\n",
            "\n",
            "Opt Risky Allocation = 1.400, Opt Val = -0.291\n",
            "Optimal Weights below:\n",
            "array([[ 0.04992729,  1.14322909,  0.06191486, -0.02253809]])\n",
            "\n",
            "Time 1\n",
            "\n",
            "Opt Risky Allocation = 1.400, Opt Val = -0.328\n",
            "Optimal Weights below:\n",
            "array([[-0.0056287 ,  1.06953962,  0.07161126, -0.02476941]])\n",
            "\n",
            "Analytical Solution\n",
            "-------------------\n",
            "\n",
            "Time 0\n",
            "\n",
            "Opt Risky Allocation = 1.402, Opt Val = -0.291\n",
            "Bias Weight = 0.045\n",
            "W_t Weight = 1.145\n",
            "x_t Weight = 0.064\n",
            "x_t^2 Weight = -0.023\n",
            "\n",
            "Time 1\n",
            "\n",
            "Opt Risky Allocation = 1.500, Opt Val = -0.328\n",
            "Bias Weight = 0.000\n",
            "W_t Weight = 1.070\n",
            "x_t Weight = 0.060\n",
            "x_t^2 Weight = -0.020\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.weights[0].weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDLmoABrUp9Y",
        "outputId": "2f22396d-7230-4003-a077-473cd92b0eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.0056287 ,  1.06953962,  0.07161126, -0.02476941]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 Reinforcement Learning"
      ],
      "metadata": {
        "id": "sHGmfW7yVszD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous section, we used backward induction approximate dynamic programming in order to solve the problem. However, in many cases, we do not know the actual dynamics of the system.\n",
        "\n",
        "In other words, we may not knwo the change in distribution of the outcome when we take some action instead of another. Unluckily, sometimes, the only way to be able to do so is to try the action.\n",
        "\n",
        "However, there is usually still some reqularity. In other words, we do not have to learn all possible occurence (We do not have to learn all state action pair to get the full picture of the Q function.)\n",
        "\n",
        "What we do is to learn from enough state action pair and hope that we can interpolate what we have learned into other cases."
      ],
      "metadata": {
        "id": "61gWTP4SVPGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this problem specfically, let us first create an MDP for this problem. (Note that we cannot do the backward induction, because we cannot suddenly start at the last timestep without passing through all the previous time steps)"
      ],
      "metadata": {
        "id": "XO-rUKXZXAZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First, we create a class for the state\n",
        "# Note that the current state will contain wealth and time\n",
        "#Let say that wealth is stored directly as float and time is stored directly as int for simplicity\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class WealthTimeState:\n",
        "    wealth: float\n",
        "    time: int\n",
        "\n",
        "#Next, we create the action state. In this case, action will also be float, but we will save it in a class for systematicity\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class AllocationState:\n",
        "    allocation: float"
      ],
      "metadata": {
        "id": "aJQaj_o3Xcjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will create the MDP"
      ],
      "metadata": {
        "id": "ZXNb_ci3av8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class DiscreteAssetAllocationMDP(MarkovDecisionProcess[WealthTimeState, AllocationState]):\n",
        "    risky_return_distributions: Sequence[Distribution[float]]\n",
        "    riskless_returns: Sequence[float]\n",
        "    utility_func: Callable[[float], float]\n",
        "    risky_alloc_choices: Sequence[float]\n",
        "    feature_functions: Sequence[Callable[[Tuple[float, float]], float]]\n",
        "    dnn_spec: DNNSpec\n",
        "    initial_wealth_distribution: Distribution[float]\n",
        "\n",
        "    def step(self, non_terminal_state: NonTerminal[WealthTimeState], allocation: AllocationState) -> SampledDistribution[Tuple[State[WealthTimeState], float]]:\n",
        "\n",
        "        def sample_next_state_reward(non_terminal_state=non_terminal_state,allocation=allocation) -> Tuple[State[WealthTimeState], float]:\n",
        "            t = non_terminal_state.state.time\n",
        "            wealth = non_terminal_state.state.wealth\n",
        "            next_wealth: float = allocation.allocation * (1 + self.risky_return_distributions[t].sample()) + (wealth - allocation.allocation) * (1 + self.riskless_returns[t])\n",
        "            reward: float = self.utility_func(next_wealth) if t == steps - 1 else 0.\n",
        "            next_state: State[float] = Terminal(WealthTimeState(wealth=next_wealth, time=t+1)) if t == steps - 1 else NonTerminal(WealthTimeState(wealth=next_wealth, time=t+1))\n",
        "            return (next_state, reward)\n",
        "        return SampledDistribution(sampler=sample_next_state_reward, expectation_samples=100)\n",
        "\n",
        "    #Possible actions\n",
        "    def actions(self, non_terminal_state: NonTerminal[WealthTimeState]) -> Sequence[float]:\n",
        "      return [AllocationState(allocation=allocation) for allocation in alloc_choices]"
      ],
      "metadata": {
        "id": "t9rrvh-ibLLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps: int = 2\n",
        "μ: float = 0.13\n",
        "σ: float = 0.2\n",
        "r: float = 0.07\n",
        "a: float = 1.0\n",
        "init_wealth: float = 1.0\n",
        "init_wealth_stdev: float = 0.1\n",
        "\n",
        "excess: float = μ - r\n",
        "var: float = σ * σ\n",
        "base_alloc: float = excess / (a * var)\n",
        "\n",
        "risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
        "riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
        "utility_function: Callable[[float], float] = lambda x: - np.exp(-a * x) / a\n",
        "alloc_choices: Sequence[float] = np.linspace(2 / 3 * base_alloc, 4 / 3 * base_alloc,11)\n",
        "feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = [lambda _: 1., lambda w_x: w_x[0], lambda w_x: w_x[1], lambda w_x: w_x[1] * w_x[1]]\n",
        "dnn: DNNSpec = DNNSpec(neurons=[], bias=False, hidden_activation=lambda x: x, hidden_activation_deriv=lambda y: np.ones_like(y), output_activation=lambda x: - np.sign(a) * np.exp(-x),output_activation_deriv=lambda y: -y)\n",
        "init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_stdev)\n",
        "\n",
        "Hi = DiscreteAssetAllocationMDP(risky_ret, riskless_ret, utility_function, alloc_choices, feature_funcs, dnn, init_wealth_distr)"
      ],
      "metadata": {
        "id": "c3dTP7LShiMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The start state can be seen as when the wealth is the initial wealth and time = 0\n",
        "def get_start_states_distribution(init_wealth_distr) -> SampledDistribution[NonTerminal[WealthTimeState]]:\n",
        "  def states_sampler_func() -> NonTerminal[float]:\n",
        "    wealth: float = init_wealth_distr.sample()\n",
        "    return NonTerminal(WealthTimeState(wealth=wealth, time=1))\n",
        "  return SampledDistribution(states_sampler_func)\n"
      ],
      "metadata": {
        "id": "XyASZ7EWk94g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_qvf_func_approx(feature_funcs, dnn) ->  DNNApprox[Tuple[NonTerminal[WealthTimeState], AllocationState]]:\n",
        "\n",
        "        adam_gradient: AdamGradient = AdamGradient(learning_rate=0.1, decay1=0.9, decay2=0.999)\n",
        "        ffs: List[Callable[[Tuple[NonTerminal[WealthTimeState], AllocationState]], float]] = []\n",
        "        for f in feature_funcs:\n",
        "            def this_f(pair: Tuple[NonTerminal[WealthTimeState], AllocationState], f=f) -> float:\n",
        "                return f((pair[0].state.wealth, pair[1].allocation))\n",
        "            ffs.append(this_f)\n",
        "\n",
        "        return DNNApprox.create(feature_functions=ffs,dnn_spec=dnn,adam_gradient=adam_gradient)\n"
      ],
      "metadata": {
        "id": "vtu-Azcbq5Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Hey = q_learning_epsilon_greedy(mdp=Hi, epsilon=0.1, start_state_distribution= get_start_states_distribution(init_wealth_distr),\n",
        "    initial_func_approx = get_qvf_func_approx(feature_funcs, dnn),\n",
        "    gamma = 1,\n",
        "    max_episode_length = 10)"
      ],
      "metadata": {
        "id": "lNquVjpBic9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(Hey)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0UYHUJcpx_Y",
        "outputId": "a95c06c1-c0bd-48ad-b919-02a7fcffc141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DNNApprox(feature_functions=[<function get_qvf_func_approx.<locals>.this_f at 0x7f963b127320>, <function get_qvf_func_approx.<locals>.this_f at 0x7f963b1278c0>, <function get_qvf_func_approx.<locals>.this_f at 0x7f963b1275f0>, <function get_qvf_func_approx.<locals>.this_f at 0x7f963b1277a0>], dnn_spec=DNNSpec(neurons=[], bias=False, hidden_activation=<function <lambda> at 0x7f9646bd28c0>, hidden_activation_deriv=<function <lambda> at 0x7f9646bd2680>, output_activation=<function <lambda> at 0x7f9646bd2ef0>, output_activation_deriv=<function <lambda> at 0x7f963b127050>), regularization_coeff=0.0, weights=[Weights(adam_gradient=AdamGradient(learning_rate=0.1, decay1=0.9, decay2=0.999), time=4, weights=array([[ 0.62199951, -0.50970893,  0.73662396,  0.26585708]]), adam_cache1=array([[-0.00448753, -0.00381631, -0.00897506, -0.01795012]]), adam_cache2=array([[3.08323590e-05, 2.99194135e-05, 1.23329436e-04, 4.93317743e-04]]))])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. RL for Non-Finite MDP: Banker Problem"
      ],
      "metadata": {
        "id": "1lrQyluSK58H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the action of borrowing and investing is to be done in the end of the day, we let the state be the observation at the end of the day.\n",
        "\n",
        "1. Payback: We borrow $y$ yesterday and have to pay it back $(1+R)y$\n",
        "2. Deposit: Let assume that the incoming deposit is a random variable $D$\n",
        "3. Withdrawal: Let assume that the incoming withdrawal is a random varaible $W$\n",
        "4. Penalty: If the start cash is less than $C$, we have to pay a penalty of $ \\cot\\left({\\frac{\\pi c}{2C}}\\right)$.\n",
        "5. Spot Prize Growth: from 1 to $S$\n",
        "6. Number of Investment: $v$\n",
        "7. Borrow: To get more cash at the end of the day\n",
        "8. Postponed Withdraw: $\\delta$\n",
        "9. Total balance: $b$\n",
        "\n",
        "\n",
        "Assume that we start the day with cash = $c$, debt = $y$, and postponed withdrawal = $\\delta$\n",
        "\n",
        "Before the start of the business day, the amount of money we have will be (after the penalty has been paid)\n",
        "\\begin{align*}\n",
        "  c \\ge 0\n",
        "\\end{align*}\n",
        "Since $c$ can be adjusted by borrowing, this constraint can be staisfied with sufficient amount of borrowing.\n",
        "\n",
        "After the morning, the amount of cash we have will be\n",
        "\\begin{align*}\n",
        "  c + D\n",
        "\\end{align*}\n",
        "Note that this value will be non-negative\n",
        "\n",
        "After the afternoon, the amount of cash we have will be\n",
        "\\begin{align*}\n",
        "  \\max(c + D - W - \\delta, 0)\n",
        "\\end{align*}\n",
        "and we postpone the withdrawal of $\\max(W + \\delta - D - c +p(c), 0)$ to tomorrow.\n",
        "\n",
        "\n",
        "At the evening, the amount of cash we have will be\n",
        "\\begin{align*}\n",
        "  \\max(c + D - W - \\delta, 0)\n",
        "\\end{align*}\n",
        "and the total balance becomes $b+D-W$.\n",
        "\n",
        "Thus, the total wealth will be cash $ = v S + \\max(c - p(c) + D - W - \\delta, 0) - y(1-R)$.\n",
        "\n",
        "The decision we have to make is to select $v$ and $y$ such that the next day welath become $c = \\text{cash} - v + b$."
      ],
      "metadata": {
        "id": "WIbHk98qLS7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 State"
      ],
      "metadata": {
        "id": "GN-2GKNgLYeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We consider the state to consist of the following quantity.\n",
        "1. balance: $b \\ge 0$ \n",
        "2. postpone: $\\delta \\ge 0$\n",
        "3. total wealth: $x$ can be negative\n",
        "4. time: $t \\ge 0$\n",
        "\n",
        "The transition rule for time is $t' = t+1$ independent from every action or variable.\n",
        "\n",
        "$b$ will determine $W$ since $W$ cannot exceed $b$. Let $D$ be independent. \n",
        "\n",
        "Let $S$ be independent.\n",
        "\n",
        "Thus, given that the previous day we have $S = (b, \\delta, x, t)$.\n",
        "\n",
        "We selected action $A = (v, y)$ where $v$ is investment, and $y$ is amount borrowed.\n",
        "\n",
        "1. $c = x-v+y$ (At that time)\n",
        "2. $c = c - p(c)$ (At that time)\n",
        "3. $D$ is drawn from $f_D$ (Before the morning)\n",
        "4. $W$ is drawn from $f_W(b)$ (We draw it before updating $b$ since we assume that people will not depsoit and withdraw in the same day). (Before the morning)\n",
        "5. $c = c + D$ (After the morning)\n",
        "6. $b = b + D$ (After the morning)\n",
        "7. $W = W + \\delta$ (Before the afternoon)\n",
        "8. $\\delta = \\max(W-c, 0)$ (After the afternoon)\n",
        "9. $c = \\max(c-W, 0)$ (After the afternoon)\n",
        "10. $b = b - W$ (After the afternoon)\n",
        "11. $S$ is drawn from $f_S$ (After the afternoon)\n",
        "12. $x = vS + c - (1+R)y$\n",
        "13. $t = t+1$\n",
        "\n",
        "To sum up,\n",
        "1. $c = x-v+y$ \n",
        "2. $c = c - p(c) \\ge 0$\n",
        "3. $D, W, S$ are drawn from $f_D, f_W(b), f_S$, respectively.\n",
        "4. $b' = b + D - W$\n",
        "5. $c, \\delta' = \\max(c+D-W-\\delta, 0), \\max(W+\\delta-c-D, 0)$\n",
        "6. $x' = vS + c - (1+R)y$\n",
        "7. $t' = t+1$\n",
        "\n",
        "The state\n",
        "\n",
        "\\begin{align*}\n",
        "  \\mathcal{S} &= \\left\\{ (b, \\delta, x, t) \\in \\mathbf{R}_{+} \\times \\mathbf{R}_{+} \\times \\mathbf{R} \\times \\mathbf{N}_0 \\vert t \n",
        "  \\le T\\right\n",
        "  \\}\\\\\n",
        "  \\mathcal{N} &= \\left\\{ (b, \\delta, x, t) \\in \\mathbf{R}_{+} \\times \\mathbf{R}_{+} \\times \\mathbf{R} \\times \\mathbf{N}_0 \\vert t \n",
        "  \\le T-1\\right\n",
        "  \\}\\\\\n",
        "  \\mathcal{T} &= \\mathbf{R}_{+} \\times \\mathbf{R}_{+} \\times \\mathbf{R} \\times \\{T\\}\n",
        "\\end{align*}\n",
        "\n",
        "The action (which is dependent on the state)\n",
        "\\begin{align*}\n",
        "  \\mathcal{A}(x) &= \\left\\{ (v, y) \\in \\mathbf{R}_{+} \\times \\mathbf{R}_{+} \\vert x-v+y \\ge C \\text{ or } x-v+y \\ge \\cot\\left({\\frac{\\pi x-v+y}{2C}}\\right)\\right \\}\n",
        "\\end{align*}\n",
        "\n",
        "The reward will be $0$ at every transotion, except for at the transition from time $T-1$ to $T$, where the reward will be the final $x$. This problem can then be solved by backward induction with ADP."
      ],
      "metadata": {
        "id": "VkCTh5xjLZ1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class State:\n",
        "  b: float\n",
        "  delta: float\n",
        "  x: float\n",
        "  t: int\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Action:\n",
        "  v: float\n",
        "  y: float"
      ],
      "metadata": {
        "id": "gWT4_P26tEO_"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cFZEasgDA63Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vr_pS8rHI_SC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Assignment13.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}