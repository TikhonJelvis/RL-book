{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Mapping, Dict, Sequence, Iterable, Set, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.distribution import Categorical, Constant\n",
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Mapping\n",
    "\n",
    "SPACE = 'SPACE'\n",
    "BLOCK = 'BLOCK'\n",
    "GOAL = 'GOAL'\n",
    "\n",
    "maze_grid: Mapping[Tuple[int, int], str] = {(0, 0): SPACE, (0, 1): BLOCK, (0, 2): SPACE, (0, 3): SPACE, (0, 4): SPACE, \n",
    "             (0, 5): SPACE, (0, 6): SPACE, (0, 7): SPACE, (1, 0): SPACE, (1, 1): BLOCK,\n",
    "             (1, 2): BLOCK, (1, 3): SPACE, (1, 4): BLOCK, (1, 5): BLOCK, (1, 6): BLOCK, \n",
    "             (1, 7): BLOCK, (2, 0): SPACE, (2, 1): BLOCK, (2, 2): SPACE, (2, 3): SPACE, \n",
    "             (2, 4): SPACE, (2, 5): SPACE, (2, 6): BLOCK, (2, 7): SPACE, (3, 0): SPACE, \n",
    "             (3, 1): SPACE, (3, 2): SPACE, (3, 3): BLOCK, (3, 4): BLOCK, (3, 5): SPACE, \n",
    "             (3, 6): BLOCK, (3, 7): SPACE, (4, 0): SPACE, (4, 1): BLOCK, (4, 2): SPACE, \n",
    "             (4, 3): BLOCK, (4, 4): SPACE, (4, 5): SPACE, (4, 6): SPACE, (4, 7): SPACE, \n",
    "             (5, 0): BLOCK, (5, 1): BLOCK, (5, 2): SPACE, (5, 3): BLOCK, (5, 4): SPACE, \n",
    "             (5, 5): BLOCK, (5, 6): SPACE, (5, 7): BLOCK, (6, 0): SPACE, (6, 1): BLOCK, \n",
    "             (6, 2): BLOCK, (6, 3): BLOCK, (6, 4): SPACE, (6, 5): BLOCK, (6, 6): SPACE, \n",
    "             (6, 7): SPACE, (7, 0): SPACE, (7, 1): SPACE, (7, 2): SPACE, (7, 3): SPACE, \n",
    "             (7, 4): SPACE, (7, 5): BLOCK, (7, 6): BLOCK, (7, 7): GOAL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class GridState:\n",
    "    \n",
    "    x: int\n",
    "    y: int\n",
    "    \n",
    "    def get_neighbor(self, direction: str):\n",
    "        if direction == \"Up\":\n",
    "            return GridState(self.x, self.y-1)\n",
    "        elif direction == \"Down\":\n",
    "            return GridState(self.x, self.y+1)\n",
    "        elif direction == \"Left\":\n",
    "            return GridState(self.x-1, self.y)\n",
    "        elif direction == \"Right\":\n",
    "            return GridState(self.x+1, self.y)\n",
    "        else:\n",
    "            assert False, \"invalid move\"\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        '''Your code here, implement a comparison function that should satisfy'''\n",
    "        return (self.x, self.y) < (other.x, other.y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMazeMDP(FiniteMarkovDecisionProcess[GridState, int], abc.ABC):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_map: Mapping[Tuple[int,int], str]\n",
    "    ):\n",
    "        self.states: Set[GridState] = set([GridState(x[0], x[1]) for x, y in state_map.items()\n",
    "                                           if y == 'SPACE' or y == 'GOAL'])\n",
    "        self.moves = [\"Up\", \"Down\", \"Left\", \"Right\"]\n",
    "        self.goal: GridState = [GridState(x[0], x[1]) for x, y in state_map.items() if y == 'GOAL'][0]\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self):\n",
    "        d: Dict[GridState, Dict[str, Categorical[Tuple[GridState, float]]]] = {}\n",
    "\n",
    "        for state in self.states\n",
    "            if state != self.goal:\n",
    "                d1: Dict[str, Categorical[Tuple[GridState, float]]] = {}\n",
    "                for move in self.moves:\n",
    "                    next_state = state.get_neighbor(move)\n",
    "                    if next_state in self.states:\n",
    "                        d1[move] = Constant((next_state, self.reward_func(next_state)))\n",
    "\n",
    "                d[state] = d1\n",
    "        return d\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def reward_func(self, next_state) -> float:\n",
    "        pass\n",
    "\n",
    "        \n",
    "class GridMazeMDP_Dense(GridMazeMDP):\n",
    "    \n",
    "    def reward_func(self, next_state) -> float:\n",
    "        return -1\n",
    "\n",
    "class GridMazeMDP_Sparse(GridMazeMDP):\n",
    "    \n",
    "    def reward_func(self, next_state) -> float:\n",
    "        if next_state == self.goal:\n",
    "            return 1\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.dynamic_programming import value_iteration\n",
    "from typing import (Callable, Iterable, Iterator, Optional, TypeVar)\n",
    "from rl.iterate import converged, iterate, last\n",
    "from rl.markov_process import NonTerminal\n",
    "from rl.markov_decision_process import (FiniteMarkovDecisionProcess,\n",
    "                                        FiniteMarkovRewardProcess)\n",
    "from rl.policy import DeterministicFinitePolicy\n",
    "from rl.dynamic_programming import value_iteration, almost_equal_vfs, greedy_policy_from_vf\n",
    "X = TypeVar('X')\n",
    "Y = TypeVar('Y')\n",
    "A = TypeVar('A')\n",
    "S = TypeVar('S')\n",
    "\n",
    "DEFAULT_TOLERANCE = 1e-8\n",
    "\n",
    "# A representation of a value function for a finite MDP with states of\n",
    "# type S\n",
    "V = Mapping[NonTerminal[S], float]\n",
    "\n",
    "def tracked_converge(values: Iterator[X], done: Callable[[X, X], bool]) -> Iterator[X]:\n",
    "    '''Read from an iterator until two consecutive values satisfy the\n",
    "    given done function or the input iterator ends.\n",
    "    Raises an error if the input iterator is empty.\n",
    "    Will loop forever if the input iterator doesn't end *or* converge.\n",
    "    '''\n",
    "    a = next(values, None)\n",
    "    if a is None:\n",
    "        return\n",
    "\n",
    "    yield a\n",
    "\n",
    "    for i,b in enumerate(values):\n",
    "        if done(a, b):\n",
    "            print(f'took {i} iterations to converge')  ### This is the only part you needed to change\n",
    "            return\n",
    "\n",
    "        a = b\n",
    "        yield b\n",
    "\n",
    "def tracked_converged(values: Iterator[X],\n",
    "              done: Callable[[X, X], bool]) -> X:\n",
    "    '''Return the final value of the given iterator when its values\n",
    "    converge according to the done function.\n",
    "    Raises an error if the iterator is empty.\n",
    "    Will loop forever if the input iterator doesn't end *or* converge.\n",
    "    '''\n",
    "    result = last(tracked_converge(values, done))\n",
    "\n",
    "    if result is None:\n",
    "        raise ValueError(\"converged called on an empty iterator\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def almost_equal_vfs(\n",
    "    v1: V[S],\n",
    "    v2: V[S],\n",
    "    tolerance: float = DEFAULT_TOLERANCE\n",
    ") -> bool:\n",
    "    '''Return whether the two value function tables are within the given\n",
    "    tolerance of each other.\n",
    "    '''\n",
    "    return max(abs(v1[s] - v2[s]) for s in v1) < tolerance\n",
    "\n",
    "def tracked_value_iteration_result(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    gamma: float\n",
    ") -> Tuple[V[S], DeterministicFinitePolicy[S, A]]:\n",
    "    opt_vf: V[S] = tracked_converged(\n",
    "        value_iteration(mdp, gamma),\n",
    "        done=almost_equal_vfs\n",
    "    )\n",
    "    opt_policy: DeterministicFinitePolicy[S, A] = greedy_policy_from_vf(\n",
    "        mdp,\n",
    "        opt_vf,\n",
    "        gamma\n",
    "    )\n",
    "\n",
    "    return opt_vf, opt_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmdp = GridMazeMDP_Dense(maze_grid)\n",
    "gmdp2 = GridMazeMDP_Sparse(maze_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense rewards:\n",
      "took 16 iterations to converge\n",
      "sparse rewards:\n",
      "took 16 iterations to converge\n"
     ]
    }
   ],
   "source": [
    "print(\"dense rewards:\")\n",
    "vf, op = tracked_value_iteration_result(gmdp, gamma=1)\n",
    "print(\"sparse rewards:\")\n",
    "vf_2, op_2 = tracked_value_iteration_result(gmdp2, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_2.action_for == op.action_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
