## Summary and Real-World Considerations {#sec:concluding-chapter}

The purpose of this chapter is two-fold: Firstly to summarize the key learnings from this book, and secondly to provide some commentary on how to take the learnings from this book into practice (to solve real-world problems). On the latter, we specifically focus on the challenges one faces in the real-world—modeling difficulties, problem-size difficulties, operational challenges, data challenges (access, cleaning, organization), product management challenges (e.g., addressing the gap between the technical problem being solved and the business problem to be solved), and also change-management challenges as one shifts an enterprise from legacy systems to an AI system.

### Summary of Key Learnings from This Book

In Module I, we covered the Markov Decision Process framework, the Bellman Equations, Dynamic Programming algorithms, Function Approximation, and Approximate Dynamic Programming.

Module I started with Chapter [-@sec:mrp-chapter], where we first introduced the very important *Markov Property*, a concept that enables us to reason effectively and compute efficiently in practical systems involving sequential uncertainty. Such systems are best approached through the very simple framework of *Markov Processes*, involving probabilistic state transitions. Next, we developed the framework of *Markov Reward Processes* (MRP), the *MRP Value Function*, and the *MRP Bellman Equation*, which expresses the MRP Value Function recursively. We showed how this MRP Bellman Equation can be solved with simple linear-algebraic-calculations when the state space is finite and not too large.

In Chapter [-@sec:mdp-chapter], we developed the framework of *Markov Decision Processes* (MDP). A key learning from this chapter is that an MDP evaluated with a fixed Policy is equivalent to an MRP. Calculating the Value Function of an MDP evaluated with a fixed Policy (i.e. calculating the Value Function of an MRP) is known as the *Prediction* problem. We developed the 4 forms of the MDP Bellman Policy Equations (which are essentially equivalent to the MRP Bellman Equation). Next, we defined the *Control* problem as the calculation of the Optimal Value Function (and an associated Optimal Policy) of an MDP. Correspondingly, we developed the 4 forms of the MDP Bellman Optimality Equation. We stated and proved an important theorem on the existence of an Optimal Policy, and of each Optimal Policy achieving the Optimal Value Function. We finished this chapter with some commentary on variants and extensions of MDPs. Here we introduced the two Curses in the context of solving MDP Prediction and Control—the Curse of Dimensionality and the Curse of Modeling, which can be battled with appropriate approximation of the Value Function and with appropriate sampling from the state-reward transition probability function. Here, we also covered Partially-Observable Markov Decision Processes (POMDP), which refers to situations where all components of the *State* are not observable (quite typical in the real-world). Often, we pretend a POMDP is an MDP as the MDP framework fetches us computational tractability. Modeling a POMDP as an MDP is indeed a very challenging endeavor in the real-world. However, sometimes partial state-observability cannot be ignored, and in such situations, we have to employ (computationally expensive) algorithms to solve the POMDP.

In Chapter [-@sec:dp-chapter], we first covered the foundation of the classical Dynamic Programming (DP) algorithms—the Banach Fixed-Point Theorem, which gives us a simple method for iteratively solving for a fixed-point of a contraction function. Next, we constructed the Bellman Policy Operator and showed that it's a contraction function, meaning we can take advantage of the Banach Fixed-Point Theorem, yielding a DP algorithm to solve the Prediction problem, referred to as the Policy Evaluation algorithm. Next, we introduced the notions of a Greedy Policy and Policy Improvement, which yields a DP algorithm known as  Policy Iteration to solve the Control problem. Next, we constructed the Bellman Optimality Operator and showed that it's a contraction function, meaning we can take advantage of the Banach Fixed-Point Theorem, yielding a DP algorithm to solve the Control problem, referred to as the Value Iteration algorithm. Next, we introduced the all-important concept of *Generalized Policy Iteration* (GPI)—the powerful idea of alternating between *any* method for Policy Evaluation and *any* method for Policy Improvement, including methods that are partial applications of Policy Evaluation or Policy Improvement. This generalized perspective unifies almost all of the algorithms that solve MDP Control problems (including Reinforcement Learning algorithms). We finished this chapter with coverage of Backward Induction algorithms to solve Prediction and Control problems for finite-horizon MDPs—Backward Induction is a simple technique to backpropagate the Value Function from horizon-end to the start. It is important to note that the DP algorithms in this chapter apply to MDPs with a finite number of states and that these algorithms are computationally feasible only if the state space is not too large (the next chapter extends these DP algorithms to handle large state spaces, including infinite state spaces).

In Chapter [-@sec:funcapprox-chapter], we first covered a refresher on Function Approximation by developing the calculations first for linear function approximation and then for feed-forward fully-connected deep neural networks. We also explained that a Tabular prediction can be viewed as a special form of function approximation (since it satisfies the interface we designed for Function Approximation). With this apparatus for Function Approximation, we extended the DP algorithms of the previous chapter to Approximate Dynamic Programming (ADP) algorithms in a rather straightforward manner. In fact, DP algorithms can be viewed as special cases of ADP algorithms by setting the function approximation to be Tabular. Essentially, we replace tabular Value Function updates with updates to Function Approximation parameters (where the Function Approximation represents the Value Function). The sweeps over all states in the tabular (DP) algorithms are replaced by sampling states in the ADP algorithms, and expectation calculations in Bellman Operators are handled in ADP as averages of the corresponding calculations over transition samples (versus calculations using explicit transition probabilities in the DP algorithms).

Module II was about Modeling Financial Applications as MDPs. We started Module II with a basic coverage of Utility Theory in Chapter [-@sec:utility-theory-chapter]. The concept of Utility is vital since Utility of cashflows is the appropriate *Reward* in the MDP for many financial applications. In this chapter, we explained that an individual's financial risk-aversion is represented by the concave nature of the individual's Utility as a function of financial outcomes. We showed that the Risk-Premium (compensation an individual seeks for taking financial risk) is roughly proportional to the individual's financial risk-aversion and also proportional to the measure of uncertainty in financial outcomes. Risk-Adjusted-Return in finance should be thought of as the Certainty-Equivalent-Value, whose Utility is the Expected Utility across uncertain (risky) financial outcomes. We finished this chapter by covering the Constant Absolute Risk-Aversion (CARA) and the Constant Relative Risk-Aversion (CRRA) Utility functions, along with simple asset allocation examples for each of CARA and CRRA Utility functions.

In Chapter [-@sec:portfolio-chapter], we covered the problem of Dynamic Asset-Allocation and Consumption. This is a fundamental problem in Mathematical Finance of jointly deciding on A) optimal investment allocation (among risky and riskless investment assets) and B) optimal consumption, over a finite horizon. We first covered Merton's landmark paper from 1969 that provided an elegant closed-form solution under assumptions of continuous-time, normal distribution of returns on the assets, CRRA utility, and frictionless transactions. In a more general setting of this problem, we need to model it as an MDP. If the MDP is not too large and if the asset return distributions are known, we can employ finite-horizon ADP algorithms to solve it. However, in typical real-world situations, the action space can be quite large and the asset return distributions are unknown. This points to RL, and specifically RL algorithms that are well suited to tackle large action spaces (such as Policy Gradient Algorithms).

In Chapter [-@sec:derivatives-pricing-chapter], we covered the problem of pricing and hedging of derivative securities. We started with the fundamental concepts of Arbitrage, Market-Completeness and Risk-Neutral Probability Measure. Based on these concepts, we stated and proved the two fundamental theorems of Asset Pricing for the simple case of a single discrete time-step. These theorems imply that the pricing of derivatives in an arbitrage-free and complete market can be done in two equivalent ways: A) Based on construction of a replicating portfolio and B) Based on riskless rate-discounted expectation in the risk-neutral probability measure. Finally, we covered two financial trading problems that can be cast as MDPs. The first problem is the Optimal Exercise of American Options (and its generalization to Optimal Stopping problems). The second problem is the Pricing and Hedging of Derivatives in an Incomplete (real-world) Market.

In Chapter [-@sec:order-book-algos-chapter], we covered problems involving trading optimally on an Order Book. We started with developing an understanding of the core ingredients of an Order Book: Limit Orders, Market Orders, Order Book Dynamics, and Price Impact. The rest of the chapter covered two important problems that can be cast as MDPs. These are the problems of Optimal Order Execution and Optimal Market-Making. For each of these two problems, we derived closed-form solutions under highly simplified assumptions (e.g., Bertsimas-Lo, Avellaneda-Stoikov formulations), which helps develop intuition. Since these problems are modeled as finite-horizon MDPs, we can implement backward-induction ADP algorithms to solve them. However, in practice, we need to develop Reinforcement Learning algorithms (and associated market simulators) to solve these problems in real-world settings to overcome the Curse of Dimensionality and Curse of Modeling.

Module III covered Reinforcement Learning algorithms. Module III starts by motivating the case for Reinforcement Learning (RL). In the real-world, we typically do not have access to a model of state-reward transition probabilities. Typically, we simply have access to an environment, that serves up the next state and reward, given current state and action, at each step in the AI Agent's interaction with the environment. The environment could be the real environment or could be a simulated environment (the latter from a learnt model of the environment). RL algorithms for Prediction/Control learn the requisite Value Function/Policy by obtaining sufficient data (*atomic experiences*) from interaction with the environment. This is a sort of "trial and error" learning, through a process of prioritizing actions that seems to fetch good rewards, and deprioritizing actions that seem to fetch poor rewards. Specifically, RL algorithms are in the business of learning an approximate Q-Value Function, an estimate of the Expected Return for any given action in any given state. The success of RL algorithms depends not only on their ability to learn the Q-Value Function in an incremental manner through interactions with the environment, but also on their ability to perform good generalization of the Q-Value Function with appropriate function approximation (often using deep neural networks, in which case we term it as Deep RL). Most RL algorithms are founded on the Bellman Equations and all RL Control algorithms are based on the fundamental idea of *Generalized Policy Iteration*. 

In Chapter [-@sec:rl-prediction-chapter], we covered RL Prediction algorithms. Specifically, we covered Monte-Carlo (MC) and Temporal-Difference (TD) algorithms for Prediction. A key learning from this chapter was the Bias-Variance tradeoff in MC versus TD. Another key learning was that while MC Prediction learns the statistical mean of the observed returns, TD Prediction learns something "deeper"—TD implicitly estimates an MRP from the observed data and produces the Value Function of the implicitly-estimated MRP. We emphasized viewing TD versus MC versus DP from the perspectives of "bootstrapping" and "experiencing". We finished this chapter by covering $\lambda$-Return Prediction and TD($\lambda$) Prediction algorithms, which give us a way to tradeoff bias versus variance (along the spectrum of MC to TD) by tuning the $\lambda$ parameter. TD is equivalent to TD(0) and MC is "equivalent" to TD(1).

In Chapter [-@sec:rl-control-chapter], we covered RL Control algorithms. We re-emphasized that RL Control is based on the idea of Generalized Policy Iteration (GPI). We explained that Policy Evaluation is done for the $Q$-Value Function (instead of the State-Value Function), and that the Improved Policy needs to be exploratory, e.g., $\epsilon$-greedy. Next, we described an important concept—*Greedy in the Limit with Infinite Exploration* (GLIE). Our first RL Control algorithm was GLIE Monte-Carlo Control. Next, we covered two important TD Control algorithms: SARSA (which is On-Policy) and Q-Learning (which is Off-Policy). We briefly covered Importance Sampling, which is a different way of doing Off-Policy algorithms. We wrapped up this chapter with some commentary on the convergence of RL Prediction and RL Control algorithms. We highlighted a strong pattern of situations when we run into convergence issues—it is when all three of [Bootstrapping, Function Approximation, Off-Policy] are done together. We've seen how each of these three is individually beneficial, but when the three come together, it's "too much of a good thing", bringing about convergence issues. The confluence of these three is known as the *Deadly Triad* (an example of this would be Q-Learning with Function Approximation).

In Chapter [-@sec:batch-rl-chapter], we covered the more nuanced RL Algorithms, going beyond the plain-vanilla MC and TD algorithms we covered in chapters [-@sec:rl-prediction-chapter] and [-@sec:rl-control-chapter]. We started this chapter by introducing the novel ideas of *Batch RL* and *Experience-Replay*. Next, we covered the Least-Squares Monte-Carlo (LSMC) Prediction algorithm and the Least-Squares Temporal-Difference (LSTD) algorithm, which is a direct (gradient-free) solution of Batch TD. Next, we covered the very important Deep Q-Networks (DQN) algorithm, which uses Experience-Replay and fixed Q-learning targets, in order to avoid the pitfalls of time-correlation and varying TD Target. Next, we covered the Least-Squares Policy Iteration (LSPI) algorithm, which is an Off-Policy, Experience-Replay Control Algorithm using LSTDQ for Policy Evaluation. Then we showed how Optimal Exercise of American Options can be tackled with LSPI and Deep Q-Learning algorithms. In the second half of this chapter, we looked deeper into the issue of the *Deadly Triad* by viewing Value Functions as Vectors so as to understand Value Function Vector transformations with a balance of geometric intuition and mathematical rigor, providing insights into convergence issues for a variety of traditional loss functions used to develop RL algorithms. Finally, this treatment of Value Functions as Vectors led us in the direction of overcoming the Deadly Triad by defining an appropriate loss function, calculating whose gradient provides a more robust set of RL algorithms known as Gradient Temporal Difference (Gradient TD).

In Chapter [-@sec:policy-gradient-chapter], we covered Policy Gradient (PG) algorithms, which are based on GPI with Policy Improvement as a Stochastic Gradient Ascent for an *Expected Returns Objective* using a policy function approximation. We started with the Policy Gradient Theorem that gives us a simple formula for the gradient of the Expected Returns Objective in terms of the score of the policy function approximation. Our first PG algorithm was the REINFORCE algorithm, a Monte-Carlo Policy Gradient algorithm with no bias but high variance. We showed how to tackle the Optimal Asset Allocation problem with REINFORCE. Next, we showed how we can reduce variance in PG algorithms by using a critic and by using an estimate of the advantage function in place of the Q-Value Function. Next, we showed how to overcome bias in PG Algorithms based on the *Compatible Function Approximation Theorem*. Finally, we covered two specialized PG algorithms that have worked well in practice—Natural Policy Gradient and Deterministic Policy Gradient. We also provided some coverage of Evolutionary Strategies, which are technically not RL algorithms, but they resemble PG Algorithms and can sometimes be quite effective in solving MDP Control problems.

In Module IV, we provided some finishing touches by covering the topic of Exploration versus Exploitation and the topic of Blending Learning and Planning in some detail. In Chapter [-@sec:multi-armed-bandits-chapter], we provided significant coverage of algorithms for the Multi-Armed Bandit (MAB) problem, which provides a simple setting to understand and appreciate the nuances of the Explore versus Exploit dilemma that we typically need to resolve within RL Control algorithms. We started with simple methods such as Naive Exploration (e.g., $\epsilon$-greedy) and Optimistic Initialization. Next, we covered methods based on the broad approach of *Optimism in the Face of Uncertainty* (e.g., Upper-Confidence Bounds). Next, we covered the powerful and practically effective method of Probability Matching (e.g., Thompson Sampling). Then we also covered Gradient Bandit Algorithms and a disciplined approach to balancing exploration and exploitation by forming Information State Space MDPs (incorporating value of Information), typically solved by treating as Bayes-Adaptive MDPs. Finally, we noted that the above MAB algorithms are well-extensible to Contextual Bandits and RL Control.

In Chapter [-@sec:blending-learning-planning-chapter], we covered the issue of Planning versus Learning, and showed how to blend Planning and Learning. Next, we covered Monte-Carlo Tree-Search (MCTS), which is a Planning algorithm based on Tree-Search and based on sampling/RL techniques. Lastly, we covered Adaptive Multi-Stage Sampling (AMS), that we consider to be the spiritual origin of MCTS—it is an efficient algorithm for finite-horizon MDPs with very large state space and fairly small action space.

### RL in the Real-World

Although this is an academic book on the Foundations of RL, we (the authors of this book) have significant experience in leveraging the power of Applied Mathematics to solve problems in the real-world. So we devote this subsection to the various nuances of applying RL in the real-world.

The most important point we'd like to make is that in order to develop models and algorithms that will be effective in the real-world, one should not only have a deep technical understanding but to also have a deep understanding of the business domain. If the business domain is Financial Trading, one needs to be well-versed in the practical details of the specific market one is working in, and the operational details and transactional frictions involved in trading. These details need to be carefully captured in the MDP one is constructing. These details have ramifications on the choices made in defining the state space and the action space. More importantly, defining the reward function is typically not an obvious choice at all—it requires considerable thought and typically one would need to consult with the business head to identify what exactly is the objective function in running the business, e.g., the precise definition of the Utility function. One should also bear in mind that a typical real-world problem is actually a Partially Observable Markov Decision Process (POMDP) rather than an MDP. In the pursuit of computational tractability, one might approximate the POMDP as an MDP but in order to do so, one requires strong understanding of the business domain. However, sometimes partial state-observability cannot be ignored, and in such situations, we have to employ (computationally expensive) algorithms to solve the POMDP. Indeed, controlling state space explosion is one of the biggest challenges in the real-world. Much of the effort in modeling an MDP is to define a state space that finds the appropriate balance between capturing the key aspects of the real-world problem and attaining computational tractability.

\index{Markov decision process}
\index{partially observable Markov decision process}

Now we'd like to share the approach we usually take when encountering a new problem, like one of the Financial Applications we covered in Module II. Our first stab at the problem is to create a simpler version of the problem that lends itself to analytical tractability, exploring ways to develop a closed-form solution (like we obtained for some of the Financial Applications in Module II). This typically requires removing some of the frictions and constraints of the real-world problem. For Financial Applications, this might involve assuming no transaction costs, perhaps assuming continuous trading, perhaps assuming no liquidity constraints. There are multiple advantages of deriving a closed-form solution with simplified assumptions. Firstly, the closed-form solution immediately provides tremendous intuition as it shows the analytical dependency of the Optimal Value Function/Optimal Policy on the inputs and parameters of the problem. Secondly, when we eventually obtain the solution to the full-fledged model, we can test the solution by creating a special case of the full-fledged model that reduces to the simplified model for which we have a closed-form solution. Thirdly, the expressions within the closed-form solution provide us with some guidance on constructing appropriate features for function approximation when solving the full-fledged model.

\index{dynamic programming}
\index{dynamic programming!approximate}
The next stage would be to bring in some of the real-world frictions and constraints, and attempt to solve the problem with Dynamic Programming (or Approximate Dynamic Programming). This means we need to construct a model of state-reward transition probabilities. Such a model would be estimated from real-world data obtained from interaction with the real environment. However, often we find that Dynamic Programming (or Approximate Dynamic Programming) is not an option due to the Curse of Modeling (i.e., hard to build a model of transition probabilities). This leaves us with the eventual go-to option of pursuing a Reinforcement Learning technique. In most real-world problems, we'd employ RL not with real environment interactions, but with simulated environment interactions. This means we need to build a sampling model estimated from real-world data obtained from interactions with the real environment. In fact, in many real-world problems, we'd want to augment the data-learnt simulator with human knowledge/assumptions (specifically information that might not be readily obtained from electronic data that a human expert might be knowledgeable about). Having a simulator of the environment is very valuable because we can run it indefinitely and also because we can create a variety of scenarios (with different settings/assumptions) to run the simulator in. Deep Learning-based function approximations have been quite successful in the context of Reinforcement Learning algorithms (we refer to this as Deep Reinforcement Learning). Lastly, it pays to re-emphasize that the learnings from Chapter [-@sec:blending-learning-planning-chapter] are very important for real-world problems. In particular, the idea of blending model-based RL with model-free RL (Figure \ref{fig:planning_learning}) is an attractive option for real-world applications because the real-world is typically not stationary and hence, models need to be updated continuously.

Given the plethora of choices for different types of RL algorithms, it is indeed difficult to figure out which RL algorithm would be most suitable for a given real-world problem. As ever, we recommend starting with a simple algorithm such as the MC and TD methods we used in Chapters [-@sec:rl-prediction-chapter] and [-@sec:rl-control-chapter]. Although the simple algorithms may not be powerful enough for many real-world applications, they are a good place to start to try out on a smaller size of the actual problem—these simple RL algorithms are very easy to implement, reason about and debug. However, the most important advice we can give you is that after having understood the various nuances of the specific real-world problem you want to solve, you should aim to construct an RL algorithm that is customized for your problem. One must recognize that the set of RL algorithms is not a fixed menu to choose from. Rather, there are various pieces of RL algorithms that are open to modification. In fact, we can combine different aspects of different algorithms to suit our specific needs for a given real-world problem. We not only make choices on features in function approximations and on hyper-parameters, we also make choices on the exact design of the algorithm method, e.g., how exactly we'd like to do Off-Policy Learning, or how exactly we'd like to do the Policy Evaluation component of Generalized Policy Iteration in our Control algorithm. In practice, we've found that we often end up with the more advanced algorithms due to the typical real-world problem complexity or state-space/action-space size. There is no silver bullet here, and one has to try various algorithms to see which one works best for the given problem. However, it pays to share that the algorithms that have worked well for us in real-world problems are Least-Squares Policy Iteration, Gradient Temporal-Difference, Deep Q-Networks and Natural Policy Gradient. We have always paid attention to Richard Sutton's mantra of avoiding the Deadly Triad. We recommend [the excellent paper by Hasselt, Doron, Strub, Hessel, Sonnerat, Modayil](https://arxiv.org/abs/1812.02648) [@journals/corr/abs-1812-02648] to understand the nuances of the Deadly Triad in the context of Deep Reinforcement Learning.

\index{reinforcement learning!deadly triad}
\index{reinforcement learning!least squares!lspi}
\index{reinforcement learning!temporal difference!gradient td}
\index{reinforcement learning!temporal difference!dqn}
\index{reinforcement learning!deep reinforcement learning}
\index{reinforcement learning!policy gradient!natural policy gradient}

It's important to recognize that the code we developed in this book is for educational purposes and we barely made an attempt to make the code performant. In practice, this type of educational code won't suffice—we need to develop highly performant code and make the code parallelizable wherever possible. This requires an investment in a suitable distributed system for storage and compute, so the RL algorithms can be trained in an efficient manner. 

When it comes to making an RL algorithm successful in a real-world application, the design and implementation of the model and the algorithm is only a small piece of the overall puzzle. Indeed, one needs to build an entire ecosystem of data management, software engineering, model training infrastructure, model deployment platform, tools for easy debugging, measurements/instrumentation and explainability of results. Moreover, it is vital to have a strong Product Management practice in order to ensure that the algorithm is serving the needs of the overall product being built. Indeed, the goal is to build a successful product, not just a model and an algorithm. A key challenge in many organizations is to replace a legacy system or a manual system with a modern solution (e.g., with an RL-based solution). This requires investment in a culture change in the organization so that all stakeholders are supportive, otherwise the change management will be very challenging.

When the product carrying the RL algorithm runs in production, it is vital to evaluate whether the real-world problem is actually being solved effectively by defining, evaluating and reporting the appropriate success metrics. If those metrics are found to be inadequate, we need the appropriate feedback system in the organization to investigate why the product (and perhaps the model) is not delivering the requisite results. It could be that we have designed a model which is not quite the right fit for the real-world problem, in which case we improve the model in the next iteration of this feedback system. It often takes several iterations of evaluating the success metrics, providing feedback, and improving the model (and sometimes the algorithm) in order to achieve adequate results. An important point to note is that typically in practice, we rarely need to solve all the way to an optima—typically, being close to optimum is good enough to achieve the requisite success metrics. A Product Manager must constantly question whether we are solving the right problem, and whether we are investing our efforts in the most important aspects of the problem (e.g., ask if it suffices to be reasonably close to optimum).

Lastly, one must recognize that typically in the real-world, we are plagued with noisy data, incomplete data and sometimes plain wrong data. The design of the model needs to take this into account. Also, there is no such thing as the "perfect model"—in practice, a model is simply a crude approximation of reality. It should be assumed by default that we have bad data and that we have an imperfect model. Hence, it is important to build a system that can reason about uncertainties in data and about uncertainties with the model.

A book can simply not do justice to explaining the various nuances and complications that arise in developing and deploying an RL-based solution in the real-world. Here we have simply scratched the surface of the various issues that arise. You would truly understand and appreciate these nuances and complications only by stepping into the real-world and experiencing it for yourself. However, it is important to first be grounded in the foundations of RL, which is what we hope you got from this book.
