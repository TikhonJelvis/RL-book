{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ReadMe:\n",
        "\n",
        "Question 1 is answered in part 1.\n",
        "\n",
        "Question 2 is answered in part 1.\n",
        "\n",
        "Question 3 is answered in part 3.\n",
        "\n",
        "Question 4 is answered in part 2."
      ],
      "metadata": {
        "id": "bWp2PQY4P2-x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ7fOoXubGwK"
      },
      "outputs": [],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import sys\n",
        "%cd /content/drive/My\\ Drive/Winter\\ 2022/CME241"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r394i0lObkhY"
      },
      "outputs": [],
      "source": [
        "from rl.distribution import *\n",
        "from rl.markov_process import *\n",
        "from rl.markov_decision_process import *\n",
        "from rl.policy import *\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcY29EPnRizD"
      },
      "source": [
        "#1. Deterministic Policy of an Infinitely-Countable Spaces Game"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note that since the policy is deterministic, we know that if the current state is $s$, the result action can be only one action: $a= \\pi_D(s)$.\n",
        "\n",
        "The Bellman Equations can be writtewn as\n",
        "\n",
        "\\begin{align*}\n",
        "  V^{\\pi_D}(s) &= Q^{\\pi_D}(s, \\pi_D(s))\\\\\n",
        "  Q^{\\pi_D}(s, a) &= \\mathcal{R}(s,a) + \\gamma \\cdot \\sum_{s' \\in \\mathcal{N}} \\mathcal{P}(s,a,s')\\cdot V^{\\pi_D}(s')\\\\\n",
        "  V^{\\pi_D}(s) &= \\mathcal{R}(s,\\pi_D(s)) + \\gamma \\cdot \\sum_{s' \\in \\mathcal{N}} \\mathcal{P}(s,\\pi_D(s),s')\\cdot V^{\\pi_D}(s')\\\\\n",
        "  Q^{\\pi_D}(s, a) &= \\mathcal{R}(s,a) + \\gamma \\cdot \\sum_{s' \\in \\mathcal{N}} \\mathcal{P}(s,a,s')\\cdot Q^{\\pi_D}(s', \\pi_D(s'))\n",
        "\\end{align*}\n",
        "\n",
        "Next, we will introduce the game.\n",
        "1. The states are $\\{1,2,3,4,\\dots\\}$\n",
        "2. The start state is at $1$.\n",
        "3. Policy to be chosen at any state is $a$ from $[0,1]$. With probability $a$, the state will become $s+1$ with reward $(1-a)$. With probability $1-a$, the state will remain at $s$ with reward $(1+a)$.\n",
        "4. The discount factor $\\gamma = 0.5$\n",
        "\n",
        "From the condition, at any state, \n",
        "\\begin{align*}\n",
        "  \\mathcal{R}(s,a) &= a(1-a) + (1-a)(1+a)\\\\\n",
        "  &= 1+a-2a^2\n",
        "\\end{align*}\n",
        "\n",
        "Notice that the reward and the transition do not depend on $s$. Thus, what is optimal in a state $s$ will also be optimal at any other state $s'$.\n",
        "We know there is a deterministic policy that is optimla for a game, discrete-time, countable-spaces, time-homogeneous MDP. Since we want to get a deterministic policy, we will denote the optimal choice of $a$ at any $s$ to be $a^*$.\n",
        "\n",
        "Consider\n",
        "\\begin{align*}\n",
        "  Q^{\\pi_D}(s, a^*) &= \\mathcal{R}(s,a^*) + \\gamma \\cdot \\sum_{s' \\in \\mathcal{N}} \\mathcal{P}(s,a^*,s')\\cdot Q^{\\pi_D}(s', \\pi_D(s'))\\\\\n",
        "  Q^*(s, a^*)&= \\mathcal{R}(s,a^*) + \\gamma \\cdot \\sum_{s' \\in \\mathcal{N}} \\mathcal{P}(s,a^*,s')\\cdot Q^*(s, a^*)\\\\\n",
        "  &= \\mathcal{R}(s,a^*) + \\gamma \\cdot  Q^*(s, a^*)\\\\\n",
        "  &^= \\frac{\\mathcal{R}(s,a^*)}{1-\\gamma} = 2\\mathcal{R}(s,a^*)\n",
        "\\end{align*}\n",
        "\n",
        "This suggests that the policy is to choose $a^*$ such that $\\mathcal{R}(s,a^*)$ is maximized. Thus, $a^*=1/4$ and $\\mathcal{R}(s,a^*)=9/8$.\n",
        "\n",
        "Therefore,\n",
        "\\begin{align*}\n",
        "Q^*(s, a^*)&=9/4\\\\\n",
        "V^*(s)&=9/4\\\\\n",
        "\\pi^*(s)&=1/4\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "lDShW0BcK7Gr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkP8hiL4bK2M"
      },
      "source": [
        "# 2. Myopic Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Myopic strategy is equivalent to setting $\\gamma=0$. Thus, $Q^{\\pi}(s, a) = \\mathcal{R}(s,a)$ for any $\\pi$ including the optimal $\\pi$. Thus, $a^*$, which is the optimal action condition on the state, will be the maximizer of $\\mathcal{R}(s,a)$.\n",
        "\n",
        "In this problem, states are real numbers. For any $s$, the transition will be into $s' \\sim s + \\mathcal{N}(0,\\sigma^2)$ with the reward of $-e^{a{s'}}$.\n",
        "\n",
        "Thus, in the myopic case, we want $a$ (which can be a function of $s$) to minimize \n",
        "\\begin{align*}\n",
        "\\mathbf{E}\\left[e^{a{s'}}\\right] = e^{as} \\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{x^2}{2\\sigma^2}}e^{ax}dx\n",
        "= e^{as+\\frac{a^{2}\\sigma^{2}}{2}}\n",
        "\\end{align*}\n",
        "\n",
        "This is attainable by minimizing\n",
        "\\begin{align*}\n",
        "2as+a^{2}\\sigma^{2}\n",
        "\\end{align*}\n",
        "\n",
        "Thus, the optimnal action is to choose $a=-\\frac{s}{\\sigma^2}$.\n",
        "\n",
        "However, note that the action does not affect the transition of state. Thus, the optimal myopic strategy will also be the optimal strategy in a non-myopic case.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u3LdaVNcZeVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Frog Escape "
      ],
      "metadata": {
        "id": "28l75lDperpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider an array of $ + 1$ lilypads on a pond, numbered $0$ to $n$. A frog sits on a lilypad other\n",
        "than the lilypads numbered 0 or n. When on lilypad $i (1 \\le i \\le n − 1)$, the frog can croak one of two\n",
        "sounds A or B. If it croaks A when on lilypad $i (1 \\le i \\le n−1)$, it is thrown to lilypad $i−1$ with \n",
        "probability $\\frac{i}{n}$ and is thrown to lilypad $i + 1$ with probability $\\frac{n-i}{n}$ . If it croaks B when on lilypad $i \n",
        "(1 ≤ i ≤ n−1)$, it is thrown to one of the lilypads $0,\\dots,i−1,i+1,\\dots,n$ with uniform probability $n$. A snake, perched on lilypad $0$, will eat the frog if the frog lands on lilypad $0$. The frog can escape the pond (and hence, escape the snake!) if it lands on lilypad $n$.\n",
        "\n",
        "The state space is $\\{0,1,2,3,\\dots,n\\}$ with $0$ and $n$ be a terminal state. Landing on each state will yield reward of $0$ except for $n$ that gives a reward of $1$. The discoubted factor $\\gamma=1$ (in order to extract the probability to win the game (escaping)). Thus, the reward function can be defined as\n",
        "\n",
        "\\begin{align*}\n",
        "r(s,a,s') = \\mathbf{1}_{s'=n}\n",
        "\\end{align*}\n",
        "\n",
        "The action space is $\\{A, B\\}$ (finite). Transition function can be writen as\n",
        "\n",
        "\\begin{align*}\n",
        "p(s,A,s') &= \\frac{s}{n}\\mathbf{1}_{s'=s-1} + \\frac{n-s}{n}\\mathbf{1}_{s'=s+1}\\\\\n",
        "p(s,B,s') &= \\frac{1}{n}\\mathbf{1}_{s'\\ne n}\n",
        "\\end{align*}\n",
        "\n",
        "In summary,\n",
        "\\begin{align*}\n",
        "\\mathcal{S} &= \\{0,1,2,3,\\dots,n\\}\\\\\n",
        "\\mathcal{N} &= \\{1,2,3,\\dots,n-1\\}\\\\\n",
        "\\mathcal{A} &= \\{A,B\\}\\\\\n",
        "p(s,A,s') &= \\frac{s}{n}\\mathbf{1}_{s'=s-1} + \\frac{n-s}{n}\\mathbf{1}_{s'=s+1}\\\\\n",
        "p(s,B,s') &= \\frac{1}{n}\\mathbf{1}_{s'\\ne n}\\\\\n",
        "r(s,a,s') &= \\mathbf{1}_{s'=n}\n",
        "\\end{align*}\n",
        "\n"
      ],
      "metadata": {
        "id": "pXaOS5o3e9LC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class LilyPadState:\n",
        "  lilypad: int\n",
        "\n",
        "class FrogMDP(FiniteMarkovDecisionProcess[LilyPadState, str]):\n",
        "\n",
        "    def __init__(self, n: int):\n",
        "        self.n: int = n\n",
        "        super().__init__(self.get_action_transition_reward_map())\n",
        "\n",
        "    def get_action_transition_reward_map(self) -> Mapping[LilyPadState, Mapping[str, Categorical[Tuple[LilyPadState, float]]]]:\n",
        "        d: Dict[LilyPadState, Dict[str, Categorical[Tuple[LilyPadState, float]]]] = {}\n",
        "        for s in range(1,self.n):\n",
        "          small_d: Dict[str, Categorical[Tuple[LilyPadState, float]]] = {}\n",
        "          small_d['A'] = Categorical({(LilyPadState(s-1), 0) : s/self.n, (LilyPadState(s+1), int(s==self.n-1)) : 1-s/self.n})\n",
        "          small_small_d: Dict[Tuple[LilyPadState, float]] = {}\n",
        "          small_small_d = {(LilyPadState(i), int(i==self.n)):1/self.n for i in range(self.n+1) if i != s}\n",
        "          small_d['B'] = Categorical(small_small_d)\n",
        "          d[LilyPadState(s)] = small_d\n",
        "        return d"
      ],
      "metadata": {
        "id": "7xmyyRU-jCH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in [3,6,9]:\n",
        "  #Initialize MDP\n",
        "  FrogEscape = FrogMDP(n)\n",
        "\n",
        "  for i in range(2**(n-1)):\n",
        "    #Create policies\n",
        "    action_for: Dict[LilyPadState, str] = {}\n",
        "    for s in range(1, n):\n",
        "        if i%2:\n",
        "          action_for[LilyPadState(s)] = 'A'\n",
        "        else:\n",
        "          action_for[LilyPadState(s)] = 'B'\n",
        "        i = i//2\n",
        "    #Create MRP\n",
        "    FrogEscapeMRP = FrogEscape.apply_finite_policy(FiniteDeterministicPolicy(action_for))\n",
        "    vec = FrogEscapeMRP.get_value_function_vec(gamma=1)\n",
        "    plt.plot(np.arange(1,n), vec)\n",
        "  plt.ylabel('Winning Probability')\n",
        "  plt.xlabel('Frog Position')\n",
        "  plt.title('Winning Probabilities across Strategies (n='+str(n)+')')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "jfMzMsXV0jwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in [3,6,9]:\n",
        "  #Initialize MDP\n",
        "  FrogEscape = FrogMDP(n)\n",
        "\n",
        "  for i in range(2**(n-1)):\n",
        "    #Create policies\n",
        "    action_for: Dict[LilyPadState, str] = {}\n",
        "    for s in range(1, n):\n",
        "        if i%2:\n",
        "          action_for[LilyPadState(s)] = 'A'\n",
        "        else:\n",
        "          action_for[LilyPadState(s)] = 'B'\n",
        "        i = i//2\n",
        "    #Create MRP\n",
        "    FrogEscapeMRP = FrogEscape.apply_finite_policy(FiniteDeterministicPolicy(action_for))\n",
        "    vec = FrogEscapeMRP.get_value_function_vec(gamma=1)\n",
        "    plt.plot(np.arange(1,n), vec)\n",
        "    for d in range(1,n):\n",
        "      plt.text(d, vec[d-1], action_for[LilyPadState(d)])\n",
        "  plt.ylabel('Winning Probability')\n",
        "  plt.xlabel('Frog Position')\n",
        "  plt.title('Winning Probabilities across Strategies (n='+str(n)+')')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "4snDpm0wzktm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the three examples, the frog should only croak B when it is on the first liliy pad. Apart from that, it should always croak A."
      ],
      "metadata": {
        "id": "WImSig7x00Rp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}